{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import h2o \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display matplotlib graphics in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_121\"; OpenJDK Runtime Environment (Zulu 8.20.0.5-macosx) (build 1.8.0_121-b15); OpenJDK 64-Bit Server VM (Zulu 8.20.0.5-macosx) (build 25.121-b15, mixed mode)\n",
      "  Starting server from /Users/bear/anaconda/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/lh/42j8mfjx069d1bkc2wlf2pw40000gn/T/tmpw_a3y2kf\n",
      "  JVM stdout: /var/folders/lh/42j8mfjx069d1bkc2wlf2pw40000gn/T/tmpw_a3y2kf/h2o_bear_started_from_python.out\n",
      "  JVM stderr: /var/folders/lh/42j8mfjx069d1bkc2wlf2pw40000gn/T/tmpw_a3y2kf/h2o_bear_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/New_York</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.1.1</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>25 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_bear_d9y5oa</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.556 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster timezone:       America/New_York\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.1.1\n",
       "H2O cluster version age:    25 days\n",
       "H2O cluster name:           H2O_from_python_bear_d9y5oa\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.556 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start and connect to h2o server\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load clean data\n",
    "path = 'data/loan.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define input variable measurement levels \n",
    "# strings automatically parsed as enums (nominal)\n",
    "# numbers automatically parsed as numeric\n",
    "col_types = {'bad_loan': 'enum'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "frame = h2o.import_file(path=path, col_types=col_types) # import from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:163987\n",
      "Cols:15\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>loan_amnt         </th><th>term     </th><th>int_rate          </th><th>emp_length        </th><th>home_ownership  </th><th>annual_inc        </th><th>purpose           </th><th>addr_state  </th><th>dti               </th><th>delinq_2yrs       </th><th>revol_util        </th><th>total_acc         </th><th>bad_loan  </th><th>longest_credit_length  </th><th>verification_status  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>int               </td><td>enum     </td><td>real              </td><td>int               </td><td>enum            </td><td>real              </td><td>enum              </td><td>enum        </td><td>real              </td><td>int               </td><td>real              </td><td>int               </td><td>enum      </td><td>int                    </td><td>enum                 </td></tr>\n",
       "<tr><td>mins   </td><td>500.0             </td><td>         </td><td>5.42              </td><td>0.0               </td><td>                </td><td>1896.0            </td><td>                  </td><td>            </td><td>0.0               </td><td>0.0               </td><td>0.0               </td><td>1.0               </td><td>          </td><td>0.0                    </td><td>                     </td></tr>\n",
       "<tr><td>mean   </td><td>13074.169141456336</td><td>         </td><td>13.715904065566173</td><td>5.68435293299533  </td><td>                </td><td>71915.67051974901 </td><td>                  </td><td>            </td><td>15.881530121290117</td><td>0.2273570060625282</td><td>54.07917280242258 </td><td>24.579733834274638</td><td>          </td><td>14.854273655448353     </td><td>                     </td></tr>\n",
       "<tr><td>maxs   </td><td>35000.0           </td><td>         </td><td>26.06             </td><td>10.0              </td><td>                </td><td>7141778.0         </td><td>                  </td><td>            </td><td>39.99             </td><td>29.0              </td><td>150.70000000000002</td><td>118.0             </td><td>          </td><td>65.0                   </td><td>                     </td></tr>\n",
       "<tr><td>sigma  </td><td>7993.556188734649 </td><td>         </td><td>4.391939870545795 </td><td>3.6106637311002365</td><td>                </td><td>59070.915654918244</td><td>                  </td><td>            </td><td>7.587668224192549 </td><td>0.6941679229284182</td><td>25.285366766770505</td><td>11.685190365910659</td><td>          </td><td>6.947732922546696      </td><td>                     </td></tr>\n",
       "<tr><td>zeros  </td><td>0                 </td><td>         </td><td>0                 </td><td>14248             </td><td>                </td><td>0                 </td><td>                  </td><td>            </td><td>270               </td><td>139459            </td><td>1562              </td><td>0                 </td><td>          </td><td>11                     </td><td>                     </td></tr>\n",
       "<tr><td>missing</td><td>0                 </td><td>0        </td><td>0                 </td><td>5804              </td><td>0               </td><td>4                 </td><td>0                 </td><td>0           </td><td>0                 </td><td>29                </td><td>193               </td><td>29                </td><td>0         </td><td>29                     </td><td>0                    </td></tr>\n",
       "<tr><td>0      </td><td>5000.0            </td><td>36 months</td><td>10.65             </td><td>10.0              </td><td>RENT            </td><td>24000.0           </td><td>credit_card       </td><td>AZ          </td><td>27.65             </td><td>0.0               </td><td>83.7              </td><td>9.0               </td><td>0         </td><td>26.0                   </td><td>verified             </td></tr>\n",
       "<tr><td>1      </td><td>2500.0            </td><td>60 months</td><td>15.27             </td><td>0.0               </td><td>RENT            </td><td>30000.0           </td><td>car               </td><td>GA          </td><td>1.0               </td><td>0.0               </td><td>9.4               </td><td>4.0               </td><td>1         </td><td>12.0                   </td><td>verified             </td></tr>\n",
       "<tr><td>2      </td><td>2400.0            </td><td>36 months</td><td>15.96             </td><td>10.0              </td><td>RENT            </td><td>12252.0           </td><td>small_business    </td><td>IL          </td><td>8.72              </td><td>0.0               </td><td>98.5              </td><td>10.0              </td><td>0         </td><td>10.0                   </td><td>not verified         </td></tr>\n",
       "<tr><td>3      </td><td>10000.0           </td><td>36 months</td><td>13.49             </td><td>10.0              </td><td>RENT            </td><td>49200.0           </td><td>other             </td><td>CA          </td><td>20.0              </td><td>0.0               </td><td>21.0              </td><td>37.0              </td><td>0         </td><td>15.0                   </td><td>verified             </td></tr>\n",
       "<tr><td>4      </td><td>5000.0            </td><td>36 months</td><td>7.9               </td><td>3.0               </td><td>RENT            </td><td>36000.0           </td><td>wedding           </td><td>AZ          </td><td>11.2              </td><td>0.0               </td><td>28.3              </td><td>12.0              </td><td>0         </td><td>7.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>5      </td><td>3000.0            </td><td>36 months</td><td>18.64             </td><td>9.0               </td><td>RENT            </td><td>48000.0           </td><td>car               </td><td>CA          </td><td>5.3500000000000005</td><td>0.0               </td><td>87.5              </td><td>4.0               </td><td>0         </td><td>4.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>6      </td><td>5600.0            </td><td>60 months</td><td>21.28             </td><td>4.0               </td><td>OWN             </td><td>40000.0           </td><td>small_business    </td><td>CA          </td><td>5.55              </td><td>0.0               </td><td>32.6              </td><td>13.0              </td><td>1         </td><td>7.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>7      </td><td>5375.0            </td><td>60 months</td><td>12.69             </td><td>0.0               </td><td>RENT            </td><td>15000.0           </td><td>other             </td><td>TX          </td><td>18.08             </td><td>0.0               </td><td>36.5              </td><td>3.0               </td><td>1         </td><td>7.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>8      </td><td>6500.0            </td><td>60 months</td><td>14.65             </td><td>5.0               </td><td>OWN             </td><td>72000.0           </td><td>debt_consolidation</td><td>AZ          </td><td>16.12             </td><td>0.0               </td><td>20.6              </td><td>23.0              </td><td>0         </td><td>13.0                   </td><td>not verified         </td></tr>\n",
       "<tr><td>9      </td><td>12000.0           </td><td>36 months</td><td>12.69             </td><td>10.0              </td><td>OWN             </td><td>75000.0           </td><td>debt_consolidation</td><td>CA          </td><td>10.78             </td><td>0.0               </td><td>67.10000000000001 </td><td>34.0              </td><td>0         </td><td>22.0                   </td><td>verified             </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame.describe() # summarize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into 40% training, 30% validation, and 30% test\n",
    "train, valid, test = frame.split_frame([0.4, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad_loan\n",
      "['loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership', 'annual_inc', 'purpose', 'addr_state', 'dti', 'delinq_2yrs', 'revol_util', 'total_acc', 'longest_credit_length', 'verification_status']\n"
     ]
    }
   ],
   "source": [
    "# assign target and inputs\n",
    "y = 'bad_loan'\n",
    "X = [name for name in frame.columns if name != y]\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['term', 'home_ownership', 'purpose', 'addr_state', 'verification_status']\n",
      "['loan_amnt', 'int_rate', 'emp_length', 'annual_inc', 'dti', 'delinq_2yrs', 'revol_util', 'total_acc', 'longest_credit_length']\n"
     ]
    }
   ],
   "source": [
    "# determine column types\n",
    "reals, enums = [], []\n",
    "for key, val in frame.types.items():\n",
    "    if key in X:\n",
    "        if val == 'enum':\n",
    "            enums.append(key)\n",
    "        else: \n",
    "            reals.append(key)\n",
    "\n",
    "print(enums)\n",
    "print(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute missing values\n",
    "_ = frame[reals].impute(method='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set target to factor - for binary classification\n",
    "# just to be safe ...\n",
    "train[y] = train[y].asfactor()\n",
    "valid[y] = valid[y].asfactor()\n",
    "test[y] = test[y].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Model Build progress: |██████████████████████████████████████| 100%\n",
      "Model Details\n",
      "=============\n",
      "H2ODeepLearningEstimator :  Deep Learning\n",
      "Model Key:  nn_model\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.14012752126328837\n",
      "RMSE: 0.3743361073464439\n",
      "LogLoss: 0.4443499533129185\n",
      "Mean Per-Class Error: 0.35657985163704065\n",
      "AUC: 0.6915638683615544\n",
      "pr_auc: 0.3288191580005072\n",
      "Gini: 0.38312773672310874\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.19205222874853198: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>5439.0</td>\n",
       "<td>2757.0</td>\n",
       "<td>0.3364</td>\n",
       "<td> (2757.0/8196.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>704.0</td>\n",
       "<td>1158.0</td>\n",
       "<td>0.3781</td>\n",
       "<td> (704.0/1862.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>6143.0</td>\n",
       "<td>3915.0</td>\n",
       "<td>0.3441</td>\n",
       "<td> (3461.0/10058.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1     Error    Rate\n",
       "-----  ----  ----  -------  ----------------\n",
       "0      5439  2757  0.3364   (2757.0/8196.0)\n",
       "1      704   1158  0.3781   (704.0/1862.0)\n",
       "Total  6143  3915  0.3441   (3461.0/10058.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1920522</td>\n",
       "<td>0.4009001</td>\n",
       "<td>241.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1022221</td>\n",
       "<td>0.5587889</td>\n",
       "<td>340.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2764877</td>\n",
       "<td>0.3648265</td>\n",
       "<td>166.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5601438</td>\n",
       "<td>0.8152714</td>\n",
       "<td>16.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.6143296</td>\n",
       "<td>0.6363636</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0491335</td>\n",
       "<td>1.0</td>\n",
       "<td>397.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.6404717</td>\n",
       "<td>0.9998780</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2416466</td>\n",
       "<td>0.2318230</td>\n",
       "<td>194.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1852852</td>\n",
       "<td>0.6385607</td>\n",
       "<td>248.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1767027</td>\n",
       "<td>0.6434201</td>\n",
       "<td>257.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.192052     0.4009    241\n",
       "max f2                       0.102222     0.558789  340\n",
       "max f0point5                 0.276488     0.364826  166\n",
       "max accuracy                 0.560144     0.815271  16\n",
       "max precision                0.61433      0.636364  4\n",
       "max recall                   0.0491335    1         397\n",
       "max specificity              0.640472     0.999878  0\n",
       "max absolute_mcc             0.241647     0.231823  194\n",
       "max min_per_class_accuracy   0.185285     0.638561  248\n",
       "max mean_per_class_accuracy  0.176703     0.64342   257"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.51 %, avg score: 19.14 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100418</td>\n",
       "<td>0.5282891</td>\n",
       "<td>2.5671534</td>\n",
       "<td>2.5671534</td>\n",
       "<td>0.4752475</td>\n",
       "<td>0.5664361</td>\n",
       "<td>0.4752475</td>\n",
       "<td>0.5664361</td>\n",
       "<td>0.0257787</td>\n",
       "<td>0.0257787</td>\n",
       "<td>156.7153386</td>\n",
       "<td>156.7153386</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200835</td>\n",
       "<td>0.4910055</td>\n",
       "<td>2.4067063</td>\n",
       "<td>2.4869298</td>\n",
       "<td>0.4455446</td>\n",
       "<td>0.5071435</td>\n",
       "<td>0.4603960</td>\n",
       "<td>0.5367898</td>\n",
       "<td>0.0241676</td>\n",
       "<td>0.0499463</td>\n",
       "<td>140.6706299</td>\n",
       "<td>148.6929842</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300259</td>\n",
       "<td>0.4660290</td>\n",
       "<td>2.1606874</td>\n",
       "<td>2.3789026</td>\n",
       "<td>0.4</td>\n",
       "<td>0.4780564</td>\n",
       "<td>0.4403974</td>\n",
       "<td>0.5173416</td>\n",
       "<td>0.0214823</td>\n",
       "<td>0.0714286</td>\n",
       "<td>116.0687433</td>\n",
       "<td>137.8902554</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400676</td>\n",
       "<td>0.4445939</td>\n",
       "<td>2.2997416</td>\n",
       "<td>2.3590632</td>\n",
       "<td>0.4257426</td>\n",
       "<td>0.4538818</td>\n",
       "<td>0.4367246</td>\n",
       "<td>0.5014373</td>\n",
       "<td>0.0230934</td>\n",
       "<td>0.0945220</td>\n",
       "<td>129.9741575</td>\n",
       "<td>135.9063202</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500099</td>\n",
       "<td>0.4264832</td>\n",
       "<td>2.5388077</td>\n",
       "<td>2.3947977</td>\n",
       "<td>0.47</td>\n",
       "<td>0.4359910</td>\n",
       "<td>0.4433400</td>\n",
       "<td>0.4884261</td>\n",
       "<td>0.0252417</td>\n",
       "<td>0.1197637</td>\n",
       "<td>153.8807734</td>\n",
       "<td>139.4797701</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000199</td>\n",
       "<td>0.3559731</td>\n",
       "<td>2.0618886</td>\n",
       "<td>2.2283432</td>\n",
       "<td>0.3817097</td>\n",
       "<td>0.3896852</td>\n",
       "<td>0.4125249</td>\n",
       "<td>0.4390557</td>\n",
       "<td>0.1031149</td>\n",
       "<td>0.2228786</td>\n",
       "<td>106.1888604</td>\n",
       "<td>122.8343153</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500298</td>\n",
       "<td>0.3097427</td>\n",
       "<td>1.5786335</td>\n",
       "<td>2.0117733</td>\n",
       "<td>0.2922465</td>\n",
       "<td>0.3320255</td>\n",
       "<td>0.3724321</td>\n",
       "<td>0.4033789</td>\n",
       "<td>0.0789474</td>\n",
       "<td>0.3018260</td>\n",
       "<td>57.8633462</td>\n",
       "<td>101.1773256</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000398</td>\n",
       "<td>0.2744718</td>\n",
       "<td>1.7182405</td>\n",
       "<td>1.9383901</td>\n",
       "<td>0.3180915</td>\n",
       "<td>0.2918115</td>\n",
       "<td>0.3588469</td>\n",
       "<td>0.3754871</td>\n",
       "<td>0.0859291</td>\n",
       "<td>0.3877551</td>\n",
       "<td>71.8240503</td>\n",
       "<td>93.8390068</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000597</td>\n",
       "<td>0.2226946</td>\n",
       "<td>1.2994194</td>\n",
       "<td>1.7253998</td>\n",
       "<td>0.2405567</td>\n",
       "<td>0.2465595</td>\n",
       "<td>0.3194168</td>\n",
       "<td>0.3325112</td>\n",
       "<td>0.1299678</td>\n",
       "<td>0.5177229</td>\n",
       "<td>29.9419381</td>\n",
       "<td>72.5399839</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.3999801</td>\n",
       "<td>0.1883361</td>\n",
       "<td>1.1233425</td>\n",
       "<td>1.5749977</td>\n",
       "<td>0.2079602</td>\n",
       "<td>0.2045363</td>\n",
       "<td>0.2915735</td>\n",
       "<td>0.3005414</td>\n",
       "<td>0.1122449</td>\n",
       "<td>0.6299678</td>\n",
       "<td>12.3342471</td>\n",
       "<td>57.4997737</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1592447</td>\n",
       "<td>0.9450323</td>\n",
       "<td>1.4489796</td>\n",
       "<td>0.1749503</td>\n",
       "<td>0.1728870</td>\n",
       "<td>0.2682442</td>\n",
       "<td>0.2750054</td>\n",
       "<td>0.0945220</td>\n",
       "<td>0.7244898</td>\n",
       "<td>-5.4967723</td>\n",
       "<td>44.8979592</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6000199</td>\n",
       "<td>0.1362810</td>\n",
       "<td>0.8054252</td>\n",
       "<td>1.3417028</td>\n",
       "<td>0.1491054</td>\n",
       "<td>0.1473830</td>\n",
       "<td>0.2483844</td>\n",
       "<td>0.2537315</td>\n",
       "<td>0.0805585</td>\n",
       "<td>0.8050483</td>\n",
       "<td>-19.4574764</td>\n",
       "<td>34.1702760</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999403</td>\n",
       "<td>0.1168601</td>\n",
       "<td>0.5858580</td>\n",
       "<td>1.2338016</td>\n",
       "<td>0.1084577</td>\n",
       "<td>0.1263447</td>\n",
       "<td>0.2284091</td>\n",
       "<td>0.2355463</td>\n",
       "<td>0.0585392</td>\n",
       "<td>0.8635875</td>\n",
       "<td>-41.4141965</td>\n",
       "<td>23.3801631</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999602</td>\n",
       "<td>0.0984589</td>\n",
       "<td>0.6174927</td>\n",
       "<td>1.1567439</td>\n",
       "<td>0.1143141</td>\n",
       "<td>0.1075378</td>\n",
       "<td>0.2141437</td>\n",
       "<td>0.2195413</td>\n",
       "<td>0.0617615</td>\n",
       "<td>0.9253491</td>\n",
       "<td>-38.2507319</td>\n",
       "<td>15.6743862</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999801</td>\n",
       "<td>0.0794413</td>\n",
       "<td>0.4671466</td>\n",
       "<td>1.0801050</td>\n",
       "<td>0.0864811</td>\n",
       "<td>0.0890481</td>\n",
       "<td>0.1999558</td>\n",
       "<td>0.2050388</td>\n",
       "<td>0.0467240</td>\n",
       "<td>0.9720730</td>\n",
       "<td>-53.2853363</td>\n",
       "<td>8.0105019</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0412045</td>\n",
       "<td>0.2792141</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0516899</td>\n",
       "<td>0.0684373</td>\n",
       "<td>0.1851263</td>\n",
       "<td>0.1913760</td>\n",
       "<td>0.0279270</td>\n",
       "<td>1.0</td>\n",
       "<td>-72.0785918</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100418                   0.528289           2.56715   2.56715            0.475248         0.566436   0.475248                    0.566436            0.0257787       0.0257787                  156.715   156.715\n",
       "    2        0.0200835                   0.491005           2.40671   2.48693            0.445545         0.507143   0.460396                    0.53679             0.0241676       0.0499463                  140.671   148.693\n",
       "    3        0.0300259                   0.466029           2.16069   2.3789             0.4              0.478056   0.440397                    0.517342            0.0214823       0.0714286                  116.069   137.89\n",
       "    4        0.0400676                   0.444594           2.29974   2.35906            0.425743         0.453882   0.436725                    0.501437            0.0230934       0.094522                   129.974   135.906\n",
       "    5        0.0500099                   0.426483           2.53881   2.3948             0.47             0.435991   0.44334                     0.488426            0.0252417       0.119764                   153.881   139.48\n",
       "    6        0.10002                     0.355973           2.06189   2.22834            0.38171          0.389685   0.412525                    0.439056            0.103115        0.222879                   106.189   122.834\n",
       "    7        0.15003                     0.309743           1.57863   2.01177            0.292247         0.332026   0.372432                    0.403379            0.0789474       0.301826                   57.8633   101.177\n",
       "    8        0.20004                     0.274472           1.71824   1.93839            0.318091         0.291812   0.358847                    0.375487            0.0859291       0.387755                   71.8241   93.839\n",
       "    9        0.30006                     0.222695           1.29942   1.7254             0.240557         0.24656    0.319417                    0.332511            0.129968        0.517723                   29.9419   72.54\n",
       "    10       0.39998                     0.188336           1.12334   1.575              0.20796          0.204536   0.291573                    0.300541            0.112245        0.629968                   12.3342   57.4998\n",
       "    11       0.5                         0.159245           0.945032  1.44898            0.17495          0.172887   0.268244                    0.275005            0.094522        0.72449                    -5.49677  44.898\n",
       "    12       0.60002                     0.136281           0.805425  1.3417             0.149105         0.147383   0.248384                    0.253731            0.0805585       0.805048                   -19.4575  34.1703\n",
       "    13       0.69994                     0.11686            0.585858  1.2338             0.108458         0.126345   0.228409                    0.235546            0.0585392       0.863588                   -41.4142  23.3802\n",
       "    14       0.79996                     0.0984589          0.617493  1.15674            0.114314         0.107538   0.214144                    0.219541            0.0617615       0.925349                   -38.2507  15.6744\n",
       "    15       0.89998                     0.0794413          0.467147  1.08011            0.0864811        0.0890481  0.199956                    0.205039            0.046724        0.972073                   -53.2853  8.0105\n",
       "    16       1                           0.0412045          0.279214  1                  0.0516899        0.0684373  0.185126                    0.191376            0.027927        1                          -72.0786  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.13897766905487463\n",
      "RMSE: 0.37279708831330033\n",
      "LogLoss: 0.4413991844691089\n",
      "Mean Per-Class Error: 0.35983832063361787\n",
      "AUC: 0.6906684424910284\n",
      "pr_auc: 0.32383478570708496\n",
      "Gini: 0.38133688498205687\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.1974062202858475: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>27617.0</td>\n",
       "<td>12638.0</td>\n",
       "<td>0.3139</td>\n",
       "<td> (12638.0/40255.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>3672.0</td>\n",
       "<td>5313.0</td>\n",
       "<td>0.4087</td>\n",
       "<td> (3672.0/8985.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>31289.0</td>\n",
       "<td>17951.0</td>\n",
       "<td>0.3312</td>\n",
       "<td> (16310.0/49240.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      27617  12638  0.3139   (12638.0/40255.0)\n",
       "1      3672   5313   0.4087   (3672.0/8985.0)\n",
       "Total  31289  17951  0.3312   (16310.0/49240.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1974062</td>\n",
       "<td>0.3944906</td>\n",
       "<td>235.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1137712</td>\n",
       "<td>0.5579900</td>\n",
       "<td>327.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2646972</td>\n",
       "<td>0.3525951</td>\n",
       "<td>179.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5346475</td>\n",
       "<td>0.8179529</td>\n",
       "<td>26.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.6573121</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0420548</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.6573121</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2067180</td>\n",
       "<td>0.2228142</td>\n",
       "<td>226.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1822872</td>\n",
       "<td>0.6386039</td>\n",
       "<td>250.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1868849</td>\n",
       "<td>0.6401617</td>\n",
       "<td>245.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.197406     0.394491  235\n",
       "max f2                       0.113771     0.55799   327\n",
       "max f0point5                 0.264697     0.352595  179\n",
       "max accuracy                 0.534647     0.817953  26\n",
       "max precision                0.657312     1         0\n",
       "max recall                   0.0420548    1         399\n",
       "max specificity              0.657312     1         0\n",
       "max absolute_mcc             0.206718     0.222814  226\n",
       "max min_per_class_accuracy   0.182287     0.638604  250\n",
       "max mean_per_class_accuracy  0.186885     0.640162  245"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.25 %, avg score: 18.94 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100122</td>\n",
       "<td>0.5279233</td>\n",
       "<td>2.8234933</td>\n",
       "<td>2.8234933</td>\n",
       "<td>0.5152130</td>\n",
       "<td>0.5629432</td>\n",
       "<td>0.5152130</td>\n",
       "<td>0.5629432</td>\n",
       "<td>0.0282693</td>\n",
       "<td>0.0282693</td>\n",
       "<td>182.3493291</td>\n",
       "<td>182.3493291</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200041</td>\n",
       "<td>0.4928367</td>\n",
       "<td>2.3057128</td>\n",
       "<td>2.5648659</td>\n",
       "<td>0.4207317</td>\n",
       "<td>0.5086217</td>\n",
       "<td>0.4680203</td>\n",
       "<td>0.5358100</td>\n",
       "<td>0.0230384</td>\n",
       "<td>0.0513077</td>\n",
       "<td>130.5712773</td>\n",
       "<td>156.4865865</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300162</td>\n",
       "<td>0.4682940</td>\n",
       "<td>2.3121520</td>\n",
       "<td>2.4805709</td>\n",
       "<td>0.4219067</td>\n",
       "<td>0.4803190</td>\n",
       "<td>0.4526387</td>\n",
       "<td>0.5173005</td>\n",
       "<td>0.0231497</td>\n",
       "<td>0.0744574</td>\n",
       "<td>131.2151986</td>\n",
       "<td>148.0570911</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400081</td>\n",
       "<td>0.4445787</td>\n",
       "<td>2.2166031</td>\n",
       "<td>2.4146460</td>\n",
       "<td>0.4044715</td>\n",
       "<td>0.4560534</td>\n",
       "<td>0.4406091</td>\n",
       "<td>0.5020043</td>\n",
       "<td>0.0221480</td>\n",
       "<td>0.0966055</td>\n",
       "<td>121.6603101</td>\n",
       "<td>141.4645955</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.05</td>\n",
       "<td>0.4254451</td>\n",
       "<td>2.1163547</td>\n",
       "<td>2.3550362</td>\n",
       "<td>0.3861789</td>\n",
       "<td>0.4347478</td>\n",
       "<td>0.4297319</td>\n",
       "<td>0.4885639</td>\n",
       "<td>0.0211464</td>\n",
       "<td>0.1177518</td>\n",
       "<td>111.6354719</td>\n",
       "<td>135.5036171</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1</td>\n",
       "<td>0.3539732</td>\n",
       "<td>1.9766277</td>\n",
       "<td>2.1658319</td>\n",
       "<td>0.3606824</td>\n",
       "<td>0.3866333</td>\n",
       "<td>0.3952071</td>\n",
       "<td>0.4375986</td>\n",
       "<td>0.0988314</td>\n",
       "<td>0.2165832</td>\n",
       "<td>97.6627713</td>\n",
       "<td>116.5831942</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.15</td>\n",
       "<td>0.3056372</td>\n",
       "<td>1.6605454</td>\n",
       "<td>1.9974031</td>\n",
       "<td>0.3030057</td>\n",
       "<td>0.3279686</td>\n",
       "<td>0.3644733</td>\n",
       "<td>0.4010553</td>\n",
       "<td>0.0830273</td>\n",
       "<td>0.2996105</td>\n",
       "<td>66.0545353</td>\n",
       "<td>99.7403079</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2706790</td>\n",
       "<td>1.5648303</td>\n",
       "<td>1.8892599</td>\n",
       "<td>0.2855402</td>\n",
       "<td>0.2874530</td>\n",
       "<td>0.3447400</td>\n",
       "<td>0.3726547</td>\n",
       "<td>0.0782415</td>\n",
       "<td>0.3778520</td>\n",
       "<td>56.4830273</td>\n",
       "<td>88.9259878</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3</td>\n",
       "<td>0.2208702</td>\n",
       "<td>1.3756260</td>\n",
       "<td>1.7180486</td>\n",
       "<td>0.2510154</td>\n",
       "<td>0.2442839</td>\n",
       "<td>0.3134985</td>\n",
       "<td>0.3298644</td>\n",
       "<td>0.1375626</td>\n",
       "<td>0.5154146</td>\n",
       "<td>37.5626043</td>\n",
       "<td>71.8048600</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1854103</td>\n",
       "<td>1.1329994</td>\n",
       "<td>1.5717863</td>\n",
       "<td>0.2067425</td>\n",
       "<td>0.2022861</td>\n",
       "<td>0.2868095</td>\n",
       "<td>0.2979698</td>\n",
       "<td>0.1132999</td>\n",
       "<td>0.6287145</td>\n",
       "<td>13.2999444</td>\n",
       "<td>57.1786311</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1575939</td>\n",
       "<td>0.9159711</td>\n",
       "<td>1.4406233</td>\n",
       "<td>0.1671405</td>\n",
       "<td>0.1710349</td>\n",
       "<td>0.2628757</td>\n",
       "<td>0.2725829</td>\n",
       "<td>0.0915971</td>\n",
       "<td>0.7203116</td>\n",
       "<td>-8.4028937</td>\n",
       "<td>44.0623261</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1346954</td>\n",
       "<td>0.8302727</td>\n",
       "<td>1.3388982</td>\n",
       "<td>0.1515028</td>\n",
       "<td>0.1457614</td>\n",
       "<td>0.2443136</td>\n",
       "<td>0.2514460</td>\n",
       "<td>0.0830273</td>\n",
       "<td>0.8033389</td>\n",
       "<td>-16.9727323</td>\n",
       "<td>33.8898164</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7</td>\n",
       "<td>0.1145950</td>\n",
       "<td>0.7067334</td>\n",
       "<td>1.2485889</td>\n",
       "<td>0.1289602</td>\n",
       "<td>0.1244929</td>\n",
       "<td>0.2278345</td>\n",
       "<td>0.2333098</td>\n",
       "<td>0.0706733</td>\n",
       "<td>0.8740122</td>\n",
       "<td>-29.3266555</td>\n",
       "<td>24.8588918</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0964226</td>\n",
       "<td>0.5453534</td>\n",
       "<td>1.1606845</td>\n",
       "<td>0.0995126</td>\n",
       "<td>0.1054342</td>\n",
       "<td>0.2117943</td>\n",
       "<td>0.2173253</td>\n",
       "<td>0.0545353</td>\n",
       "<td>0.9285476</td>\n",
       "<td>-45.4646633</td>\n",
       "<td>16.0684474</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9</td>\n",
       "<td>0.0786919</td>\n",
       "<td>0.4451864</td>\n",
       "<td>1.0811847</td>\n",
       "<td>0.0812348</td>\n",
       "<td>0.0876290</td>\n",
       "<td>0.1972877</td>\n",
       "<td>0.2029146</td>\n",
       "<td>0.0445186</td>\n",
       "<td>0.9730662</td>\n",
       "<td>-55.4813578</td>\n",
       "<td>8.1184691</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0412108</td>\n",
       "<td>0.2693378</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0491470</td>\n",
       "<td>0.0673565</td>\n",
       "<td>0.1824736</td>\n",
       "<td>0.1893588</td>\n",
       "<td>0.0269338</td>\n",
       "<td>1.0</td>\n",
       "<td>-73.0662215</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100122                   0.527923           2.82349   2.82349            0.515213         0.562943   0.515213                    0.562943            0.0282693       0.0282693                  182.349   182.349\n",
       "    2        0.0200041                   0.492837           2.30571   2.56487            0.420732         0.508622   0.46802                     0.53581             0.0230384       0.0513077                  130.571   156.487\n",
       "    3        0.0300162                   0.468294           2.31215   2.48057            0.421907         0.480319   0.452639                    0.5173              0.0231497       0.0744574                  131.215   148.057\n",
       "    4        0.0400081                   0.444579           2.2166    2.41465            0.404472         0.456053   0.440609                    0.502004            0.022148        0.0966055                  121.66    141.465\n",
       "    5        0.05                        0.425445           2.11635   2.35504            0.386179         0.434748   0.429732                    0.488564            0.0211464       0.117752                   111.635   135.504\n",
       "    6        0.1                         0.353973           1.97663   2.16583            0.360682         0.386633   0.395207                    0.437599            0.0988314       0.216583                   97.6628   116.583\n",
       "    7        0.15                        0.305637           1.66055   1.9974             0.303006         0.327969   0.364473                    0.401055            0.0830273       0.29961                    66.0545   99.7403\n",
       "    8        0.2                         0.270679           1.56483   1.88926            0.28554          0.287453   0.34474                     0.372655            0.0782415       0.377852                   56.483    88.926\n",
       "    9        0.3                         0.22087            1.37563   1.71805            0.251015         0.244284   0.313499                    0.329864            0.137563        0.515415                   37.5626   71.8049\n",
       "    10       0.4                         0.18541            1.133     1.57179            0.206742         0.202286   0.28681                     0.29797             0.1133          0.628715                   13.2999   57.1786\n",
       "    11       0.5                         0.157594           0.915971  1.44062            0.167141         0.171035   0.262876                    0.272583            0.0915971       0.720312                   -8.40289  44.0623\n",
       "    12       0.6                         0.134695           0.830273  1.3389             0.151503         0.145761   0.244314                    0.251446            0.0830273       0.803339                   -16.9727  33.8898\n",
       "    13       0.7                         0.114595           0.706733  1.24859            0.12896          0.124493   0.227835                    0.23331             0.0706733       0.874012                   -29.3267  24.8589\n",
       "    14       0.8                         0.0964226          0.545353  1.16068            0.0995126        0.105434   0.211794                    0.217325            0.0545353       0.928548                   -45.4647  16.0684\n",
       "    15       0.9                         0.0786919          0.445186  1.08118            0.0812348        0.087629   0.197288                    0.202915            0.0445186       0.973066                   -55.4814  8.11847\n",
       "    16       1                           0.0412108          0.269338  1                  0.049147         0.0673565  0.182474                    0.189359            0.0269338       1                          -73.0662  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>training_speed</b></td>\n",
       "<td><b>epochs</b></td>\n",
       "<td><b>iterations</b></td>\n",
       "<td><b>samples</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_r2</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_r2</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:06:37</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>None</td>\n",
       "<td>0.0</td>\n",
       "<td>0</td>\n",
       "<td>0.0</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:06:40</td>\n",
       "<td> 3.425 sec</td>\n",
       "<td>61287 obs/sec</td>\n",
       "<td>1.5207721</td>\n",
       "<td>1</td>\n",
       "<td>99898.0</td>\n",
       "<td>0.3763558</td>\n",
       "<td>0.4485231</td>\n",
       "<td>0.0610579</td>\n",
       "<td>0.6865071</td>\n",
       "<td>0.3172083</td>\n",
       "<td>2.6741181</td>\n",
       "<td>0.3517598</td>\n",
       "<td>0.3747239</td>\n",
       "<td>0.4456429</td>\n",
       "<td>0.0587156</td>\n",
       "<td>0.6844977</td>\n",
       "<td>0.3105912</td>\n",
       "<td>2.4010809</td>\n",
       "<td>0.3523152</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:06:42</td>\n",
       "<td> 5.076 sec</td>\n",
       "<td>67805 obs/sec</td>\n",
       "<td>3.0409049</td>\n",
       "<td>2</td>\n",
       "<td>199754.0</td>\n",
       "<td>0.3756996</td>\n",
       "<td>0.4471919</td>\n",
       "<td>0.0643289</td>\n",
       "<td>0.6904794</td>\n",
       "<td>0.3253858</td>\n",
       "<td>2.5136710</td>\n",
       "<td>0.3578246</td>\n",
       "<td>0.3736552</td>\n",
       "<td>0.4434750</td>\n",
       "<td>0.0640770</td>\n",
       "<td>0.6902084</td>\n",
       "<td>0.3223763</td>\n",
       "<td>2.6456354</td>\n",
       "<td>0.3314582</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:06:43</td>\n",
       "<td> 6.674 sec</td>\n",
       "<td>70761 obs/sec</td>\n",
       "<td>4.5652545</td>\n",
       "<td>3</td>\n",
       "<td>299887.0</td>\n",
       "<td>0.3758601</td>\n",
       "<td>0.4469642</td>\n",
       "<td>0.0635294</td>\n",
       "<td>0.6935855</td>\n",
       "<td>0.3279707</td>\n",
       "<td>2.7276005</td>\n",
       "<td>0.3468880</td>\n",
       "<td>0.3745827</td>\n",
       "<td>0.4446944</td>\n",
       "<td>0.0594248</td>\n",
       "<td>0.6907063</td>\n",
       "<td>0.3198812</td>\n",
       "<td>2.6011710</td>\n",
       "<td>0.3358245</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:06:46</td>\n",
       "<td>10.050 sec</td>\n",
       "<td>57386 obs/sec</td>\n",
       "<td>6.0838497</td>\n",
       "<td>4</td>\n",
       "<td>399642.0</td>\n",
       "<td>0.3750631</td>\n",
       "<td>0.4457444</td>\n",
       "<td>0.0674966</td>\n",
       "<td>0.6933985</td>\n",
       "<td>0.3257405</td>\n",
       "<td>2.4601887</td>\n",
       "<td>0.3481806</td>\n",
       "<td>0.3736062</td>\n",
       "<td>0.4432613</td>\n",
       "<td>0.0643221</td>\n",
       "<td>0.6903752</td>\n",
       "<td>0.3185696</td>\n",
       "<td>2.5233582</td>\n",
       "<td>0.3434809</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:06:48</td>\n",
       "<td>11.742 sec</td>\n",
       "<td>60042 obs/sec</td>\n",
       "<td>7.6075142</td>\n",
       "<td>5</td>\n",
       "<td>499730.0</td>\n",
       "<td>0.3749771</td>\n",
       "<td>0.4450299</td>\n",
       "<td>0.0679246</td>\n",
       "<td>0.6933639</td>\n",
       "<td>0.3254129</td>\n",
       "<td>2.5136710</td>\n",
       "<td>0.3515609</td>\n",
       "<td>0.3735537</td>\n",
       "<td>0.4426301</td>\n",
       "<td>0.0645849</td>\n",
       "<td>0.6902950</td>\n",
       "<td>0.3202217</td>\n",
       "<td>2.5678226</td>\n",
       "<td>0.3479082</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:06:50</td>\n",
       "<td>13.418 sec</td>\n",
       "<td>61783 obs/sec</td>\n",
       "<td>9.1299457</td>\n",
       "<td>6</td>\n",
       "<td>599737.0</td>\n",
       "<td>0.3793766</td>\n",
       "<td>0.4577637</td>\n",
       "<td>0.0459246</td>\n",
       "<td>0.6930467</td>\n",
       "<td>0.3265816</td>\n",
       "<td>2.5136710</td>\n",
       "<td>0.3281965</td>\n",
       "<td>0.3777288</td>\n",
       "<td>0.4549449</td>\n",
       "<td>0.0435586</td>\n",
       "<td>0.6905258</td>\n",
       "<td>0.3190850</td>\n",
       "<td>2.6567516</td>\n",
       "<td>0.3372055</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:06:52</td>\n",
       "<td>15.069 sec</td>\n",
       "<td>63289 obs/sec</td>\n",
       "<td>10.6501545</td>\n",
       "<td>7</td>\n",
       "<td>699598.0</td>\n",
       "<td>0.3802903</td>\n",
       "<td>0.4612184</td>\n",
       "<td>0.0413235</td>\n",
       "<td>0.6895221</td>\n",
       "<td>0.3254187</td>\n",
       "<td>2.5671534</td>\n",
       "<td>0.4082323</td>\n",
       "<td>0.3781773</td>\n",
       "<td>0.4571188</td>\n",
       "<td>0.0412860</td>\n",
       "<td>0.6903672</td>\n",
       "<td>0.3217577</td>\n",
       "<td>2.7456805</td>\n",
       "<td>0.3325142</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:06:53</td>\n",
       "<td>16.646 sec</td>\n",
       "<td>64747 obs/sec</td>\n",
       "<td>12.1739104</td>\n",
       "<td>8</td>\n",
       "<td>799692.0</td>\n",
       "<td>0.3765912</td>\n",
       "<td>0.4498010</td>\n",
       "<td>0.0598829</td>\n",
       "<td>0.6913469</td>\n",
       "<td>0.3277308</td>\n",
       "<td>2.7810828</td>\n",
       "<td>0.3712468</td>\n",
       "<td>0.3748770</td>\n",
       "<td>0.4466274</td>\n",
       "<td>0.0579461</td>\n",
       "<td>0.6907285</td>\n",
       "<td>0.3237248</td>\n",
       "<td>2.7123321</td>\n",
       "<td>0.3479285</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:06:55</td>\n",
       "<td>18.213 sec</td>\n",
       "<td>65944 obs/sec</td>\n",
       "<td>13.6960831</td>\n",
       "<td>9</td>\n",
       "<td>899682.0</td>\n",
       "<td>0.3743361</td>\n",
       "<td>0.4443500</td>\n",
       "<td>0.0711083</td>\n",
       "<td>0.6915639</td>\n",
       "<td>0.3288192</td>\n",
       "<td>2.5671534</td>\n",
       "<td>0.3441042</td>\n",
       "<td>0.3727971</td>\n",
       "<td>0.4413992</td>\n",
       "<td>0.0683706</td>\n",
       "<td>0.6906684</td>\n",
       "<td>0.3238348</td>\n",
       "<td>2.8234933</td>\n",
       "<td>0.3312348</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:06:57</td>\n",
       "<td>20.124 sec</td>\n",
       "<td>65649 obs/sec</td>\n",
       "<td>15.2189255</td>\n",
       "<td>10</td>\n",
       "<td>999716.0</td>\n",
       "<td>0.3773150</td>\n",
       "<td>0.4491249</td>\n",
       "<td>0.0562657</td>\n",
       "<td>0.6928652</td>\n",
       "<td>0.3261943</td>\n",
       "<td>2.6741181</td>\n",
       "<td>0.3504673</td>\n",
       "<td>0.3760556</td>\n",
       "<td>0.4468454</td>\n",
       "<td>0.0520133</td>\n",
       "<td>0.6903754</td>\n",
       "<td>0.3182404</td>\n",
       "<td>2.5789388</td>\n",
       "<td>0.3345248</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:06:57</td>\n",
       "<td>20.420 sec</td>\n",
       "<td>65615 obs/sec</td>\n",
       "<td>15.2189255</td>\n",
       "<td>10</td>\n",
       "<td>999716.0</td>\n",
       "<td>0.3743361</td>\n",
       "<td>0.4443500</td>\n",
       "<td>0.0711083</td>\n",
       "<td>0.6915639</td>\n",
       "<td>0.3288192</td>\n",
       "<td>2.5671534</td>\n",
       "<td>0.3441042</td>\n",
       "<td>0.3727971</td>\n",
       "<td>0.4413992</td>\n",
       "<td>0.0683706</td>\n",
       "<td>0.6906684</td>\n",
       "<td>0.3238348</td>\n",
       "<td>2.8234933</td>\n",
       "<td>0.3312348</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    training_speed    epochs    iterations    samples    training_rmse    training_logloss    training_r2    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_r2    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  ----------------  --------  ------------  ---------  ---------------  ------------------  -------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ---------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2019-01-22 22:06:37  0.000 sec                     0         0             0          nan              nan                 nan            nan             nan                nan              nan                              nan                nan                   nan              nan               nan                  nan                nan\n",
       "    2019-01-22 22:06:40  3.425 sec   61287 obs/sec     1.52077   1             99898      0.376356         0.448523            0.0610579      0.686507        0.317208           2.67412          0.35176                          0.374724           0.445643              0.0587156        0.684498          0.310591             2.40108            0.352315\n",
       "    2019-01-22 22:06:42  5.076 sec   67805 obs/sec     3.0409    2             199754     0.3757           0.447192            0.0643289      0.690479        0.325386           2.51367          0.357825                         0.373655           0.443475              0.064077         0.690208          0.322376             2.64564            0.331458\n",
       "    2019-01-22 22:06:43  6.674 sec   70761 obs/sec     4.56525   3             299887     0.37586          0.446964            0.0635294      0.693585        0.327971           2.7276           0.346888                         0.374583           0.444694              0.0594248        0.690706          0.319881             2.60117            0.335825\n",
       "    2019-01-22 22:06:46  10.050 sec  57386 obs/sec     6.08385   4             399642     0.375063         0.445744            0.0674966      0.693398        0.325741           2.46019          0.348181                         0.373606           0.443261              0.0643221        0.690375          0.31857              2.52336            0.343481\n",
       "    2019-01-22 22:06:48  11.742 sec  60042 obs/sec     7.60751   5             499730     0.374977         0.44503             0.0679246      0.693364        0.325413           2.51367          0.351561                         0.373554           0.44263               0.0645849        0.690295          0.320222             2.56782            0.347908\n",
       "    2019-01-22 22:06:50  13.418 sec  61783 obs/sec     9.12995   6             599737     0.379377         0.457764            0.0459246      0.693047        0.326582           2.51367          0.328196                         0.377729           0.454945              0.0435586        0.690526          0.319085             2.65675            0.337206\n",
       "    2019-01-22 22:06:52  15.069 sec  63289 obs/sec     10.6502   7             699598     0.38029          0.461218            0.0413235      0.689522        0.325419           2.56715          0.408232                         0.378177           0.457119              0.041286         0.690367          0.321758             2.74568            0.332514\n",
       "    2019-01-22 22:06:53  16.646 sec  64747 obs/sec     12.1739   8             799692     0.376591         0.449801            0.0598829      0.691347        0.327731           2.78108          0.371247                         0.374877           0.446627              0.0579461        0.690729          0.323725             2.71233            0.347929\n",
       "    2019-01-22 22:06:55  18.213 sec  65944 obs/sec     13.6961   9             899682     0.374336         0.44435             0.0711083      0.691564        0.328819           2.56715          0.344104                         0.372797           0.441399              0.0683706        0.690668          0.323835             2.82349            0.331235\n",
       "    2019-01-22 22:06:57  20.124 sec  65649 obs/sec     15.2189   10            999716     0.377315         0.449125            0.0562657      0.692865        0.326194           2.67412          0.350467                         0.376056           0.446845              0.0520133        0.690375          0.31824              2.57894            0.334525\n",
       "    2019-01-22 22:06:57  20.420 sec  65615 obs/sec     15.2189   10            999716     0.374336         0.44435             0.0711083      0.691564        0.328819           2.56715          0.344104                         0.372797           0.441399              0.0683706        0.690668          0.323835             2.82349            0.331235"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>purpose.small_business</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0609041</td></tr>\n",
       "<tr><td>addr_state.CO</td>\n",
       "<td>0.5995688</td>\n",
       "<td>0.5995688</td>\n",
       "<td>0.0365162</td></tr>\n",
       "<tr><td>annual_inc</td>\n",
       "<td>0.5280226</td>\n",
       "<td>0.5280226</td>\n",
       "<td>0.0321587</td></tr>\n",
       "<tr><td>int_rate</td>\n",
       "<td>0.5251197</td>\n",
       "<td>0.5251197</td>\n",
       "<td>0.0319819</td></tr>\n",
       "<tr><td>addr_state.WV</td>\n",
       "<td>0.4575950</td>\n",
       "<td>0.4575950</td>\n",
       "<td>0.0278694</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>addr_state.missing(NA)</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>purpose.missing(NA)</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>home_ownership.missing(NA)</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>term.missing(NA)</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>verification_status.missing(NA)</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "variable                         relative_importance    scaled_importance    percentage\n",
       "-------------------------------  ---------------------  -------------------  --------------------\n",
       "purpose.small_business           1.0                    1.0                  0.060904085562791865\n",
       "addr_state.CO                    0.5995688438415527     0.5995688438415527   0.03651619216611012\n",
       "annual_inc                       0.5280225872993469     0.5280225872993469   0.03215873283596617\n",
       "int_rate                         0.5251196622848511     0.5251196622848511   0.03198193284250094\n",
       "addr_state.WV                    0.45759496092796326    0.45759496092796326  0.027869402653459076\n",
       "---                              ---                    ---                  ---\n",
       "addr_state.missing(NA)           0.0                    0.0                  0.0\n",
       "purpose.missing(NA)              0.0                    0.0                  0.0\n",
       "home_ownership.missing(NA)       0.0                    0.0                  0.0\n",
       "term.missing(NA)                 0.0                    0.0                  0.0\n",
       "verification_status.missing(NA)  0.0                    0.0                  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neural network\n",
    "\n",
    "# initialize nn model\n",
    "nn_model = H2ODeepLearningEstimator(\n",
    "    epochs=50,                    # read over the data 50 times, but in mini-batches\n",
    "    hidden=[100],                 # 100 hidden units in 1 hidden layer\n",
    "    input_dropout_ratio=0.2,      # randomly drop 20% of inputs for each iteration, helps w/ generalization\n",
    "    hidden_dropout_ratios=[0.05], # randomly set 5% of hidden weights to 0 each iteration, helps w/ generalization\n",
    "    activation='TanhWithDropout', # bounded activation function that allows for dropout, tanh\n",
    "    l1=0.001,                     # L1 penalty can help generalization   \n",
    "    l2=0.01,                      # L2 penalty can increase stability in presence of highly correlated inputs\n",
    "    adaptive_rate=True,           # adjust magnitude of weight updates automatically (+stability, +accuracy)\n",
    "    stopping_rounds=5,            # stop after validation error does not decrease for 5 iterations\n",
    "    score_each_iteration=True,    # score validation error on every iteration\n",
    "    model_id='nn_model')          # for easy lookup in flow\n",
    "\n",
    "# train nn model\n",
    "nn_model.train(\n",
    "    x=X,\n",
    "    y=y,\n",
    "    training_frame=train,\n",
    "    validation_frame=valid)\n",
    "\n",
    "# print model information\n",
    "nn_model\n",
    "\n",
    "# view detailed results at http://localhost:54321/flow/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6915638683615544\n",
      "0.6906684424910284\n",
      "0.6956753398991676\n"
     ]
    }
   ],
   "source": [
    "# measure nn AUC\n",
    "print(nn_model.auc(train=True))\n",
    "print(nn_model.auc(valid=True))\n",
    "print(nn_model.model_performance(test_data=test).auc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# NN with random hyperparameter search\n",
    "# train many different NN models with random hyperparameters\n",
    "# and select best model based on validation error\n",
    "\n",
    "# define random grid search parameters\n",
    "hyper_parameters = {'hidden':[[170, 320], [80, 190], [320, 160, 80], [100], [50, 50, 50, 50]],\n",
    "                    'l1':[s/1e4 for s in range(0, 1000, 100)],\n",
    "                    'l2':[s/1e5 for s in range(0, 1000, 100)],\n",
    "                    'input_dropout_ratio':[s/1e2 for s in range(0, 20, 2)]}\n",
    "\n",
    "# define search strategy\n",
    "search_criteria = {'strategy':'RandomDiscrete',\n",
    "                   'max_models':20,\n",
    "                   'max_runtime_secs':600}\n",
    "\n",
    "# initialize grid search\n",
    "gsearch = H2OGridSearch(H2ODeepLearningEstimator,\n",
    "                        hyper_params=hyper_parameters,\n",
    "                        search_criteria=search_criteria)\n",
    "\n",
    "# execute training w/ grid search\n",
    "gsearch.train(x=X,\n",
    "              y=y,\n",
    "              training_frame=train,\n",
    "              validation_frame=valid)\n",
    "\n",
    "# view detailed results at http://ip:port/flow/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                hidden input_dropout_ratio    l1     l2  \\\n",
      "0                [100]                 0.0   0.0  0.003   \n",
      "1                [100]                0.16   0.0  0.006   \n",
      "2           [170, 320]                 0.0   0.0  0.009   \n",
      "3           [170, 320]                0.06  0.07  0.006   \n",
      "4            [80, 190]                0.12  0.05  0.002   \n",
      "5     [50, 50, 50, 50]                0.18  0.08  0.009   \n",
      "6           [170, 320]                 0.0  0.01  0.009   \n",
      "7                [100]                 0.1  0.04  0.008   \n",
      "8     [50, 50, 50, 50]                0.12  0.03  0.008   \n",
      "9            [80, 190]                 0.0  0.02  0.006   \n",
      "10      [320, 160, 80]                0.18  0.08    0.0   \n",
      "11    [50, 50, 50, 50]                0.04  0.07    0.0   \n",
      "12      [320, 160, 80]                0.02  0.07  0.004   \n",
      "13      [320, 160, 80]                0.08  0.02  0.003   \n",
      "14          [170, 320]                0.16  0.04  0.009   \n",
      "15               [100]                0.06  0.06  0.007   \n",
      "16           [80, 190]                0.16  0.01  0.006   \n",
      "17          [170, 320]                 0.0  0.02  0.001   \n",
      "18      [320, 160, 80]                0.18  0.02  0.002   \n",
      "19               [100]                0.02  0.05  0.001   \n",
      "\n",
      "                                                                model_ids  \\\n",
      "0   Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_model_4   \n",
      "1   Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_mode...   \n",
      "2   Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_model_5   \n",
      "3   Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_mode...   \n",
      "4   Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_mode...   \n",
      "5   Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_mode...   \n",
      "6   Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_model_9   \n",
      "7   Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_mode...   \n",
      "8   Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_model_2   \n",
      "9   Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_mode...   \n",
      "10  Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_model_7   \n",
      "11  Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_model_8   \n",
      "12  Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_model_1   \n",
      "13  Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_mode...   \n",
      "14  Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_mode...   \n",
      "15  Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_mode...   \n",
      "16  Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_model_6   \n",
      "17  Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_model_3   \n",
      "18  Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_mode...   \n",
      "19  Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_mode...   \n",
      "\n",
      "                logloss  \n",
      "0   0.44092351643259303  \n",
      "1    0.4443169689842515  \n",
      "2   0.44777615659277836  \n",
      "3    0.4751245521436998  \n",
      "4   0.47526618794661657  \n",
      "5    0.4754214359385578  \n",
      "6   0.47548375735728876  \n",
      "7    0.4755765022643118  \n",
      "8    0.4756250068385829  \n",
      "9    0.4756706372390496  \n",
      "10   0.4761257167663083  \n",
      "11   0.4761717953604289  \n",
      "12   0.4762053305368573  \n",
      "13   0.4763941041967503  \n",
      "14   0.4764412458354611  \n",
      "15   0.4812486711359482  \n",
      "16  0.48376347302381506  \n",
      "17  0.48376541523613664  \n",
      "18  0.48427833738692344  \n",
      "19   0.4863340143327703  \n",
      "Model Details\n",
      "=============\n",
      "H2ODeepLearningEstimator :  Deep Learning\n",
      "Model Key:  Grid_DeepLearning_py_7_sid_b0d1_model_python_1548212791865_24_model_4\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.1373519914200528\n",
      "RMSE: 0.3706102958905119\n",
      "LogLoss: 0.4349174231468685\n",
      "Mean Per-Class Error: 0.3410720687074309\n",
      "AUC: 0.7145815998888247\n",
      "pr_auc: 0.3537231416142696\n",
      "Gini: 0.4291631997776495\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.19309119092123808: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>5653.0</td>\n",
       "<td>2578.0</td>\n",
       "<td>0.3132</td>\n",
       "<td> (2578.0/8231.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>704.0</td>\n",
       "<td>1159.0</td>\n",
       "<td>0.3779</td>\n",
       "<td> (704.0/1863.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>6357.0</td>\n",
       "<td>3737.0</td>\n",
       "<td>0.3251</td>\n",
       "<td> (3282.0/10094.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1     Error    Rate\n",
       "-----  ----  ----  -------  ----------------\n",
       "0      5653  2578  0.3132   (2578.0/8231.0)\n",
       "1      704   1159  0.3779   (704.0/1863.0)\n",
       "Total  6357  3737  0.3251   (3282.0/10094.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1930912</td>\n",
       "<td>0.4139286</td>\n",
       "<td>232.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0949289</td>\n",
       "<td>0.5759181</td>\n",
       "<td>318.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2957195</td>\n",
       "<td>0.3838362</td>\n",
       "<td>158.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5573295</td>\n",
       "<td>0.8172181</td>\n",
       "<td>29.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7209391</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0079578</td>\n",
       "<td>1.0</td>\n",
       "<td>397.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.7209391</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2307897</td>\n",
       "<td>0.2508450</td>\n",
       "<td>202.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1789096</td>\n",
       "<td>0.6539910</td>\n",
       "<td>243.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1445658</td>\n",
       "<td>0.6589279</td>\n",
       "<td>272.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.193091     0.413929  232\n",
       "max f2                       0.0949289    0.575918  318\n",
       "max f0point5                 0.29572      0.383836  158\n",
       "max accuracy                 0.557329     0.817218  29\n",
       "max precision                0.720939     1         0\n",
       "max recall                   0.00795781   1         397\n",
       "max specificity              0.720939     1         0\n",
       "max absolute_mcc             0.23079      0.250845  202\n",
       "max min_per_class_accuracy   0.17891      0.653991  243\n",
       "max mean_per_class_accuracy  0.144566     0.658928  272"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.46 %, avg score: 17.98 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100059</td>\n",
       "<td>0.5604169</td>\n",
       "<td>3.0577637</td>\n",
       "<td>3.0577637</td>\n",
       "<td>0.5643564</td>\n",
       "<td>0.6072087</td>\n",
       "<td>0.5643564</td>\n",
       "<td>0.6072087</td>\n",
       "<td>0.0305958</td>\n",
       "<td>0.0305958</td>\n",
       "<td>205.7763747</td>\n",
       "<td>205.7763747</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200119</td>\n",
       "<td>0.5234729</td>\n",
       "<td>2.7358939</td>\n",
       "<td>2.8968288</td>\n",
       "<td>0.5049505</td>\n",
       "<td>0.5406898</td>\n",
       "<td>0.5346535</td>\n",
       "<td>0.5739492</td>\n",
       "<td>0.0273752</td>\n",
       "<td>0.0579710</td>\n",
       "<td>173.5893879</td>\n",
       "<td>189.6828813</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300178</td>\n",
       "<td>0.4885754</td>\n",
       "<td>2.4140240</td>\n",
       "<td>2.7358939</td>\n",
       "<td>0.4455446</td>\n",
       "<td>0.5057157</td>\n",
       "<td>0.5049505</td>\n",
       "<td>0.5512047</td>\n",
       "<td>0.0241546</td>\n",
       "<td>0.0821256</td>\n",
       "<td>141.4024011</td>\n",
       "<td>173.5893879</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400238</td>\n",
       "<td>0.4641976</td>\n",
       "<td>1.8775742</td>\n",
       "<td>2.5213140</td>\n",
       "<td>0.3465347</td>\n",
       "<td>0.4762840</td>\n",
       "<td>0.4653465</td>\n",
       "<td>0.5324745</td>\n",
       "<td>0.0187869</td>\n",
       "<td>0.1009125</td>\n",
       "<td>87.7574231</td>\n",
       "<td>152.1313967</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500297</td>\n",
       "<td>0.4449217</td>\n",
       "<td>2.2530891</td>\n",
       "<td>2.4676690</td>\n",
       "<td>0.4158416</td>\n",
       "<td>0.4553938</td>\n",
       "<td>0.4554455</td>\n",
       "<td>0.5170584</td>\n",
       "<td>0.0225443</td>\n",
       "<td>0.1234568</td>\n",
       "<td>125.3089077</td>\n",
       "<td>146.7668989</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000594</td>\n",
       "<td>0.3756950</td>\n",
       "<td>1.9741352</td>\n",
       "<td>2.2209021</td>\n",
       "<td>0.3643564</td>\n",
       "<td>0.4077109</td>\n",
       "<td>0.4099010</td>\n",
       "<td>0.4623846</td>\n",
       "<td>0.0987654</td>\n",
       "<td>0.2222222</td>\n",
       "<td>97.4135191</td>\n",
       "<td>122.0902090</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1499901</td>\n",
       "<td>0.3252005</td>\n",
       "<td>1.9350510</td>\n",
       "<td>2.1257443</td>\n",
       "<td>0.3571429</td>\n",
       "<td>0.3499487</td>\n",
       "<td>0.3923382</td>\n",
       "<td>0.4249555</td>\n",
       "<td>0.0966184</td>\n",
       "<td>0.3188406</td>\n",
       "<td>93.5050993</td>\n",
       "<td>112.5744261</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000198</td>\n",
       "<td>0.2843098</td>\n",
       "<td>1.7810133</td>\n",
       "<td>2.0395188</td>\n",
       "<td>0.3287129</td>\n",
       "<td>0.3037995</td>\n",
       "<td>0.3764240</td>\n",
       "<td>0.3946515</td>\n",
       "<td>0.0891036</td>\n",
       "<td>0.4079442</td>\n",
       "<td>78.1013270</td>\n",
       "<td>103.9518828</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999802</td>\n",
       "<td>0.2259268</td>\n",
       "<td>1.2941253</td>\n",
       "<td>1.7911364</td>\n",
       "<td>0.2388503</td>\n",
       "<td>0.2532546</td>\n",
       "<td>0.3305812</td>\n",
       "<td>0.3475348</td>\n",
       "<td>0.1293612</td>\n",
       "<td>0.5373054</td>\n",
       "<td>29.4125283</td>\n",
       "<td>79.1136368</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000396</td>\n",
       "<td>0.1798498</td>\n",
       "<td>1.1265445</td>\n",
       "<td>1.6249061</td>\n",
       "<td>0.2079208</td>\n",
       "<td>0.2015194</td>\n",
       "<td>0.2999009</td>\n",
       "<td>0.3110128</td>\n",
       "<td>0.1127214</td>\n",
       "<td>0.6500268</td>\n",
       "<td>12.6544539</td>\n",
       "<td>62.4906119</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1462399</td>\n",
       "<td>1.0739629</td>\n",
       "<td>1.5147611</td>\n",
       "<td>0.1982161</td>\n",
       "<td>0.1621809</td>\n",
       "<td>0.2795720</td>\n",
       "<td>0.2812583</td>\n",
       "<td>0.1073537</td>\n",
       "<td>0.7573806</td>\n",
       "<td>7.3962890</td>\n",
       "<td>51.4761138</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999604</td>\n",
       "<td>0.1157107</td>\n",
       "<td>0.7517740</td>\n",
       "<td>1.3876386</td>\n",
       "<td>0.1387512</td>\n",
       "<td>0.1305770</td>\n",
       "<td>0.2561096</td>\n",
       "<td>0.2561530</td>\n",
       "<td>0.0751476</td>\n",
       "<td>0.8325282</td>\n",
       "<td>-24.8225977</td>\n",
       "<td>38.7638615</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000198</td>\n",
       "<td>0.0901271</td>\n",
       "<td>0.6222817</td>\n",
       "<td>1.2782400</td>\n",
       "<td>0.1148515</td>\n",
       "<td>0.1026604</td>\n",
       "<td>0.2359185</td>\n",
       "<td>0.2342131</td>\n",
       "<td>0.0622652</td>\n",
       "<td>0.8947933</td>\n",
       "<td>-37.7718255</td>\n",
       "<td>27.8240025</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999802</td>\n",
       "<td>0.0649302</td>\n",
       "<td>0.4886531</td>\n",
       "<td>1.1795783</td>\n",
       "<td>0.0901883</td>\n",
       "<td>0.0772967</td>\n",
       "<td>0.2177090</td>\n",
       "<td>0.2146058</td>\n",
       "<td>0.0488459</td>\n",
       "<td>0.9436393</td>\n",
       "<td>-51.1346885</td>\n",
       "<td>17.9578329</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999406</td>\n",
       "<td>0.0413871</td>\n",
       "<td>0.3597776</td>\n",
       "<td>1.0885194</td>\n",
       "<td>0.0664024</td>\n",
       "<td>0.0532490</td>\n",
       "<td>0.2009027</td>\n",
       "<td>0.1966832</td>\n",
       "<td>0.0359635</td>\n",
       "<td>0.9796028</td>\n",
       "<td>-64.0222432</td>\n",
       "<td>8.8519438</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0001068</td>\n",
       "<td>0.2038509</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0376238</td>\n",
       "<td>0.0276929</td>\n",
       "<td>0.1845651</td>\n",
       "<td>0.1797741</td>\n",
       "<td>0.0203972</td>\n",
       "<td>1.0</td>\n",
       "<td>-79.6149084</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100059                   0.560417           3.05776   3.05776            0.564356         0.607209   0.564356                    0.607209            0.0305958       0.0305958                  205.776   205.776\n",
       "    2        0.0200119                   0.523473           2.73589   2.89683            0.50495          0.54069    0.534653                    0.573949            0.0273752       0.057971                   173.589   189.683\n",
       "    3        0.0300178                   0.488575           2.41402   2.73589            0.445545         0.505716   0.50495                     0.551205            0.0241546       0.0821256                  141.402   173.589\n",
       "    4        0.0400238                   0.464198           1.87757   2.52131            0.346535         0.476284   0.465347                    0.532475            0.0187869       0.100913                   87.7574   152.131\n",
       "    5        0.0500297                   0.444922           2.25309   2.46767            0.415842         0.455394   0.455446                    0.517058            0.0225443       0.123457                   125.309   146.767\n",
       "    6        0.100059                    0.375695           1.97414   2.2209             0.364356         0.407711   0.409901                    0.462385            0.0987654       0.222222                   97.4135   122.09\n",
       "    7        0.14999                     0.325201           1.93505   2.12574            0.357143         0.349949   0.392338                    0.424956            0.0966184       0.318841                   93.5051   112.574\n",
       "    8        0.20002                     0.28431            1.78101   2.03952            0.328713         0.3038     0.376424                    0.394652            0.0891036       0.407944                   78.1013   103.952\n",
       "    9        0.29998                     0.225927           1.29413   1.79114            0.23885          0.253255   0.330581                    0.347535            0.129361        0.537305                   29.4125   79.1136\n",
       "    10       0.40004                     0.17985            1.12654   1.62491            0.207921         0.201519   0.299901                    0.311013            0.112721        0.650027                   12.6545   62.4906\n",
       "    11       0.5                         0.14624            1.07396   1.51476            0.198216         0.162181   0.279572                    0.281258            0.107354        0.757381                   7.39629   51.4761\n",
       "    12       0.59996                     0.115711           0.751774  1.38764            0.138751         0.130577   0.25611                     0.256153            0.0751476       0.832528                   -24.8226  38.7639\n",
       "    13       0.70002                     0.0901271          0.622282  1.27824            0.114851         0.10266    0.235918                    0.234213            0.0622652       0.894793                   -37.7718  27.824\n",
       "    14       0.79998                     0.0649302          0.488653  1.17958            0.0901883        0.0772967  0.217709                    0.214606            0.0488459       0.943639                   -51.1347  17.9578\n",
       "    15       0.899941                    0.0413871          0.359778  1.08852            0.0664024        0.053249   0.200903                    0.196683            0.0359635       0.979603                   -64.0222  8.85194\n",
       "    16       1                           0.000106829        0.203851  1                  0.0376238        0.0276929  0.184565                    0.179774            0.0203972       1                          -79.6149  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.13879047534556194\n",
      "RMSE: 0.3725459372286348\n",
      "LogLoss: 0.44092351643259303\n",
      "Mean Per-Class Error: 0.35729763796421077\n",
      "AUC: 0.6967998458906275\n",
      "pr_auc: 0.33044848884930395\n",
      "Gini: 0.393599691781255\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.1811819503331: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>26221.0</td>\n",
       "<td>14034.0</td>\n",
       "<td>0.3486</td>\n",
       "<td> (14034.0/40255.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>3289.0</td>\n",
       "<td>5696.0</td>\n",
       "<td>0.3661</td>\n",
       "<td> (3289.0/8985.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>29510.0</td>\n",
       "<td>19730.0</td>\n",
       "<td>0.3518</td>\n",
       "<td> (17323.0/49240.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      26221  14034  0.3486   (14034.0/40255.0)\n",
       "1      3289   5696   0.3661   (3289.0/8985.0)\n",
       "Total  29510  19730  0.3518   (17323.0/49240.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1811820</td>\n",
       "<td>0.3967264</td>\n",
       "<td>244.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0848574</td>\n",
       "<td>0.5602304</td>\n",
       "<td>327.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3143748</td>\n",
       "<td>0.3642425</td>\n",
       "<td>153.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6323327</td>\n",
       "<td>0.8179732</td>\n",
       "<td>21.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8665182</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0042249</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8665182</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2264356</td>\n",
       "<td>0.2310770</td>\n",
       "<td>210.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1776821</td>\n",
       "<td>0.6416346</td>\n",
       "<td>247.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1792132</td>\n",
       "<td>0.6427024</td>\n",
       "<td>246.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.181182     0.396726  244\n",
       "max f2                       0.0848574    0.56023   327\n",
       "max f0point5                 0.314375     0.364242  153\n",
       "max accuracy                 0.632333     0.817973  21\n",
       "max precision                0.866518     1         0\n",
       "max recall                   0.00422494   1         398\n",
       "max specificity              0.866518     1         0\n",
       "max absolute_mcc             0.226436     0.231077  210\n",
       "max min_per_class_accuracy   0.177682     0.641635  247\n",
       "max mean_per_class_accuracy  0.179213     0.642702  246"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.25 %, avg score: 18.04 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100122</td>\n",
       "<td>0.5724630</td>\n",
       "<td>2.7456805</td>\n",
       "<td>2.7456805</td>\n",
       "<td>0.5010142</td>\n",
       "<td>0.6229843</td>\n",
       "<td>0.5010142</td>\n",
       "<td>0.6229843</td>\n",
       "<td>0.0274903</td>\n",
       "<td>0.0274903</td>\n",
       "<td>174.5680484</td>\n",
       "<td>174.5680484</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200041</td>\n",
       "<td>0.5273744</td>\n",
       "<td>2.5284870</td>\n",
       "<td>2.6371940</td>\n",
       "<td>0.4613821</td>\n",
       "<td>0.5469630</td>\n",
       "<td>0.4812183</td>\n",
       "<td>0.5850123</td>\n",
       "<td>0.0252643</td>\n",
       "<td>0.0527546</td>\n",
       "<td>152.8486954</td>\n",
       "<td>163.7193970</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300162</td>\n",
       "<td>0.4972833</td>\n",
       "<td>2.3788487</td>\n",
       "<td>2.5510206</td>\n",
       "<td>0.4340771</td>\n",
       "<td>0.5126340</td>\n",
       "<td>0.4654939</td>\n",
       "<td>0.5608698</td>\n",
       "<td>0.0238175</td>\n",
       "<td>0.0765721</td>\n",
       "<td>137.8848678</td>\n",
       "<td>155.1020608</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400081</td>\n",
       "<td>0.4731430</td>\n",
       "<td>2.1831870</td>\n",
       "<td>2.4591556</td>\n",
       "<td>0.3983740</td>\n",
       "<td>0.4851073</td>\n",
       "<td>0.4487310</td>\n",
       "<td>0.5419484</td>\n",
       "<td>0.0218141</td>\n",
       "<td>0.0983862</td>\n",
       "<td>118.3186974</td>\n",
       "<td>145.9155558</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.05</td>\n",
       "<td>0.4521931</td>\n",
       "<td>2.1609096</td>\n",
       "<td>2.3995548</td>\n",
       "<td>0.3943089</td>\n",
       "<td>0.4623610</td>\n",
       "<td>0.4378554</td>\n",
       "<td>0.5260439</td>\n",
       "<td>0.0215915</td>\n",
       "<td>0.1199777</td>\n",
       "<td>116.0909556</td>\n",
       "<td>139.9554814</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1</td>\n",
       "<td>0.3768911</td>\n",
       "<td>1.9855314</td>\n",
       "<td>2.1925431</td>\n",
       "<td>0.3623071</td>\n",
       "<td>0.4117142</td>\n",
       "<td>0.4000812</td>\n",
       "<td>0.4688790</td>\n",
       "<td>0.0992766</td>\n",
       "<td>0.2192543</td>\n",
       "<td>98.5531441</td>\n",
       "<td>119.2543127</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.15</td>\n",
       "<td>0.3235952</td>\n",
       "<td>1.8408459</td>\n",
       "<td>2.0753107</td>\n",
       "<td>0.3359058</td>\n",
       "<td>0.3488400</td>\n",
       "<td>0.3786894</td>\n",
       "<td>0.4288660</td>\n",
       "<td>0.0920423</td>\n",
       "<td>0.3112966</td>\n",
       "<td>84.0845854</td>\n",
       "<td>107.5310703</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2853326</td>\n",
       "<td>1.5047301</td>\n",
       "<td>1.9326656</td>\n",
       "<td>0.2745735</td>\n",
       "<td>0.3038265</td>\n",
       "<td>0.3526604</td>\n",
       "<td>0.3976061</td>\n",
       "<td>0.0752365</td>\n",
       "<td>0.3865331</td>\n",
       "<td>50.4730106</td>\n",
       "<td>93.2665554</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3</td>\n",
       "<td>0.2246803</td>\n",
       "<td>1.3689482</td>\n",
       "<td>1.7447598</td>\n",
       "<td>0.2497969</td>\n",
       "<td>0.2527313</td>\n",
       "<td>0.3183726</td>\n",
       "<td>0.3493145</td>\n",
       "<td>0.1368948</td>\n",
       "<td>0.5234279</td>\n",
       "<td>36.8948247</td>\n",
       "<td>74.4759785</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1808495</td>\n",
       "<td>1.0951586</td>\n",
       "<td>1.5823595</td>\n",
       "<td>0.1998375</td>\n",
       "<td>0.2017972</td>\n",
       "<td>0.2887388</td>\n",
       "<td>0.3124352</td>\n",
       "<td>0.1095159</td>\n",
       "<td>0.6329438</td>\n",
       "<td>9.5158598</td>\n",
       "<td>58.2359488</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1459258</td>\n",
       "<td>0.9348915</td>\n",
       "<td>1.4528659</td>\n",
       "<td>0.1705930</td>\n",
       "<td>0.1629274</td>\n",
       "<td>0.2651097</td>\n",
       "<td>0.2825336</td>\n",
       "<td>0.0934891</td>\n",
       "<td>0.7264329</td>\n",
       "<td>-6.5108514</td>\n",
       "<td>45.2865888</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1166097</td>\n",
       "<td>0.8380634</td>\n",
       "<td>1.3503988</td>\n",
       "<td>0.1529245</td>\n",
       "<td>0.1308124</td>\n",
       "<td>0.2464121</td>\n",
       "<td>0.2572468</td>\n",
       "<td>0.0838063</td>\n",
       "<td>0.8102393</td>\n",
       "<td>-16.1936561</td>\n",
       "<td>35.0398813</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7</td>\n",
       "<td>0.0902488</td>\n",
       "<td>0.6666667</td>\n",
       "<td>1.2527228</td>\n",
       "<td>0.1216491</td>\n",
       "<td>0.1032746</td>\n",
       "<td>0.2285888</td>\n",
       "<td>0.2352507</td>\n",
       "<td>0.0666667</td>\n",
       "<td>0.8769060</td>\n",
       "<td>-33.3333333</td>\n",
       "<td>25.2722792</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0645946</td>\n",
       "<td>0.5831942</td>\n",
       "<td>1.1690317</td>\n",
       "<td>0.1064175</td>\n",
       "<td>0.0774366</td>\n",
       "<td>0.2133174</td>\n",
       "<td>0.2155240</td>\n",
       "<td>0.0583194</td>\n",
       "<td>0.9352254</td>\n",
       "<td>-41.6805787</td>\n",
       "<td>16.9031720</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9</td>\n",
       "<td>0.0405852</td>\n",
       "<td>0.4207012</td>\n",
       "<td>1.0858839</td>\n",
       "<td>0.0767669</td>\n",
       "<td>0.0523873</td>\n",
       "<td>0.1981451</td>\n",
       "<td>0.1973977</td>\n",
       "<td>0.0420701</td>\n",
       "<td>0.9772955</td>\n",
       "<td>-57.9298831</td>\n",
       "<td>8.5883881</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000008</td>\n",
       "<td>0.2270451</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0414297</td>\n",
       "<td>0.0270958</td>\n",
       "<td>0.1824736</td>\n",
       "<td>0.1803675</td>\n",
       "<td>0.0227045</td>\n",
       "<td>1.0</td>\n",
       "<td>-77.2954925</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100122                   0.572463           2.74568   2.74568            0.501014         0.622984   0.501014                    0.622984            0.0274903       0.0274903                  174.568   174.568\n",
       "    2        0.0200041                   0.527374           2.52849   2.63719            0.461382         0.546963   0.481218                    0.585012            0.0252643       0.0527546                  152.849   163.719\n",
       "    3        0.0300162                   0.497283           2.37885   2.55102            0.434077         0.512634   0.465494                    0.56087             0.0238175       0.0765721                  137.885   155.102\n",
       "    4        0.0400081                   0.473143           2.18319   2.45916            0.398374         0.485107   0.448731                    0.541948            0.0218141       0.0983862                  118.319   145.916\n",
       "    5        0.05                        0.452193           2.16091   2.39955            0.394309         0.462361   0.437855                    0.526044            0.0215915       0.119978                   116.091   139.955\n",
       "    6        0.1                         0.376891           1.98553   2.19254            0.362307         0.411714   0.400081                    0.468879            0.0992766       0.219254                   98.5531   119.254\n",
       "    7        0.15                        0.323595           1.84085   2.07531            0.335906         0.34884    0.378689                    0.428866            0.0920423       0.311297                   84.0846   107.531\n",
       "    8        0.2                         0.285333           1.50473   1.93267            0.274574         0.303826   0.35266                     0.397606            0.0752365       0.386533                   50.473    93.2666\n",
       "    9        0.3                         0.22468            1.36895   1.74476            0.249797         0.252731   0.318373                    0.349315            0.136895        0.523428                   36.8948   74.476\n",
       "    10       0.4                         0.180849           1.09516   1.58236            0.199838         0.201797   0.288739                    0.312435            0.109516        0.632944                   9.51586   58.2359\n",
       "    11       0.5                         0.145926           0.934891  1.45287            0.170593         0.162927   0.26511                     0.282534            0.0934891       0.726433                   -6.51085  45.2866\n",
       "    12       0.6                         0.11661            0.838063  1.3504             0.152924         0.130812   0.246412                    0.257247            0.0838063       0.810239                   -16.1937  35.0399\n",
       "    13       0.7                         0.0902488          0.666667  1.25272            0.121649         0.103275   0.228589                    0.235251            0.0666667       0.876906                   -33.3333  25.2723\n",
       "    14       0.8                         0.0645946          0.583194  1.16903            0.106418         0.0774366  0.213317                    0.215524            0.0583194       0.935225                   -41.6806  16.9032\n",
       "    15       0.9                         0.0405852          0.420701  1.08588            0.0767669        0.0523873  0.198145                    0.197398            0.0420701       0.977295                   -57.9299  8.58839\n",
       "    16       1                           8.19897e-07        0.227045  1                  0.0414297        0.0270958  0.182474                    0.180367            0.0227045       1                          -77.2955  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>training_speed</b></td>\n",
       "<td><b>epochs</b></td>\n",
       "<td><b>iterations</b></td>\n",
       "<td><b>samples</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_r2</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_r2</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:07:46</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>None</td>\n",
       "<td>0.0</td>\n",
       "<td>0</td>\n",
       "<td>0.0</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:07:47</td>\n",
       "<td>48.439 sec</td>\n",
       "<td>72987 obs/sec</td>\n",
       "<td>1.0</td>\n",
       "<td>1</td>\n",
       "<td>65689.0</td>\n",
       "<td>0.3805142</td>\n",
       "<td>0.4539653</td>\n",
       "<td>0.0379383</td>\n",
       "<td>0.6921919</td>\n",
       "<td>0.3235207</td>\n",
       "<td>2.8431838</td>\n",
       "<td>0.3452546</td>\n",
       "<td>0.3810818</td>\n",
       "<td>0.4565396</td>\n",
       "<td>0.0265030</td>\n",
       "<td>0.6818557</td>\n",
       "<td>0.3159448</td>\n",
       "<td>2.5900549</td>\n",
       "<td>0.3565800</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-01-22 22:07:51</td>\n",
       "<td>52.769 sec</td>\n",
       "<td>131430 obs/sec</td>\n",
       "<td>10.0</td>\n",
       "<td>10</td>\n",
       "<td>656890.0</td>\n",
       "<td>0.3706103</td>\n",
       "<td>0.4349174</td>\n",
       "<td>0.0873671</td>\n",
       "<td>0.7145816</td>\n",
       "<td>0.3537231</td>\n",
       "<td>3.0577637</td>\n",
       "<td>0.3251436</td>\n",
       "<td>0.3725459</td>\n",
       "<td>0.4409235</td>\n",
       "<td>0.0696254</td>\n",
       "<td>0.6967998</td>\n",
       "<td>0.3304485</td>\n",
       "<td>2.7456805</td>\n",
       "<td>0.3518075</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    training_speed    epochs    iterations    samples    training_rmse    training_logloss    training_r2    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_r2    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  ----------------  --------  ------------  ---------  ---------------  ------------------  -------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ---------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2019-01-22 22:07:46  0.000 sec                     0         0             0          nan              nan                 nan            nan             nan                nan              nan                              nan                nan                   nan              nan               nan                  nan                nan\n",
       "    2019-01-22 22:07:47  48.439 sec  72987 obs/sec     1         1             65689      0.380514         0.453965            0.0379383      0.692192        0.323521           2.84318          0.345255                         0.381082           0.45654               0.026503         0.681856          0.315945             2.59005            0.35658\n",
       "    2019-01-22 22:07:51  52.769 sec  131430 obs/sec    10        10            656890     0.37061          0.434917            0.0873671      0.714582        0.353723           3.05776          0.325144                         0.372546           0.440924              0.0696254        0.6968            0.330448             2.74568            0.351807"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>addr_state.MO</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0151903</td></tr>\n",
       "<tr><td>purpose.small_business</td>\n",
       "<td>0.9904335</td>\n",
       "<td>0.9904335</td>\n",
       "<td>0.0150450</td></tr>\n",
       "<tr><td>addr_state.TN</td>\n",
       "<td>0.9665326</td>\n",
       "<td>0.9665326</td>\n",
       "<td>0.0146819</td></tr>\n",
       "<tr><td>purpose.renewable_energy</td>\n",
       "<td>0.9472735</td>\n",
       "<td>0.9472735</td>\n",
       "<td>0.0143894</td></tr>\n",
       "<tr><td>addr_state.DC</td>\n",
       "<td>0.9444687</td>\n",
       "<td>0.9444687</td>\n",
       "<td>0.0143468</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>addr_state.missing(NA)</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>purpose.missing(NA)</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>home_ownership.missing(NA)</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>term.missing(NA)</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>verification_status.missing(NA)</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "variable                         relative_importance    scaled_importance    percentage\n",
       "-------------------------------  ---------------------  -------------------  --------------------\n",
       "addr_state.MO                    1.0                    1.0                  0.015190305352325678\n",
       "purpose.small_business           0.9904335141181946     0.9904335141181946   0.015044987510632341\n",
       "addr_state.TN                    0.9665326476097107     0.9665326476097107   0.014681926050183296\n",
       "purpose.renewable_energy         0.9472734928131104     0.9472734928131104   0.01438937360799523\n",
       "addr_state.DC                    0.9444687366485596     0.9444687366485596   0.014346768505416886\n",
       "---                              ---                    ---                  ---\n",
       "addr_state.missing(NA)           0.0                    0.0                  0.0\n",
       "purpose.missing(NA)              0.0                    0.0                  0.0\n",
       "home_ownership.missing(NA)       0.0                    0.0                  0.0\n",
       "term.missing(NA)                 0.0                    0.0                  0.0\n",
       "verification_status.missing(NA)  0.0                    0.0                  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show grid search results\n",
    "gsearch.show()\n",
    "\n",
    "# select best model\n",
    "nn_model2 = gsearch.get_grid()[0]\n",
    "\n",
    "# print model information\n",
    "nn_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7145815998888247\n",
      "0.6967998458906275\n",
      "0.7041325607237255\n"
     ]
    }
   ],
   "source": [
    "# measure nn AUC\n",
    "print(nn_model2.auc(train=True))\n",
    "print(nn_model2.auc(valid=True))\n",
    "print(nn_model2.model_performance(test_data=test).auc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial dependence plots \n",
    "\n",
    "The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model. A partial dependence plot can show whether the relationship between the target and a feature is linear, monotonous or more complex.\n",
    "\n",
    "[https://christophm.github.io/interpretable-ml-book/pdp.html](https://christophm.github.io/interpretable-ml-book/pdp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PartialDependencePlot progress: |█████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAALWCAYAAACnePHjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 504x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# partial dependence plots are a powerful machine learning interpretation tool\n",
    "# to calculate partial dependence across the domain a variable\n",
    "# hold column of interest at constant value\n",
    "# find the mean prediction of the model with this column constant\n",
    "# repeat for multiple values of the variable of interest\n",
    "# h2o has a built-in function for partial dependence as well\n",
    "par_dep_dti1 = nn_model2.partial_plot(data=train, cols=['dti'], server=True, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_b0d1 closed.\n"
     ]
    }
   ],
   "source": [
    "# shutdown h2o\n",
    "# be careful ... this can erase your work\n",
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
