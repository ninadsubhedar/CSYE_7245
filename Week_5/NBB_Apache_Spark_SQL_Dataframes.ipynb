{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark SQL Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make plots larger\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark # Test that pyspark is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.107.76.125:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL, DataFrames and Datasets Guide\n",
    "\n",
    "The [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html) is at https://spark.apache.org/docs/latest/sql-programming-guide.html \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL and Dataframes\") \\\n",
    "    .config(\"spark.some.config.option\", \"spark-sql\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  a|\n",
      "|  1|  b|\n",
      "|  2|  c|\n",
      "|  3|  d|\n",
      "|  4|  5|\n",
      "|  0|  e|\n",
      "|  3|  b|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(0,'a'),(1,'b'),(2,'c'),(3,'d'),(4,'5'),(0,'e'),(3,'b')])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  0|  a|\n",
      "|  1|  b|\n",
      "|  2|  c|\n",
      "|  3|  d|\n",
      "|  4|  5|\n",
      "|  0|  e|\n",
      "|  3|  b|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(0,'a'),(1,'b'),(2,'c'),(3,'d'),(4,'5'),(0,'e'),(3,'b')],['A','B'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|      date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+----------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2016-07-12|    0|VfBHSwC5Vz_pbFluy...|    5|My girlfriend and...|     0|cjpdDjZyprfyDG3Rl...|\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2016-10-02|    0|3zRpneRKDsOPq92tq...|    3|If you need an in...|     0|bjTcT8Ty4cJZhEOEo...|\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2015-09-17|    0|ne5WhI1jUFOcRn-b-...|    3|Mittlerweile gibt...|     0|AXgRULmWcME7J6Ix3...|\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2016-08-21|    0|llmdwOgDReucVoWEr...|    4|Location is every...|     0|oU2SSOmsp_A8JYI7Z...|\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2013-11-20|    0|DuffS87NaSMDmIflu...|    5|gute lage im stad...|     0|0xtbPEna2Kei11vsU...|\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2016-06-05|    0|GvLmUkjUrOyFH8KFn...|    5|Erstklassige Lage...|     0|rW8q706dz5-NnXDzM...|\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2015-02-21|    0|lGEl24NGj2HVBJrod...|    4|Beautiful space, ...|     0|yx8vNXUL0D0HS8rUI...|\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2013-07-07|    0|cUgvEy5wj7zYE68v1...|    4|This is a fairly ...|     0|zXnH6W74FAJQ7q7b-...|\n",
      "|uYHaNptLzDLoV_JZ_...|   2|2013-04-27|    0|FSB_BnvysBgH3JYrb...|    4|First time at thi...|     2|c5yp5hxwC1N98MjbV...|\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2015-04-13|    0|dhl3ZW9aAEX_T7_um...|    4|Location location...|     2|xJisL5w4wOgiYLokG...|\n",
      "|uYHaNptLzDLoV_JZ_...|   1|2016-11-08|    1|JQJvnM3p-3eML05eK...|    4|A hotel that has ...|     1|tgV6tsYQ66DZ3LQKv...|\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2015-07-27|    0|6JF4WfHgwYrrdZ2Ve...|    3|Stayed here for t...|     1|Q-3YCVywc03w56wYt...|\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2014-05-07|    0|fbVYETRuWDw8Qnpim...|    4|Well, i like the ...|     1|Cx4UCow0zQgFQOp47...|\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2015-02-26|    0|lobj38NgaokqVseN8...|    4|I really do love ...|     1|eqWEgMH-DCP74i82B...|\n",
      "|uYHaNptLzDLoV_JZ_...|   1|2015-08-21|    0|ysfjtAWreLoy7um8W...|    5|Motel One sets th...|     1|d0DGZRp6lHXGECJSc...|\n",
      "|uYHaNptLzDLoV_JZ_...|   0|2013-12-07|    0|OF1ToqGAubsWGri5i...|    3|Had Continental s...|     1|IpLZ7RevQrFPJWYc9...|\n",
      "|jQsNFOzDpxPmOurSW...|   0|2017-06-03|    0|ByRzJ8rF2KJWLr-cU...|    1|This place is hor...|     0|kzyLOqiJvyw_FWFTw...|\n",
      "|jQsNFOzDpxPmOurSW...|   0|2015-03-26|    0|i5UwUPlQFPLcE8p2g...|    4|For being fairly ...|     1|WZXp9-V2dqRRJqhGg...|\n",
      "|jQsNFOzDpxPmOurSW...|   1|2012-12-30|    1|EyQyvTTg2jX4or9bB...|    5|I decided to try ...|     2|XylT12exfdLiI_3uD...|\n",
      "|jQsNFOzDpxPmOurSW...|   1|2009-01-12|    1|G-EFA005besj5uHsH...|    3|I'm not saying Pe...|     1|Ji9PeffxjwqPLO7pE...|\n",
      "+--------------------+----+----------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "reviews = spark.read.json(\"data/review.3333.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Craft Beers Dataset\n",
    "\n",
    "Information on 2K+ craft canned beers from the US and 500+ breweries in the United States.\n",
    "\n",
    "from [https://github.com/nickhould/craft-beers-dataset](https://github.com/nickhould/craft-beers-dataset)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     _corrupt_record|\n",
      "+--------------------+\n",
      "|  id,name,city,state|\n",
      "|0,NorthGate Brewi...|\n",
      "|1,Against the Gra...|\n",
      "|2,Jack's Abby Cra...|\n",
      "|3,Mike Hess Brewi...|\n",
      "|4,Fort Point Beer...|\n",
      "|5,COAST Brewing C...|\n",
      "|6,Great Divide Br...|\n",
      "|7,Tapistry Brewin...|\n",
      "|8,Big Lake Brewin...|\n",
      "|9,The Mitten Brew...|\n",
      "|10,Brewery Vivant...|\n",
      "|11,Petoskey Brewi...|\n",
      "|12,Blackrocks Bre...|\n",
      "|13,Perrin Brewing...|\n",
      "|14,Witch's Hat Br...|\n",
      "|15,Founders Brewi...|\n",
      "|16,Flat 12 Bierwe...|\n",
      "|17,Tin Man Brewin...|\n",
      "|18,Black Acre Bre...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "breweries = spark.read.json(\"data/breweries.csv\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "breweries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "breweries.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _corrupt_record: string \n",
    "\n",
    "Sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------+-----+\n",
      "|_c0|                 _c1|          _c2|  _c3|\n",
      "+---+--------------------+-------------+-----+\n",
      "| id|                name|         city|state|\n",
      "|  0|  NorthGate Brewing |  Minneapolis|   MN|\n",
      "|  1|Against the Grain...|   Louisville|   KY|\n",
      "|  2|Jack's Abby Craft...|   Framingham|   MA|\n",
      "|  3|Mike Hess Brewing...|    San Diego|   CA|\n",
      "|  4|Fort Point Beer C...|San Francisco|   CA|\n",
      "|  5|COAST Brewing Com...|   Charleston|   SC|\n",
      "|  6|Great Divide Brew...|       Denver|   CO|\n",
      "|  7|    Tapistry Brewing|     Bridgman|   MI|\n",
      "|  8|    Big Lake Brewing|      Holland|   MI|\n",
      "|  9|The Mitten Brewin...| Grand Rapids|   MI|\n",
      "| 10|      Brewery Vivant| Grand Rapids|   MI|\n",
      "| 11|    Petoskey Brewing|     Petoskey|   MI|\n",
      "| 12|  Blackrocks Brewery|    Marquette|   MI|\n",
      "| 13|Perrin Brewing Co...|Comstock Park|   MI|\n",
      "| 14|Witch's Hat Brewi...|   South Lyon|   MI|\n",
      "| 15|Founders Brewing ...| Grand Rapids|   MI|\n",
      "| 16|   Flat 12 Bierwerks| Indianapolis|   IN|\n",
      "| 17|Tin Man Brewing C...|   Evansville|   IN|\n",
      "| 18|Black Acre Brewin...| Indianapolis|   IN|\n",
      "+---+--------------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "breweries = spark.read.csv(\"data/breweries.csv\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "breweries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### header=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------+-----+\n",
      "| id|                name|         city|state|\n",
      "+---+--------------------+-------------+-----+\n",
      "|  0|  NorthGate Brewing |  Minneapolis|   MN|\n",
      "|  1|Against the Grain...|   Louisville|   KY|\n",
      "|  2|Jack's Abby Craft...|   Framingham|   MA|\n",
      "|  3|Mike Hess Brewing...|    San Diego|   CA|\n",
      "|  4|Fort Point Beer C...|San Francisco|   CA|\n",
      "|  5|COAST Brewing Com...|   Charleston|   SC|\n",
      "|  6|Great Divide Brew...|       Denver|   CO|\n",
      "|  7|    Tapistry Brewing|     Bridgman|   MI|\n",
      "|  8|    Big Lake Brewing|      Holland|   MI|\n",
      "|  9|The Mitten Brewin...| Grand Rapids|   MI|\n",
      "| 10|      Brewery Vivant| Grand Rapids|   MI|\n",
      "| 11|    Petoskey Brewing|     Petoskey|   MI|\n",
      "| 12|  Blackrocks Brewery|    Marquette|   MI|\n",
      "| 13|Perrin Brewing Co...|Comstock Park|   MI|\n",
      "| 14|Witch's Hat Brewi...|   South Lyon|   MI|\n",
      "| 15|Founders Brewing ...| Grand Rapids|   MI|\n",
      "| 16|   Flat 12 Bierwerks| Indianapolis|   IN|\n",
      "| 17|Tin Man Brewing C...|   Evansville|   IN|\n",
      "| 18|Black Acre Brewin...| Indianapolis|   IN|\n",
      "| 19|   Brew Link Brewing|   Plainfield|   IN|\n",
      "+---+--------------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "breweries = spark.read.csv(\"data/breweries.csv\", header=True)\n",
    "# Displays the content of the DataFrame to stdout\n",
    "breweries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "breweries.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inferSchema=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------+-----+\n",
      "| id|                name|         city|state|\n",
      "+---+--------------------+-------------+-----+\n",
      "|  0|  NorthGate Brewing |  Minneapolis|   MN|\n",
      "|  1|Against the Grain...|   Louisville|   KY|\n",
      "|  2|Jack's Abby Craft...|   Framingham|   MA|\n",
      "|  3|Mike Hess Brewing...|    San Diego|   CA|\n",
      "|  4|Fort Point Beer C...|San Francisco|   CA|\n",
      "|  5|COAST Brewing Com...|   Charleston|   SC|\n",
      "|  6|Great Divide Brew...|       Denver|   CO|\n",
      "|  7|    Tapistry Brewing|     Bridgman|   MI|\n",
      "|  8|    Big Lake Brewing|      Holland|   MI|\n",
      "|  9|The Mitten Brewin...| Grand Rapids|   MI|\n",
      "| 10|      Brewery Vivant| Grand Rapids|   MI|\n",
      "| 11|    Petoskey Brewing|     Petoskey|   MI|\n",
      "| 12|  Blackrocks Brewery|    Marquette|   MI|\n",
      "| 13|Perrin Brewing Co...|Comstock Park|   MI|\n",
      "| 14|Witch's Hat Brewi...|   South Lyon|   MI|\n",
      "| 15|Founders Brewing ...| Grand Rapids|   MI|\n",
      "| 16|   Flat 12 Bierwerks| Indianapolis|   IN|\n",
      "| 17|Tin Man Brewing C...|   Evansville|   IN|\n",
      "| 18|Black Acre Brewin...| Indianapolis|   IN|\n",
      "| 19|   Brew Link Brewing|   Plainfield|   IN|\n",
      "+---+--------------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "breweries = spark.read.csv(\"data/breweries.csv\", header=True, inferSchema=True)\n",
    "# Displays the content of the DataFrame to stdout\n",
    "breweries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "breweries.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'city', 'state']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breweries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, id: string, name: string, city: string, state: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breweries.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------+-----+\n",
      "|summary|                id|                name|    city|state|\n",
      "+-------+------------------+--------------------+--------+-----+\n",
      "|  count|               558|                 558|     558|  558|\n",
      "|   mean|             278.5|                null|    null| null|\n",
      "| stddev|161.22499806171498|                null|    null| null|\n",
      "|    min|                 0|10 Barrel Brewing...|Abingdon|   AK|\n",
      "|    max|               557|Wynkoop Brewing C...|    York|   WY|\n",
      "+-------+------------------+--------------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "breweries.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'name'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breweries['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(breweries['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|  NorthGate Brewing |\n",
      "|Against the Grain...|\n",
      "|Jack's Abby Craft...|\n",
      "|Mike Hess Brewing...|\n",
      "|Fort Point Beer C...|\n",
      "|COAST Brewing Com...|\n",
      "|Great Divide Brew...|\n",
      "|    Tapistry Brewing|\n",
      "|    Big Lake Brewing|\n",
      "|The Mitten Brewin...|\n",
      "|      Brewery Vivant|\n",
      "|    Petoskey Brewing|\n",
      "|  Blackrocks Brewery|\n",
      "|Perrin Brewing Co...|\n",
      "|Witch's Hat Brewi...|\n",
      "|Founders Brewing ...|\n",
      "|   Flat 12 Bierwerks|\n",
      "|Tin Man Brewing C...|\n",
      "|Black Acre Brewin...|\n",
      "|   Brew Link Brewing|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "breweries.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                name|state|\n",
      "+--------------------+-----+\n",
      "|  NorthGate Brewing |   MN|\n",
      "|Against the Grain...|   KY|\n",
      "|Jack's Abby Craft...|   MA|\n",
      "|Mike Hess Brewing...|   CA|\n",
      "|Fort Point Beer C...|   CA|\n",
      "|COAST Brewing Com...|   SC|\n",
      "|Great Divide Brew...|   CO|\n",
      "|    Tapistry Brewing|   MI|\n",
      "|    Big Lake Brewing|   MI|\n",
      "|The Mitten Brewin...|   MI|\n",
      "|      Brewery Vivant|   MI|\n",
      "|    Petoskey Brewing|   MI|\n",
      "|  Blackrocks Brewery|   MI|\n",
      "|Perrin Brewing Co...|   MI|\n",
      "|Witch's Hat Brewi...|   MI|\n",
      "|Founders Brewing ...|   MI|\n",
      "|   Flat 12 Bierwerks|   IN|\n",
      "|Tin Man Brewing C...|   IN|\n",
      "|Black Acre Brewin...|   IN|\n",
      "|   Brew Link Brewing|   IN|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "breweries.select(['name','state']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(breweries.select('name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, name='NorthGate Brewing ', city='Minneapolis', state=' MN'),\n",
       " Row(id=1, name='Against the Grain Brewery', city='Louisville', state=' KY'),\n",
       " Row(id=2, name=\"Jack's Abby Craft Lagers\", city='Framingham', state=' MA')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breweries.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------+-----+---+\n",
      "| id|                name|         city|state|id2|\n",
      "+---+--------------------+-------------+-----+---+\n",
      "|  0|  NorthGate Brewing |  Minneapolis|   MN|  0|\n",
      "|  1|Against the Grain...|   Louisville|   KY|  2|\n",
      "|  2|Jack's Abby Craft...|   Framingham|   MA|  4|\n",
      "|  3|Mike Hess Brewing...|    San Diego|   CA|  6|\n",
      "|  4|Fort Point Beer C...|San Francisco|   CA|  8|\n",
      "|  5|COAST Brewing Com...|   Charleston|   SC| 10|\n",
      "|  6|Great Divide Brew...|       Denver|   CO| 12|\n",
      "|  7|    Tapistry Brewing|     Bridgman|   MI| 14|\n",
      "|  8|    Big Lake Brewing|      Holland|   MI| 16|\n",
      "|  9|The Mitten Brewin...| Grand Rapids|   MI| 18|\n",
      "| 10|      Brewery Vivant| Grand Rapids|   MI| 20|\n",
      "| 11|    Petoskey Brewing|     Petoskey|   MI| 22|\n",
      "| 12|  Blackrocks Brewery|    Marquette|   MI| 24|\n",
      "| 13|Perrin Brewing Co...|Comstock Park|   MI| 26|\n",
      "| 14|Witch's Hat Brewi...|   South Lyon|   MI| 28|\n",
      "| 15|Founders Brewing ...| Grand Rapids|   MI| 30|\n",
      "| 16|   Flat 12 Bierwerks| Indianapolis|   IN| 32|\n",
      "| 17|Tin Man Brewing C...|   Evansville|   IN| 34|\n",
      "| 18|Black Acre Brewin...| Indianapolis|   IN| 36|\n",
      "| 19|   Brew Link Brewing|   Plainfield|   IN| 38|\n",
      "+---+--------------------+-------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "breweries.withColumn('id2', breweries['id']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breweries.createOrReplaceTempView('brew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_brew=spark.sql(\"SELECT * FROM brew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, name: string, city: string, state: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_brew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------+-----+\n",
      "| id|                name|         city|state|\n",
      "+---+--------------------+-------------+-----+\n",
      "|  0|  NorthGate Brewing |  Minneapolis|   MN|\n",
      "|  1|Against the Grain...|   Louisville|   KY|\n",
      "|  2|Jack's Abby Craft...|   Framingham|   MA|\n",
      "|  3|Mike Hess Brewing...|    San Diego|   CA|\n",
      "|  4|Fort Point Beer C...|San Francisco|   CA|\n",
      "|  5|COAST Brewing Com...|   Charleston|   SC|\n",
      "|  6|Great Divide Brew...|       Denver|   CO|\n",
      "|  7|    Tapistry Brewing|     Bridgman|   MI|\n",
      "|  8|    Big Lake Brewing|      Holland|   MI|\n",
      "|  9|The Mitten Brewin...| Grand Rapids|   MI|\n",
      "| 10|      Brewery Vivant| Grand Rapids|   MI|\n",
      "| 11|    Petoskey Brewing|     Petoskey|   MI|\n",
      "| 12|  Blackrocks Brewery|    Marquette|   MI|\n",
      "| 13|Perrin Brewing Co...|Comstock Park|   MI|\n",
      "| 14|Witch's Hat Brewi...|   South Lyon|   MI|\n",
      "| 15|Founders Brewing ...| Grand Rapids|   MI|\n",
      "| 16|   Flat 12 Bierwerks| Indianapolis|   IN|\n",
      "| 17|Tin Man Brewing C...|   Evansville|   IN|\n",
      "| 18|Black Acre Brewin...| Indianapolis|   IN|\n",
      "| 19|   Brew Link Brewing|   Plainfield|   IN|\n",
      "+---+--------------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_brew.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon stock data\n",
    "\n",
    "Download Amazon stock data from \n",
    "https://www.google.com/finance/historical?output=csv&q=AMZN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+-------+--------+\n",
      "|     Date|   Open|   High|    Low|  Close|  Volume|\n",
      "+---------+-------+-------+-------+-------+--------+\n",
      "|30-Oct-17|1095.01|1122.79|1093.56|1110.85| 6613064|\n",
      "|27-Oct-17|1058.14|1105.58|1050.55|1100.95|16565021|\n",
      "|26-Oct-17| 980.33|  982.9| 968.55| 972.43| 5618675|\n",
      "|25-Oct-17|  978.0| 984.44| 966.24| 972.91| 3033113|\n",
      "|24-Oct-17|  969.0| 979.85|  965.0|  975.9| 2723935|\n",
      "|23-Oct-17| 986.73| 986.78|  962.5|  966.3| 3494100|\n",
      "|20-Oct-17| 993.53| 994.62|  982.0| 982.91| 2365122|\n",
      "|19-Oct-17|  990.0| 991.05| 980.24| 986.61| 3108197|\n",
      "|18-Oct-17|1009.27|1022.31| 996.55|  997.0| 2499681|\n",
      "|17-Oct-17|1005.59|1011.47|1004.38|1009.13| 2319742|\n",
      "|16-Oct-17|1008.44|1009.57|1001.04|1006.34| 2008908|\n",
      "|13-Oct-17| 1007.0|1007.77|1001.03|1002.94| 2431462|\n",
      "|12-Oct-17| 996.81|1008.44|  992.4|1000.93| 4067317|\n",
      "|11-Oct-17| 991.27|  995.5|  986.7|  995.0| 2337113|\n",
      "|10-Oct-17| 996.67| 997.95|  980.1|  987.2| 3084921|\n",
      "| 9-Oct-17| 993.24|  998.5|  987.5| 990.99| 2938586|\n",
      "| 6-Oct-17| 975.64| 995.75| 975.64| 989.58| 3782067|\n",
      "| 5-Oct-17|  970.0| 981.51| 969.64| 980.85| 3229224|\n",
      "| 4-Oct-17| 954.21| 967.79| 954.05| 965.45| 2527352|\n",
      "| 3-Oct-17|  958.0| 963.69| 950.37|  957.1| 2666574|\n",
      "+---------+-------+-------+-------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "amzn = spark.read.csv(\"data/amzn.csv\", header=True, inferSchema=True)\n",
    "# Displays the content of the DataFrame to stdout\n",
    "amzn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amzn.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------------+-----------------+-----------------+-----------------+------------------+\n",
      "|summary|    Date|             Open|             High|              Low|            Close|            Volume|\n",
      "+-------+--------+-----------------+-----------------+-----------------+-----------------+------------------+\n",
      "|  count|     251|              251|              251|              251|              251|               251|\n",
      "|   mean|    null|903.5337051792826|909.3869322709156|895.9290039840638|902.9948605577688| 3698133.752988048|\n",
      "| stddev|    null|91.30254956909363|91.93640267354058|90.61454580035145|91.25053719228444|1819253.7805759213|\n",
      "|    min|1-Aug-17|            730.0|           743.26|            710.1|           719.07|           1837068|\n",
      "|    max|9-Oct-17|          1095.01|          1122.79|          1093.56|          1110.85|          16565021|\n",
      "+-------+--------+-----------------+-----------------+-----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amzn.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+\n",
      "|     Date|  Open|  High|   Low| Close| Volume|\n",
      "+---------+------+------+------+------+-------+\n",
      "|11-Jan-17|793.66| 799.5|789.51|799.02|2992791|\n",
      "|10-Jan-17| 796.6| 798.0|789.54| 795.9|2558369|\n",
      "| 9-Jan-17| 798.0|801.77|791.77|796.92|3446109|\n",
      "| 6-Jan-17|782.36|799.44|778.48|795.99|5986234|\n",
      "| 5-Jan-17|761.55| 782.4|760.26|780.45|5830068|\n",
      "| 4-Jan-17|758.39|759.68| 754.2|757.18|2510526|\n",
      "| 3-Jan-17|757.92|758.76| 747.7|753.67|3521066|\n",
      "|30-Dec-16|766.47| 767.4|748.28|749.87|4139449|\n",
      "|29-Dec-16| 772.4| 773.4|760.85|765.15|3158299|\n",
      "|28-Dec-16|776.25| 780.0| 770.5|772.13|3301025|\n",
      "|27-Dec-16| 763.4|774.65| 761.2| 771.4|2638725|\n",
      "|23-Dec-16|764.55| 766.5|757.99|760.59|1981616|\n",
      "|22-Dec-16|768.12|771.21|763.02|766.34|2543551|\n",
      "|21-Dec-16| 770.0|771.22| 765.7| 770.6|2044629|\n",
      "|20-Dec-16|768.65|774.39|767.71|771.22|2703629|\n",
      "|19-Dec-16|758.89| 770.5|756.16| 766.0|3113240|\n",
      "|16-Dec-16| 765.0|765.13| 754.0|757.77|4848219|\n",
      "|15-Dec-16|766.28| 769.1|760.31| 761.0|3801927|\n",
      "|14-Dec-16|778.25|780.86|762.81|768.82|5454836|\n",
      "|13-Dec-16|764.96|782.46| 762.0|774.34|5285288|\n",
      "+---------+------+------+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amzn.filter('Close < 800').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+\n",
      "|     Date|  Open|  High|   Low| Close| Volume|\n",
      "+---------+------+------+------+------+-------+\n",
      "|11-Jan-17|793.66| 799.5|789.51|799.02|2992791|\n",
      "|10-Jan-17| 796.6| 798.0|789.54| 795.9|2558369|\n",
      "| 9-Jan-17| 798.0|801.77|791.77|796.92|3446109|\n",
      "| 6-Jan-17|782.36|799.44|778.48|795.99|5986234|\n",
      "| 5-Jan-17|761.55| 782.4|760.26|780.45|5830068|\n",
      "| 4-Jan-17|758.39|759.68| 754.2|757.18|2510526|\n",
      "| 3-Jan-17|757.92|758.76| 747.7|753.67|3521066|\n",
      "|30-Dec-16|766.47| 767.4|748.28|749.87|4139449|\n",
      "|29-Dec-16| 772.4| 773.4|760.85|765.15|3158299|\n",
      "|28-Dec-16|776.25| 780.0| 770.5|772.13|3301025|\n",
      "|27-Dec-16| 763.4|774.65| 761.2| 771.4|2638725|\n",
      "|23-Dec-16|764.55| 766.5|757.99|760.59|1981616|\n",
      "|22-Dec-16|768.12|771.21|763.02|766.34|2543551|\n",
      "|21-Dec-16| 770.0|771.22| 765.7| 770.6|2044629|\n",
      "|20-Dec-16|768.65|774.39|767.71|771.22|2703629|\n",
      "|19-Dec-16|758.89| 770.5|756.16| 766.0|3113240|\n",
      "|16-Dec-16| 765.0|765.13| 754.0|757.77|4848219|\n",
      "|15-Dec-16|766.28| 769.1|760.31| 761.0|3801927|\n",
      "|14-Dec-16|778.25|780.86|762.81|768.82|5454836|\n",
      "|13-Dec-16|764.96|782.46| 762.0|774.34|5285288|\n",
      "+---------+------+------+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amzn.filter('Close < 800 and Open > 750').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+\n",
      "|     Date|  Open|  High|   Low| Close| Volume|\n",
      "+---------+------+------+------+------+-------+\n",
      "|11-Jan-17|793.66| 799.5|789.51|799.02|2992791|\n",
      "|10-Jan-17| 796.6| 798.0|789.54| 795.9|2558369|\n",
      "| 9-Jan-17| 798.0|801.77|791.77|796.92|3446109|\n",
      "| 6-Jan-17|782.36|799.44|778.48|795.99|5986234|\n",
      "| 5-Jan-17|761.55| 782.4|760.26|780.45|5830068|\n",
      "| 4-Jan-17|758.39|759.68| 754.2|757.18|2510526|\n",
      "| 3-Jan-17|757.92|758.76| 747.7|753.67|3521066|\n",
      "|30-Dec-16|766.47| 767.4|748.28|749.87|4139449|\n",
      "|29-Dec-16| 772.4| 773.4|760.85|765.15|3158299|\n",
      "|28-Dec-16|776.25| 780.0| 770.5|772.13|3301025|\n",
      "|27-Dec-16| 763.4|774.65| 761.2| 771.4|2638725|\n",
      "|23-Dec-16|764.55| 766.5|757.99|760.59|1981616|\n",
      "|22-Dec-16|768.12|771.21|763.02|766.34|2543551|\n",
      "|21-Dec-16| 770.0|771.22| 765.7| 770.6|2044629|\n",
      "|20-Dec-16|768.65|774.39|767.71|771.22|2703629|\n",
      "|19-Dec-16|758.89| 770.5|756.16| 766.0|3113240|\n",
      "|16-Dec-16| 765.0|765.13| 754.0|757.77|4848219|\n",
      "|15-Dec-16|766.28| 769.1|760.31| 761.0|3801927|\n",
      "|14-Dec-16|778.25|780.86|762.81|768.82|5454836|\n",
      "|13-Dec-16|764.96|782.46| 762.0|774.34|5285288|\n",
      "+---------+------+------+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amzn.filter(amzn['Close'] < 800).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+\n",
      "|     Date|  Open|  High|   Low| Close| Volume|\n",
      "+---------+------+------+------+------+-------+\n",
      "|11-Jan-17|793.66| 799.5|789.51|799.02|2992791|\n",
      "|10-Jan-17| 796.6| 798.0|789.54| 795.9|2558369|\n",
      "| 9-Jan-17| 798.0|801.77|791.77|796.92|3446109|\n",
      "| 6-Jan-17|782.36|799.44|778.48|795.99|5986234|\n",
      "| 5-Jan-17|761.55| 782.4|760.26|780.45|5830068|\n",
      "| 4-Jan-17|758.39|759.68| 754.2|757.18|2510526|\n",
      "| 3-Jan-17|757.92|758.76| 747.7|753.67|3521066|\n",
      "|30-Dec-16|766.47| 767.4|748.28|749.87|4139449|\n",
      "|29-Dec-16| 772.4| 773.4|760.85|765.15|3158299|\n",
      "|28-Dec-16|776.25| 780.0| 770.5|772.13|3301025|\n",
      "|27-Dec-16| 763.4|774.65| 761.2| 771.4|2638725|\n",
      "|23-Dec-16|764.55| 766.5|757.99|760.59|1981616|\n",
      "|22-Dec-16|768.12|771.21|763.02|766.34|2543551|\n",
      "|21-Dec-16| 770.0|771.22| 765.7| 770.6|2044629|\n",
      "|20-Dec-16|768.65|774.39|767.71|771.22|2703629|\n",
      "|19-Dec-16|758.89| 770.5|756.16| 766.0|3113240|\n",
      "|16-Dec-16| 765.0|765.13| 754.0|757.77|4848219|\n",
      "|15-Dec-16|766.28| 769.1|760.31| 761.0|3801927|\n",
      "|14-Dec-16|778.25|780.86|762.81|768.82|5454836|\n",
      "|13-Dec-16|764.96|782.46| 762.0|774.34|5285288|\n",
      "+---------+------+------+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amzn.filter((amzn['Close'] < 800) & (amzn['Open'] > 750)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   Open|  Close|\n",
      "+-------+-------+\n",
      "|1095.01|1110.85|\n",
      "|1058.14|1100.95|\n",
      "|  969.0|  975.9|\n",
      "|1005.59|1009.13|\n",
      "| 996.81|1000.93|\n",
      "| 991.27|  995.0|\n",
      "| 975.64| 989.58|\n",
      "|  970.0| 980.85|\n",
      "| 954.21| 965.45|\n",
      "| 960.11| 961.35|\n",
      "| 951.86|  956.4|\n",
      "|  948.0| 950.87|\n",
      "| 971.79| 973.21|\n",
      "| 983.97|  999.6|\n",
      "| 974.46| 977.96|\n",
      "|  974.0| 979.47|\n",
      "|  974.7|  980.6|\n",
      "| 958.44| 967.59|\n",
      "|  940.0| 954.06|\n",
      "| 955.52|  966.9|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amzn.filter('Open < Close').select(['Open','Close']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+\n",
      "|     Date|  Open|  High|   Low| Close| Volume|\n",
      "+---------+------+------+------+------+-------+\n",
      "| 4-Jan-17|758.39|759.68| 754.2|757.18|2510526|\n",
      "| 3-Jan-17|757.92|758.76| 747.7|753.67|3521066|\n",
      "|30-Dec-16|766.47| 767.4|748.28|749.87|4139449|\n",
      "|29-Dec-16| 772.4| 773.4|760.85|765.15|3158299|\n",
      "|23-Dec-16|764.55| 766.5|757.99|760.59|1981616|\n",
      "|22-Dec-16|768.12|771.21|763.02|766.34|2543551|\n",
      "|19-Dec-16|758.89| 770.5|756.16| 766.0|3113240|\n",
      "|16-Dec-16| 765.0|765.13| 754.0|757.77|4848219|\n",
      "|15-Dec-16|766.28| 769.1|760.31| 761.0|3801927|\n",
      "|14-Dec-16|778.25|780.86|762.81|768.82|5454836|\n",
      "|12-Dec-16| 766.4|766.89| 757.2|760.12|2963945|\n",
      "| 9-Dec-16| 770.0|770.25|765.34|768.66|2470923|\n",
      "| 8-Dec-16|771.87|773.79|765.19|767.33|3189608|\n",
      "| 6-Dec-16|763.99|768.24|757.25|764.72|3794746|\n",
      "| 5-Dec-16| 745.0|761.49| 742.0|759.36|4314723|\n",
      "| 2-Dec-16| 743.4|748.49| 736.7|740.34|3561307|\n",
      "| 1-Dec-16|752.41|753.37|738.03|743.65|4665993|\n",
      "|30-Nov-16| 762.0|768.09|750.25|750.57|4625946|\n",
      "|29-Nov-16| 768.0|769.89|761.32|762.52|3272344|\n",
      "|28-Nov-16|776.99| 777.0|764.24|766.77|4438828|\n",
      "+---------+------+------+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amzn.filter('Close < 770').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lt770=amzn.filter('Close < 770').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lt770)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='4-Jan-17', Open=758.39, High=759.68, Low=754.2, Close=757.18, Volume=2510526),\n",
       " Row(Date='3-Jan-17', Open=757.92, High=758.76, Low=747.7, Close=753.67, Volume=3521066),\n",
       " Row(Date='30-Dec-16', Open=766.47, High=767.4, Low=748.28, Close=749.87, Volume=4139449),\n",
       " Row(Date='29-Dec-16', Open=772.4, High=773.4, Low=760.85, Close=765.15, Volume=3158299),\n",
       " Row(Date='23-Dec-16', Open=764.55, High=766.5, Low=757.99, Close=760.59, Volume=1981616),\n",
       " Row(Date='22-Dec-16', Open=768.12, High=771.21, Low=763.02, Close=766.34, Volume=2543551),\n",
       " Row(Date='19-Dec-16', Open=758.89, High=770.5, Low=756.16, Close=766.0, Volume=3113240),\n",
       " Row(Date='16-Dec-16', Open=765.0, High=765.13, Low=754.0, Close=757.77, Volume=4848219),\n",
       " Row(Date='15-Dec-16', Open=766.28, High=769.1, Low=760.31, Close=761.0, Volume=3801927),\n",
       " Row(Date='14-Dec-16', Open=778.25, High=780.86, Low=762.81, Close=768.82, Volume=5454836),\n",
       " Row(Date='12-Dec-16', Open=766.4, High=766.89, Low=757.2, Close=760.12, Volume=2963945),\n",
       " Row(Date='9-Dec-16', Open=770.0, High=770.25, Low=765.34, Close=768.66, Volume=2470923),\n",
       " Row(Date='8-Dec-16', Open=771.87, High=773.79, Low=765.19, Close=767.33, Volume=3189608),\n",
       " Row(Date='6-Dec-16', Open=763.99, High=768.24, Low=757.25, Close=764.72, Volume=3794746),\n",
       " Row(Date='5-Dec-16', Open=745.0, High=761.49, Low=742.0, Close=759.36, Volume=4314723),\n",
       " Row(Date='2-Dec-16', Open=743.4, High=748.49, Low=736.7, Close=740.34, Volume=3561307),\n",
       " Row(Date='1-Dec-16', Open=752.41, High=753.37, Low=738.03, Close=743.65, Volume=4665993),\n",
       " Row(Date='30-Nov-16', Open=762.0, High=768.09, Low=750.25, Close=750.57, Volume=4625946),\n",
       " Row(Date='29-Nov-16', Open=768.0, High=769.89, Low=761.32, Close=762.52, Volume=3272344),\n",
       " Row(Date='28-Nov-16', Open=776.99, High=777.0, Low=764.24, Close=766.77, Volume=4438828),\n",
       " Row(Date='18-Nov-16', Open=761.0, High=767.74, Low=757.64, Close=760.16, Volume=4373408),\n",
       " Row(Date='17-Nov-16', Open=749.32, High=757.5, Low=748.0, Close=756.4, Volume=3651345),\n",
       " Row(Date='16-Nov-16', Open=739.88, High=749.87, Low=735.61, Close=746.49, Volume=3648791),\n",
       " Row(Date='15-Nov-16', Open=730.0, High=746.78, Low=725.99, Close=743.24, Volume=6755785),\n",
       " Row(Date='14-Nov-16', Open=745.51, High=746.0, Low=710.1, Close=719.07, Volume=7321344),\n",
       " Row(Date='11-Nov-16', Open=735.73, High=743.26, Low=728.9, Close=739.01, Volume=6622784),\n",
       " Row(Date='10-Nov-16', Open=778.81, High=778.83, Low=717.7, Close=742.38, Volume=12746994),\n",
       " Row(Date='4-Nov-16', Open=762.79, High=766.0, Low=753.23, Close=755.05, Volume=5122103),\n",
       " Row(Date='3-Nov-16', Open=765.05, High=777.0, Low=764.0, Close=767.03, Volume=3872496),\n",
       " Row(Date='2-Nov-16', Open=783.93, High=784.75, Low=763.55, Close=765.56, Volume=5026504)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='4-Jan-17', Open=758.39, High=759.68, Low=754.2, Close=757.18, Volume=2510526)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt770[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4-Jan-17'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt770[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates and timestamps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='30-Oct-17', Open=1095.01, High=1122.79, Low=1093.56, Close=1110.85, Volume=6613064)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amzn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.sql.functions module\n",
    "\n",
    "pyspark.sql.functions module \n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions)   \n",
    "\n",
    "**pyspark.sql.functions.datediff(end, start)**  \n",
    "\n",
    "Returns the number of days from start to end.\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
    ">>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
    "[Row(diff=32)]\n",
    "```\n",
    "\n",
    "**pyspark.sql.functions.dayofmonth(col)**  \n",
    "\n",
    "Extract the day of the month of a given date as integer.\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
    ">>> df.select(dayofmonth('a').alias('day')).collect()\n",
    "[Row(day=8)]\n",
    "```\n",
    "\n",
    "**pyspark.sql.functions.dayofyear(col)**  \n",
    "\n",
    "Extract the day of the year of a given date as integer.\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
    ">>> df.select(dayofyear('a').alias('day')).collect()\n",
    "[Row(day=98)]\n",
    "```\n",
    "\n",
    "**pyspark.sql.functions.hour(col)**  \n",
    "\n",
    "Extract the hours of a given date as integer.\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['a'])\n",
    ">>> df.select(hour('a').alias('hour')).collect()\n",
    "[Row(hour=13)]\n",
    "```\n",
    "\n",
    "\n",
    "**pyspark.sql.functions.last_day(date)**  \n",
    "\n",
    "Returns the last day of the month which the given date belongs to.\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
    ">>> df.select(last_day(df.d).alias('date')).collect()\n",
    "[Row(date=datetime.date(1997, 2, 28))]\n",
    "```\n",
    "\n",
    "\n",
    "**pyspark.sql.functions.minute(col)**\n",
    "\n",
    "Extract the minutes of a given date as integer.\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['a'])\n",
    ">>> df.select(minute('a').alias('minute')).collect()\n",
    "[Row(minute=8)]\n",
    "```\n",
    "\n",
    "**pyspark.sql.functions.month(col)**  \n",
    "\n",
    "Extract the month of a given date as integer.\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
    ">>> df.select(month('a').alias('month')).collect()\n",
    "[Row(month=4)]\n",
    "```\n",
    "\n",
    "**pyspark.sql.functions.months_between(date1, date2)**  \n",
    "\n",
    "Returns the number of months between date1 and date2.\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['t', 'd'])\n",
    ">>> df.select(months_between(df.t, df.d).alias('months')).collect()\n",
    "[Row(months=3.9495967...)]\n",
    "```\n",
    "\n",
    "**pyspark.sql.functions.next_day(date, dayOfWeek)** \n",
    "\n",
    "Returns the first date which is later than the value of the date column.\n",
    "\n",
    "Day of the week parameter is case insensitive, and accepts:\n",
    "“Mon”, “Tue”, “Wed”, “Thu”, “Fri”, “Sat”, “Sun”.\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n",
    ">>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n",
    "[Row(date=datetime.date(2015, 8, 2))]\n",
    "```\n",
    "\n",
    "**pyspark.sql.functions.second(col)**  \n",
    "\n",
    "Extract the seconds of a given date as integer.\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['a'])\n",
    ">>> df.select(second('a').alias('second')).collect()\n",
    "[Row(second=15)]\n",
    "```\n",
    "\n",
    "**pyspark.sql.functions.to_utc_timestamp(timestamp, tz)**  \n",
    "\n",
    "Given a timestamp, which corresponds to a certain time of day in the given timezone, returns another timestamp that corresponds to the same time of day in UTC.\n",
    "\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    ">>> df.select(to_utc_timestamp(df.t, \"PST\").alias('t')).collect()\n",
    "[Row(t=datetime.datetime(1997, 2, 28, 18, 30))]\n",
    "```\n",
    "\n",
    "**pyspark.sql.functions.unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')**  \n",
    "\n",
    "Convert time string with given pattern (‘yyyy-MM-dd HH:mm:ss’, by default) to Unix time stamp (in seconds), using the default timezone and the default locale, return null if fail.\n",
    "\n",
    "if timestamp is None, then it returns current timestamp.\n",
    "\n",
    "\n",
    "**pyspark.sql.functions.weekofyear(col)**  \n",
    "\n",
    "Extract the week number of a given date as integer.\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
    ">>> df.select(weekofyear(df.a).alias('week')).collect()\n",
    "[Row(week=15)]\n",
    "```\n",
    "\n",
    "**pyspark.sql.functions.year(col)**  \n",
    "\n",
    "Extract the year of a given date as integer.\n",
    "\n",
    "```python\n",
    ">>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
    ">>> df.select(year('a').alias('year')).collect()\n",
    "[Row(year=2015)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Date|\n",
      "+---------+\n",
      "|30-Oct-17|\n",
      "|27-Oct-17|\n",
      "|26-Oct-17|\n",
      "|25-Oct-17|\n",
      "|24-Oct-17|\n",
      "|23-Oct-17|\n",
      "|20-Oct-17|\n",
      "|19-Oct-17|\n",
      "|18-Oct-17|\n",
      "|17-Oct-17|\n",
      "|16-Oct-17|\n",
      "|13-Oct-17|\n",
      "|12-Oct-17|\n",
      "|11-Oct-17|\n",
      "|10-Oct-17|\n",
      "| 9-Oct-17|\n",
      "| 6-Oct-17|\n",
      "| 5-Oct-17|\n",
      "| 4-Oct-17|\n",
      "| 3-Oct-17|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amzn.select(amzn['Date']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__all__ = [\n",
    "    \"DataType\", \"NullType\", \"StringType\", \"BinaryType\", \"BooleanType\", \"DateType\",\n",
    "    \"TimestampType\", \"DecimalType\", \"DoubleType\", \"FloatType\", \"ByteType\", \"IntegerType\",\n",
    "    \"LongType\", \"ShortType\", \"ArrayType\", \"MapType\", \"StructField\", \"StructType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, TimestampType\n",
    "amzn = amzn.withColumn('dateTime', amzn['Date'].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='30-Oct-17', Open=1095.01, High=1122.79, Low=1093.56, Close=1110.85, Volume=6613064, dateTime=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amzn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GroupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rossmann Store Sales\n",
    "\n",
    "Forecast sales using store, promotion, and competitor data\n",
    "\n",
    "From [https://www.kaggle.com/c/rossmann-store-sales](https://www.kaggle.com/c/rossmann-store-sales)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+-----+---------+\n",
      "|Store|DayOfWeek|   Date|Sales|Customers|\n",
      "+-----+---------+-------+-----+---------+\n",
      "|    1|        5|7/31/15| 5263|      555|\n",
      "|    2|        5|7/31/15| 6064|      625|\n",
      "|    3|        5|7/31/15| 8314|      821|\n",
      "|    4|        5|7/31/15|13995|     1498|\n",
      "|    5|        5|7/31/15| 4822|      559|\n",
      "|    6|        5|7/31/15| 5651|      589|\n",
      "|    7|        5|7/31/15|15344|     1414|\n",
      "|    8|        5|7/31/15| 8492|      833|\n",
      "|    9|        5|7/31/15| 8565|      687|\n",
      "|   10|        5|7/31/15| 7185|      681|\n",
      "|   11|        5|7/31/15|10457|     1236|\n",
      "|   12|        5|7/31/15| 8959|      962|\n",
      "|   13|        5|7/31/15| 8821|      568|\n",
      "|   14|        5|7/31/15| 6544|      710|\n",
      "|   15|        5|7/31/15| 9191|      766|\n",
      "|   16|        5|7/31/15|10231|      979|\n",
      "|   17|        5|7/31/15| 8430|      946|\n",
      "|   18|        5|7/31/15|10071|      936|\n",
      "|   19|        5|7/31/15| 8234|      718|\n",
      "|   20|        5|7/31/15| 9593|      974|\n",
      "+-----+---------+-------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "rossmann = spark.read.csv(\"data/Rossmann.csv\", header=True, inferSchema=True)\n",
    "# Displays the content of the DataFrame to stdout\n",
    "rossmann.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Sales: integer (nullable = true)\n",
      " |-- Customers: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, TimestampType\n",
    "rossmann = rossmann.withColumn('dateTime', rossmann['Date'].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Sales: integer (nullable = true)\n",
      " |-- Customers: integer (nullable = true)\n",
      " |-- dateTime: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Store=1, DayOfWeek=5, Date='7/31/15', Sales=5263, Customers=555, dateTime=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rossmann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+-----+---------+--------+\n",
      "|Store|DayOfWeek|   Date|Sales|Customers|dateTime|\n",
      "+-----+---------+-------+-----+---------+--------+\n",
      "|    1|        5|7/31/15| 5263|      555|    null|\n",
      "|    2|        5|7/31/15| 6064|      625|    null|\n",
      "|    3|        5|7/31/15| 8314|      821|    null|\n",
      "|    4|        5|7/31/15|13995|     1498|    null|\n",
      "|    5|        5|7/31/15| 4822|      559|    null|\n",
      "|    6|        5|7/31/15| 5651|      589|    null|\n",
      "|    7|        5|7/31/15|15344|     1414|    null|\n",
      "|    8|        5|7/31/15| 8492|      833|    null|\n",
      "|    9|        5|7/31/15| 8565|      687|    null|\n",
      "|   10|        5|7/31/15| 7185|      681|    null|\n",
      "|   11|        5|7/31/15|10457|     1236|    null|\n",
      "|   12|        5|7/31/15| 8959|      962|    null|\n",
      "|   13|        5|7/31/15| 8821|      568|    null|\n",
      "|   14|        5|7/31/15| 6544|      710|    null|\n",
      "|   15|        5|7/31/15| 9191|      766|    null|\n",
      "|   16|        5|7/31/15|10231|      979|    null|\n",
      "|   17|        5|7/31/15| 8430|      946|    null|\n",
      "|   18|        5|7/31/15|10071|      936|    null|\n",
      "|   19|        5|7/31/15| 8234|      718|    null|\n",
      "|   20|        5|7/31/15| 9593|      974|    null|\n",
      "+-----+---------+-------+-----+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x1116bc710>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rossmann_bydate=rossmann.select(['Date','Sales','Customers']).groupBy('Date')\n",
    "rossmann_bydate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----------------+\n",
      "|    Date|        avg(Sales)|   avg(Customers)|\n",
      "+--------+------------------+-----------------+\n",
      "|  3/8/15| 180.5748878923767|33.70403587443946|\n",
      "|  2/7/14| 7242.437668161435|774.3479820627803|\n",
      "|  7/6/13|  5315.72735426009|619.7399103139013|\n",
      "|  4/4/13|5826.1094170403585|708.1076233183857|\n",
      "|12/26/14|219.50588235294117|34.41711229946524|\n",
      "|11/27/14| 8414.449197860962| 859.648128342246|\n",
      "| 11/5/14| 8013.904812834225|831.5572192513368|\n",
      "|10/14/14|  5576.19679144385|            701.8|\n",
      "| 8/21/14| 7076.151871657754|789.5989304812834|\n",
      "| 8/20/14|7511.0139037433155|811.4791443850268|\n",
      "| 6/23/14| 5923.737219730941|719.3085201793722|\n",
      "| 4/11/14| 6197.534529147982|730.0089686098655|\n",
      "| 3/25/15| 5447.339910313902|644.2959641255605|\n",
      "| 2/28/15| 6656.273542600897|703.6636771300448|\n",
      "|12/10/13| 6689.007174887893|768.9659192825112|\n",
      "|10/29/13| 5511.700448430493| 672.857399103139|\n",
      "| 6/18/13| 7264.830493273543|783.3372197309417|\n",
      "|  4/9/15| 5883.834080717489|698.0358744394618|\n",
      "| 3/22/15|211.52645739910315|36.16143497757847|\n",
      "| 7/15/14| 8219.244919786097|843.6021390374332|\n",
      "+--------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.select(['Date','Sales','Customers']).groupBy('Date').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+\n",
      "|    Date|sum(Sales)|sum(Customers)|\n",
      "+--------+----------+--------------+\n",
      "|  3/8/15|    201341|         37580|\n",
      "|  2/7/14|   8075318|        863398|\n",
      "|  7/6/13|   5927036|        691010|\n",
      "|  4/4/13|   6496112|        789540|\n",
      "|12/26/14|    205238|         32180|\n",
      "|11/27/14|   7867510|        803771|\n",
      "| 11/5/14|   7493001|        777506|\n",
      "|10/14/14|   5213744|        656183|\n",
      "| 8/21/14|   6616202|        738275|\n",
      "| 8/20/14|   7022798|        758733|\n",
      "| 6/23/14|   6604967|        802029|\n",
      "| 4/11/14|   6910251|        813960|\n",
      "| 3/25/15|   6073784|        718390|\n",
      "| 2/28/15|   7421745|        784585|\n",
      "|12/10/13|   7458243|        857397|\n",
      "|10/29/13|   6145546|        750236|\n",
      "| 6/18/13|   8100286|        873421|\n",
      "|  4/9/15|   6560475|        778310|\n",
      "| 3/22/15|    235852|         40320|\n",
      "| 7/15/14|   7684994|        788768|\n",
      "+--------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann_bydate.sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x1116466a0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rossmann_bystore=rossmann.select(['Store','Sales','Customers']).groupBy('Store')\n",
    "rossmann_bystore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+--------------+\n",
      "|Store|sum(Store)|sum(Sales)|sum(Customers)|\n",
      "+-----+----------+----------+--------------+\n",
      "|  148|    139416|   6772949|        642470|\n",
      "|  463|    436146|   4747749|        755184|\n",
      "|  471|    357018|   4422266|        367107|\n",
      "|  496|    467232|   6059458|        650918|\n",
      "|  833|    784686|   4293342|        394316|\n",
      "| 1088|   1024896|   3960984|        377930|\n",
      "|  243|    184194|   4407146|        594010|\n",
      "|  392|    369264|   5670675|        559386|\n",
      "|  540|    409320|   3513672|        243396|\n",
      "|  623|    586866|   5473895|        639137|\n",
      "|  737|    694254|   4765845|        699863|\n",
      "|  858|    650364|   2957986|        260058|\n",
      "|  897|    844974|   2717511|        268885|\n",
      "| 1025|    965550|   5762189|        672559|\n",
      "| 1084|   1021128|   4904211|        631824|\n",
      "|   31|     29202|   4596143|        459464|\n",
      "|  516|    486072|   4579807|        592467|\n",
      "|   85|     80070|   6850652|        956508|\n",
      "|  137|    103846|   5147039|        595429|\n",
      "|  251|    236442|  14896870|       1908934|\n",
      "+-----+----------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann_bystore.sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aggregate Functions\n",
    "\n",
    "pyspark.sql.functions module \n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions)  \n",
    "\n",
    "**pyspark.sql.functions.approx_count_distinct(col, rsd=None)**  \n",
    "\n",
    "Returns a new Column for approximate distinct count of col.\n",
    "\n",
    "```python\n",
    ">>> df.agg(approx_count_distinct(df.age).alias('c')).collect()\n",
    "[Row(c=2)]\n",
    "```\n",
    "\n",
    "\n",
    "**pyspark.sql.functions.avg(col)**  \n",
    "\n",
    "Aggregate function: returns the average of the values in a group.\n",
    "\n",
    "**pyspark.sql.functions.collect_list(col)**\n",
    "\n",
    "Aggregate function: returns a list of objects with duplicates.\n",
    "\n",
    "**pyspark.sql.functions.collect_set(col)**  \n",
    "\n",
    "Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
    "\n",
    "**pyspark.sql.functions.count(col)**  \n",
    "\n",
    "Aggregate function: returns the number of items in a group.\n",
    "\n",
    "\n",
    "**pyspark.sql.functions.countDistinct(col, *cols)**  \n",
    "\n",
    "Returns a new Column for distinct count of col or cols.\n",
    "\n",
    "```python\n",
    ">>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()\n",
    "[Row(c=2)]\n",
    ">>> df.agg(countDistinct(\"age\", \"name\").alias('c')).collect()\n",
    "```\n",
    "\n",
    "**pyspark.sql.functions.first(col, ignorenulls=False)**  \n",
    "\n",
    "Aggregate function: returns the first value in a group.\n",
    "\n",
    "The function by default returns the first values it sees. It will return the first non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "\n",
    "\n",
    "**pyspark.sql.functions.grouping(col)**  \n",
    "\n",
    "Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n",
    "\n",
    "```python\n",
    ">>> df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()\n",
    "+-----+--------------+--------+\n",
    "| name|grouping(name)|sum(age)|\n",
    "+-----+--------------+--------+\n",
    "| null|             1|       7|\n",
    "|Alice|             0|       2|\n",
    "|  Bob|             0|       5|\n",
    "+-----+--------------+--------+\n",
    "```\n",
    "\n",
    "**pyspark.sql.functions.kurtosis(col)**  \n",
    "\n",
    "Aggregate function: returns the kurtosis of the values in a group.\n",
    "\n",
    "\n",
    "**pyspark.sql.functions.last(col, ignorenulls=False)**  \n",
    "\n",
    "Aggregate function: returns the last value in a group.\n",
    "\n",
    "The function by default returns the last values it sees. It will return the last non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "\n",
    "**pyspark.sql.functions.max(col)**  \n",
    "\n",
    "Aggregate function: returns the maximum value of the expression in a group.\n",
    "\n",
    "**pyspark.sql.functions.mean(col)**  \n",
    "\n",
    "Aggregate function: returns the average of the values in a group.\n",
    "\n",
    "**pyspark.sql.functions.min(col)**  \n",
    "\n",
    "Aggregate function: returns the minimum value of the expression in a group.\n",
    "\n",
    "**pyspark.sql.functions.skewness(col)**  \n",
    "\n",
    "Aggregate function: returns the skewness of the values in a group.\n",
    "\n",
    "**pyspark.sql.functions.stddev(col)**  \n",
    "\n",
    "Aggregate function: returns the unbiased sample standard deviation of the expression in a group.\n",
    "\n",
    "**pyspark.sql.functions.stddev_pop(col)**  \n",
    "\n",
    "Aggregate function: returns population standard deviation of the expression in a group.\n",
    "\n",
    "**pyspark.sql.functions.stddev_samp(col)**  \n",
    "\n",
    "Aggregate function: returns the unbiased sample standard deviation of the expression in a group.\n",
    "\n",
    "**pyspark.sql.functions.sum(col)**  \n",
    "\n",
    "Aggregate function: returns the sum of all values in the expression.\n",
    "\n",
    "**pyspark.sql.functions.sumDistinct(col)**  \n",
    "\n",
    "Aggregate function: returns the sum of distinct values in the expression.\n",
    "\n",
    "\n",
    "**pyspark.sql.functions.var_pop(col)**   \n",
    "\n",
    "Aggregate function: returns the population variance of the values in a group.\n",
    "\n",
    "\n",
    "**pyspark.sql.functions.var_samp(col)**   \n",
    "\n",
    "Aggregate function: returns the unbiased variance of the values in a group.\n",
    "\n",
    "**pyspark.sql.functions.variance(col)**  \n",
    "\n",
    "Aggregate function: returns the population variance of the values in a group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Sales)|\n",
      "+-----------------+\n",
      "|5773.818972305593|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.select(pf.avg('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         av_sales|\n",
      "+-----------------+\n",
      "|5773.818972305593|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.select(pf.avg('Sales').alias('av_sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|stddev_samp(Sales)|\n",
      "+------------------+\n",
      "| 3849.926175234763|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.select(pf.stddev('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|format_number(stddev_samp(Sales), 2)|\n",
      "+------------------------------------+\n",
      "|                            3,849.93|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stddev_sales=rossmann.select(pf.stddev('Sales'))\n",
    "stddev_sales.select(pf.format_number('stddev_samp(Sales)',2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT Store)|\n",
      "+---------------------+\n",
      "|                 1115|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.select(pf.countDistinct('Store')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sorting  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+-----+---------+--------+\n",
      "|Store|DayOfWeek|   Date|Sales|Customers|dateTime|\n",
      "+-----+---------+-------+-----+---------+--------+\n",
      "|    7|        7|4/14/13|    0|        0|    null|\n",
      "|    6|        7|4/14/13|    0|        0|    null|\n",
      "|  589|        1|4/15/13|    0|        0|    null|\n",
      "| 1081|        3|4/17/13|    0|        0|    null|\n",
      "|  708|        1|4/15/13|    0|        0|    null|\n",
      "|  103|        2|4/16/13|    0|        0|    null|\n",
      "|  948|        1|4/15/13|    0|        0|    null|\n",
      "|  105|        1|4/15/13|    0|        0|    null|\n",
      "| 1081|        1|4/15/13|    0|        0|    null|\n",
      "|  708|        2|4/16/13|    0|        0|    null|\n",
      "|    1|        7|4/14/13|    0|        0|    null|\n",
      "| 1081|        2|4/16/13|    0|        0|    null|\n",
      "|    2|        7|4/14/13|    0|        0|    null|\n",
      "|  948|        3|4/17/13|    0|        0|    null|\n",
      "|    3|        7|4/14/13|    0|        0|    null|\n",
      "|  105|        2|4/16/13|    0|        0|    null|\n",
      "|    4|        7|4/14/13|    0|        0|    null|\n",
      "|  948|        2|4/16/13|    0|        0|    null|\n",
      "|    5|        7|4/14/13|    0|        0|    null|\n",
      "|  103|        1|4/15/13|    0|        0|    null|\n",
      "+-----+---------+-------+-----+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.orderBy('Sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+-----+---------+--------+\n",
      "|Store|DayOfWeek|   Date|Sales|Customers|dateTime|\n",
      "+-----+---------+-------+-----+---------+--------+\n",
      "|    7|        7|4/14/13|    0|        0|    null|\n",
      "|    6|        7|4/14/13|    0|        0|    null|\n",
      "|  589|        1|4/15/13|    0|        0|    null|\n",
      "| 1081|        3|4/17/13|    0|        0|    null|\n",
      "|  708|        1|4/15/13|    0|        0|    null|\n",
      "|  103|        2|4/16/13|    0|        0|    null|\n",
      "|  948|        1|4/15/13|    0|        0|    null|\n",
      "|  105|        1|4/15/13|    0|        0|    null|\n",
      "| 1081|        1|4/15/13|    0|        0|    null|\n",
      "|  708|        2|4/16/13|    0|        0|    null|\n",
      "|    1|        7|4/14/13|    0|        0|    null|\n",
      "| 1081|        2|4/16/13|    0|        0|    null|\n",
      "|    2|        7|4/14/13|    0|        0|    null|\n",
      "|  948|        3|4/17/13|    0|        0|    null|\n",
      "|    3|        7|4/14/13|    0|        0|    null|\n",
      "|  105|        2|4/16/13|    0|        0|    null|\n",
      "|    4|        7|4/14/13|    0|        0|    null|\n",
      "|  948|        2|4/16/13|    0|        0|    null|\n",
      "|    5|        7|4/14/13|    0|        0|    null|\n",
      "|  103|        1|4/15/13|    0|        0|    null|\n",
      "+-----+---------+-------+-----+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.orderBy('Sales','Customers').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------+-----+---------+--------+\n",
      "|Store|DayOfWeek|    Date|Sales|Customers|dateTime|\n",
      "+-----+---------+--------+-----+---------+--------+\n",
      "|  909|        1| 6/22/15|41551|     1721|    null|\n",
      "|  262|        5|  4/3/15|38722|     5132|    null|\n",
      "|  262|        5|  5/1/15|38484|     5458|    null|\n",
      "|  262|        4| 5/14/15|38367|     5192|    null|\n",
      "|   57|        1| 6/16/14|38037|     1970|    null|\n",
      "|  817|        1|12/16/13|38025|     4381|    null|\n",
      "|  261|        1|12/16/13|37646|     1964|    null|\n",
      "|  262|        4| 5/29/14|37403|     5297|    null|\n",
      "|  262|        7|12/22/13|37376|     4916|    null|\n",
      "|  262|        7|12/21/14|37122|     4962|    null|\n",
      "|  262|        7|11/30/14|36417|     4816|    null|\n",
      "|  262|        5| 3/29/13|36227|     5069|    null|\n",
      "|  262|        5| 4/18/14|35909|     5063|    null|\n",
      "|  262|        5| 10/3/14|35702|     5494|    null|\n",
      "| 1114|        1|12/23/13|35697|     4911|    null|\n",
      "|  251|        1|12/23/13|35350|     4635|    null|\n",
      "|  262|        1| 5/25/15|35159|     4989|    null|\n",
      "|  842|        1|12/16/13|35154|     1796|    null|\n",
      "|  262|        7| 12/1/13|34904|     4894|    null|\n",
      "|  262|        4|  5/1/14|34814|     4931|    null|\n",
      "+-----+---------+--------+-----+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.orderBy(rossmann.Sales.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------+-----+---------+--------+\n",
      "|Store|DayOfWeek|    Date|Sales|Customers|dateTime|\n",
      "+-----+---------+--------+-----+---------+--------+\n",
      "|  909|        1| 6/22/15|41551|     1721|    null|\n",
      "|  262|        5|  4/3/15|38722|     5132|    null|\n",
      "|  262|        5|  5/1/15|38484|     5458|    null|\n",
      "|  262|        4| 5/14/15|38367|     5192|    null|\n",
      "|   57|        1| 6/16/14|38037|     1970|    null|\n",
      "|  817|        1|12/16/13|38025|     4381|    null|\n",
      "|  261|        1|12/16/13|37646|     1964|    null|\n",
      "|  262|        4| 5/29/14|37403|     5297|    null|\n",
      "|  262|        7|12/22/13|37376|     4916|    null|\n",
      "|  262|        7|12/21/14|37122|     4962|    null|\n",
      "|  262|        7|11/30/14|36417|     4816|    null|\n",
      "|  262|        5| 3/29/13|36227|     5069|    null|\n",
      "|  262|        5| 4/18/14|35909|     5063|    null|\n",
      "|  262|        5| 10/3/14|35702|     5494|    null|\n",
      "| 1114|        1|12/23/13|35697|     4911|    null|\n",
      "|  251|        1|12/23/13|35350|     4635|    null|\n",
      "|  262|        1| 5/25/15|35159|     4989|    null|\n",
      "|  842|        1|12/16/13|35154|     1796|    null|\n",
      "|  262|        7| 12/1/13|34904|     4894|    null|\n",
      "|  262|        4|  5/1/14|34814|     4931|    null|\n",
      "+-----+---------+--------+-----+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.orderBy(rossmann['Sales'].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------+-----+---------+--------+\n",
      "|Store|DayOfWeek|    Date|Sales|Customers|dateTime|\n",
      "+-----+---------+--------+-----+---------+--------+\n",
      "|  909|        1| 6/22/15|41551|     1721|    null|\n",
      "|  262|        5|  4/3/15|38722|     5132|    null|\n",
      "|  262|        5|  5/1/15|38484|     5458|    null|\n",
      "|  262|        4| 5/14/15|38367|     5192|    null|\n",
      "|   57|        1| 6/16/14|38037|     1970|    null|\n",
      "|  817|        1|12/16/13|38025|     4381|    null|\n",
      "|  261|        1|12/16/13|37646|     1964|    null|\n",
      "|  262|        4| 5/29/14|37403|     5297|    null|\n",
      "|  262|        7|12/22/13|37376|     4916|    null|\n",
      "|  262|        7|12/21/14|37122|     4962|    null|\n",
      "|  262|        7|11/30/14|36417|     4816|    null|\n",
      "|  262|        5| 3/29/13|36227|     5069|    null|\n",
      "|  262|        5| 4/18/14|35909|     5063|    null|\n",
      "|  262|        5| 10/3/14|35702|     5494|    null|\n",
      "| 1114|        1|12/23/13|35697|     4911|    null|\n",
      "|  251|        1|12/23/13|35350|     4635|    null|\n",
      "|  262|        1| 5/25/15|35159|     4989|    null|\n",
      "|  842|        1|12/16/13|35154|     1796|    null|\n",
      "|  262|        7| 12/1/13|34904|     4894|    null|\n",
      "|  262|        4|  5/1/14|34814|     4931|    null|\n",
      "+-----+---------+--------+-----+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.orderBy(rossmann.Sales.desc(),rossmann.Customers).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------+-----+---------+--------+\n",
      "|Store|DayOfWeek|    Date|Sales|Customers|dateTime|\n",
      "+-----+---------+--------+-----+---------+--------+\n",
      "|  909|        1| 6/22/15|41551|     1721|    null|\n",
      "|  262|        5|  4/3/15|38722|     5132|    null|\n",
      "|  262|        5|  5/1/15|38484|     5458|    null|\n",
      "|  262|        4| 5/14/15|38367|     5192|    null|\n",
      "|   57|        1| 6/16/14|38037|     1970|    null|\n",
      "|  817|        1|12/16/13|38025|     4381|    null|\n",
      "|  261|        1|12/16/13|37646|     1964|    null|\n",
      "|  262|        4| 5/29/14|37403|     5297|    null|\n",
      "|  262|        7|12/22/13|37376|     4916|    null|\n",
      "|  262|        7|12/21/14|37122|     4962|    null|\n",
      "|  262|        7|11/30/14|36417|     4816|    null|\n",
      "|  262|        5| 3/29/13|36227|     5069|    null|\n",
      "|  262|        5| 4/18/14|35909|     5063|    null|\n",
      "|  262|        5| 10/3/14|35702|     5494|    null|\n",
      "| 1114|        1|12/23/13|35697|     4911|    null|\n",
      "|  251|        1|12/23/13|35350|     4635|    null|\n",
      "|  262|        1| 5/25/15|35159|     4989|    null|\n",
      "|  842|        1|12/16/13|35154|     1796|    null|\n",
      "|  262|        7| 12/1/13|34904|     4894|    null|\n",
      "|  262|        4|  5/1/14|34814|     4931|    null|\n",
      "+-----+---------+--------+-----+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rossmann.orderBy(rossmann.Sales.desc(),rossmann.Customers.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "**Planet Express data**\n",
    "\n",
    "Planet Express, Inc. is an intergalactic delivery company owned and operated by Professor Farnsworth to fund his research. Founded in 2961, its headquarters is located in New New York.   \n",
    "\n",
    "![Planet Express logo](http://nikbearbrown.com/YouTube/MachineLearning/IMG/planet-express-logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "|CompanySize|        Profession|LastDelivery|NumDeliveries|Location|Year|\n",
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "|          2|             Pilot|        3.61|         null|   Earth|3001|\n",
      "|          2|              null|        3.67|            3|   Earth|null|\n",
      "|          2|             Pilot|         4.0|            1|   Earth|3001|\n",
      "|          2|             Pilot|        3.19|            4|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|        2.93|            4|    null|null|\n",
      "|       null|Pizza Delivery Boy|         3.0|            2|    null|null|\n",
      "|          2|       MomCorp CEO|        2.98|            1|   Earth|3001|\n",
      "|          2|             Robot|        3.08|            2|   Earth|3001|\n",
      "|          2|       MomCorp CEO|        3.39|            3|   Earth|3001|\n",
      "|          2|              null|        null|         null|    null|3001|\n",
      "|          2|              null|        null|         null|    null|3001|\n",
      "|          2|       MomCorp CEO|        3.22|            1|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|         4.0|            1|   Earth|3001|\n",
      "|          2|             Robot|        3.08|            2|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|         4.0|            1|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|        3.44|            3|   Earth|3001|\n",
      "|       null|              null|        null|            4|   Earth|3001|\n",
      "|       null|              null|        null|            3|   Earth|3001|\n",
      "|          2| Internet Gangstas|        3.75|            2|   Earth|3001|\n",
      "|          2| Internet Gangstas|        3.81|            1|   Earth|3001|\n",
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "planet_express = spark.read.csv(\"data/planet-express.csv\", header=True, inferSchema=True)\n",
    "# Displays the content of the DataFrame to stdout\n",
    "planet_express.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CompanySize: integer (nullable = true)\n",
      " |-- Profession: string (nullable = true)\n",
      " |-- LastDelivery: double (nullable = true)\n",
      " |-- NumDeliveries: integer (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "planet_express.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planet_express.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planet_express.na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planet_express.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planet_express.na.drop(thresh=2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planet_express.na.drop(subset=['NumDeliveries']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planet_express.na.drop(subset=['Profession']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "|CompanySize|        Profession|LastDelivery|NumDeliveries|Location|Year|\n",
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "|          2|             Pilot|        3.61|         null|   Earth|3001|\n",
      "|          2|               New|        3.67|            3|   Earth|null|\n",
      "|          2|             Pilot|         4.0|            1|   Earth|3001|\n",
      "|          2|             Pilot|        3.19|            4|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|        2.93|            4|     New|null|\n",
      "|       null|Pizza Delivery Boy|         3.0|            2|     New|null|\n",
      "|          2|       MomCorp CEO|        2.98|            1|   Earth|3001|\n",
      "|          2|             Robot|        3.08|            2|   Earth|3001|\n",
      "|          2|       MomCorp CEO|        3.39|            3|   Earth|3001|\n",
      "|          2|               New|        null|         null|     New|3001|\n",
      "|          2|               New|        null|         null|     New|3001|\n",
      "|          2|       MomCorp CEO|        3.22|            1|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|         4.0|            1|   Earth|3001|\n",
      "|          2|             Robot|        3.08|            2|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|         4.0|            1|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|        3.44|            3|   Earth|3001|\n",
      "|       null|               New|        null|            4|   Earth|3001|\n",
      "|       null|               New|        null|            3|   Earth|3001|\n",
      "|          2| Internet Gangstas|        3.75|            2|   Earth|3001|\n",
      "|          2| Internet Gangstas|        3.81|            1|   Earth|3001|\n",
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "planet_express.na.fill('New').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "|CompanySize|        Profession|LastDelivery|NumDeliveries|Location|Year|\n",
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "|          2|             Pilot|        3.61|           99|   Earth|3001|\n",
      "|          2|              null|        3.67|            3|   Earth|  99|\n",
      "|          2|             Pilot|         4.0|            1|   Earth|3001|\n",
      "|          2|             Pilot|        3.19|            4|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|        2.93|            4|    null|  99|\n",
      "|         99|Pizza Delivery Boy|         3.0|            2|    null|  99|\n",
      "|          2|       MomCorp CEO|        2.98|            1|   Earth|3001|\n",
      "|          2|             Robot|        3.08|            2|   Earth|3001|\n",
      "|          2|       MomCorp CEO|        3.39|            3|   Earth|3001|\n",
      "|          2|              null|        99.0|           99|    null|3001|\n",
      "|          2|              null|        99.0|           99|    null|3001|\n",
      "|          2|       MomCorp CEO|        3.22|            1|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|         4.0|            1|   Earth|3001|\n",
      "|          2|             Robot|        3.08|            2|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|         4.0|            1|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|        3.44|            3|   Earth|3001|\n",
      "|         99|              null|        99.0|            4|   Earth|3001|\n",
      "|         99|              null|        99.0|            3|   Earth|3001|\n",
      "|          2| Internet Gangstas|        3.75|            2|   Earth|3001|\n",
      "|          2| Internet Gangstas|        3.81|            1|   Earth|3001|\n",
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "planet_express.na.fill(99).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "|CompanySize|        Profession|LastDelivery|NumDeliveries|Location|Year|\n",
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "|          2|             Pilot|        3.61|         null|   Earth|3001|\n",
      "|          2|           Twerker|        3.67|            3|   Earth|null|\n",
      "|          2|             Pilot|         4.0|            1|   Earth|3001|\n",
      "|          2|             Pilot|        3.19|            4|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|        2.93|            4|    null|null|\n",
      "|       null|Pizza Delivery Boy|         3.0|            2|    null|null|\n",
      "|          2|       MomCorp CEO|        2.98|            1|   Earth|3001|\n",
      "|          2|             Robot|        3.08|            2|   Earth|3001|\n",
      "|          2|       MomCorp CEO|        3.39|            3|   Earth|3001|\n",
      "|          2|           Twerker|        null|         null|    null|3001|\n",
      "|          2|           Twerker|        null|         null|    null|3001|\n",
      "|          2|       MomCorp CEO|        3.22|            1|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|         4.0|            1|   Earth|3001|\n",
      "|          2|             Robot|        3.08|            2|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|         4.0|            1|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|        3.44|            3|   Earth|3001|\n",
      "|       null|           Twerker|        null|            4|   Earth|3001|\n",
      "|       null|           Twerker|        null|            3|   Earth|3001|\n",
      "|          2| Internet Gangstas|        3.75|            2|   Earth|3001|\n",
      "|          2| Internet Gangstas|        3.81|            1|   Earth|3001|\n",
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "planet_express.na.fill('Twerker',subset=['Profession']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "|CompanySize|        Profession|LastDelivery|NumDeliveries|Location|Year|\n",
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "|          2|             Pilot|        3.61|         null|   Earth|3001|\n",
      "|          2|              null|        3.67|            3|   Earth|null|\n",
      "|          2|             Pilot|         4.0|            1|   Earth|3001|\n",
      "|          2|             Pilot|        3.19|            4|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|        2.93|            4|   Earth|null|\n",
      "|       null|Pizza Delivery Boy|         3.0|            2|   Earth|null|\n",
      "|          2|       MomCorp CEO|        2.98|            1|   Earth|3001|\n",
      "|          2|             Robot|        3.08|            2|   Earth|3001|\n",
      "|          2|       MomCorp CEO|        3.39|            3|   Earth|3001|\n",
      "|          2|              null|        null|         null|   Earth|3001|\n",
      "|          2|              null|        null|         null|   Earth|3001|\n",
      "|          2|       MomCorp CEO|        3.22|            1|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|         4.0|            1|   Earth|3001|\n",
      "|          2|             Robot|        3.08|            2|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|         4.0|            1|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|        3.44|            3|   Earth|3001|\n",
      "|       null|              null|        null|            4|   Earth|3001|\n",
      "|       null|              null|        null|            3|   Earth|3001|\n",
      "|          2| Internet Gangstas|        3.75|            2|   Earth|3001|\n",
      "|          2| Internet Gangstas|        3.81|            1|   Earth|3001|\n",
      "+-----------+------------------+------------+-------------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "planet_express.na.fill('Earth',subset=['Location']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(LastDelivery)=3.3491452991453)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=planet_express.select(pf.mean(planet_express['LastDelivery'])).collect()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(FLOOR(avg(NumDeliveries))=2)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm=planet_express.select(pf.floor(pf.mean(planet_express['NumDeliveries']))).collect()\n",
    "fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+---------------+-------------+--------+----+\n",
      "|CompanySize|        Profession|   LastDelivery|NumDeliveries|Location|Year|\n",
      "+-----------+------------------+---------------+-------------+--------+----+\n",
      "|          2|             Pilot|           3.61|         null|   Earth|3001|\n",
      "|          2|              null|           3.67|            3|   Earth|null|\n",
      "|          2|             Pilot|            4.0|            1|   Earth|3001|\n",
      "|          2|             Pilot|           3.19|            4|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|           2.93|            4|    null|null|\n",
      "|       null|Pizza Delivery Boy|            3.0|            2|    null|null|\n",
      "|          2|       MomCorp CEO|           2.98|            1|   Earth|3001|\n",
      "|          2|             Robot|           3.08|            2|   Earth|3001|\n",
      "|          2|       MomCorp CEO|           3.39|            3|   Earth|3001|\n",
      "|          2|              null|3.3491452991453|         null|    null|3001|\n",
      "|          2|              null|3.3491452991453|         null|    null|3001|\n",
      "|          2|       MomCorp CEO|           3.22|            1|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|            4.0|            1|   Earth|3001|\n",
      "|          2|             Robot|           3.08|            2|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|            4.0|            1|   Earth|3001|\n",
      "|          2|Pizza Delivery Boy|           3.44|            3|   Earth|3001|\n",
      "|       null|              null|3.3491452991453|            4|   Earth|3001|\n",
      "|       null|              null|3.3491452991453|            3|   Earth|3001|\n",
      "|          2| Internet Gangstas|           3.75|            2|   Earth|3001|\n",
      "|          2| Internet Gangstas|           3.81|            1|   Earth|3001|\n",
      "+-----------+------------------+---------------+-------------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "planet_express.na.fill(m[0][0],subset=['LastDelivery']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring Apache Spark 2.0: SQL, DataFrames, Datasets And Streaming\n",
    "\n",
    "![Structuring Apache Spark 2.0](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Structuring_Apache_Spark.png)\n",
    "\n",
    "[https://www.youtube.com/watch?v=1a4pgYzeFwE](https://www.youtube.com/watch?v=1a4pgYzeFwE)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern Spark DataFrame & Dataset  \n",
    "\n",
    "![Structuring Apache Spark 2.0](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Modern_Spark_DataFrame.png)\n",
    "\n",
    "Modern Spark DataFrame & Dataset | Apache Spark 2.0 Tutorial [https://youtu.be/_1byVWTEK1s](https://youtu.be/_1byVWTEK1s)  \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames: Simple and Fast Analysis of Structured Data\n",
    "\n",
    "![Spark DataFrames Simple and Fast Analysis of Structured Data](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Spark_DataFrames_Simple_and_Fast.png)\n",
    "\n",
    "Spark DataFrames: Simple and Fast Analysis of Structured Data - Michael ... [https://youtu.be/xWkJCUcD55w](https://youtu.be/xWkJCUcD55w)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Advanced Analytics on Relational Data with Spark SQL\n",
    "\n",
    "![Performing Advanced Analytics on Relational Data with Spark SQL](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Performing_Advanced_Analytics_with_Spark_SQL.png)\n",
    "\n",
    "Performing Advanced Analytics on Relational Data with Spark SQL - Michael... [https://youtu.be/PBpQJz4hdQ8](https://youtu.be/PBpQJz4hdQ8)   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update October 3, 2017 \n",
    "\n",
    "The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
