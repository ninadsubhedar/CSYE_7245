{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark - Resilient Distributed Dataset (RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make plots larger\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Apache Spark? \n",
    "\n",
    "![What is Apache Spark? ](http://nikbearbrown.com/YouTube/MachineLearning/IMG/spark-stack.png)\n",
    "\n",
    "* Spark SQL is Apache Spark's module for working with structured data.  \n",
    "* Spark Streaming makes it easy to build scalable fault-tolerant streaming applications.  \n",
    "* MLlib is Apache Spark's scalable machine learning library.   \n",
    "* GraphX is Apache Spark's API for graphs and graph-parallel computation.     \n",
    "![Apache Spark](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Apache_Spark.png)  \n",
    "\n",
    "What is Apache Spark? [https://youtu.be/SxAxAhn-BDU ](https://youtu.be/SxAxAhn-BDU)  \n",
    "    \n",
    "Apache Spark [http://spark.apache.org/](http://spark.apache.org/)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start pyspark server\n",
    "\n",
    "Starting the pyspark server will be a command like:  \n",
    "\n",
    "```bash\n",
    " ~/spark-2.2.0-bin-hadoop2.7/bin/pyspark\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark # Test that pyspark is running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SparkContext\n",
    "\n",
    "In order to use Spark and its API we will need to use a `SparkContext`.  When running Spark, you start a new Spark application by creating a [SparkContext](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext).  When the `SparkContext` is created, it asks the master for some cores to use to do work.  The master sets these cores aside just for you; they won't be used for other applications. You con think of the `SparkContext` as you would a database connection object. \n",
    "\n",
    "The `SparkContext` is usually created for you as `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.107.76.125:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## No SparkContext?\n",
    "\n",
    "If there is no `SparkContext` you'll have to create it yourself (usually as `sc`).\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"local\")\n",
    "         .setAppName(\"My app\")\n",
    "         .set(\"spark.executor.memory\", \"1g\"))\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  `SparkContext` attributes\n",
    "\n",
    "You can use Python's [dir()](https://docs.python.org/2/library/functions.html?highlight=dir#dir) function to get a list of all the attributes (including methods) accessible through the `sc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List sc's attributes\n",
    "dir(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## help()  \n",
    "\n",
    "You can use Python's [help()](https://docs.python.org/2/library/functions.html?highlight=help#help) function to get an easier to read list of all the attributes, including examples, that the `sc` object has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SparkContext in module pyspark.context object:\n",
      "\n",
      "class SparkContext(builtins.object)\n",
      " |  Main entry point for Spark functionality. A SparkContext represents the\n",
      " |  connection to a Spark cluster, and can be used to create L{RDD} and\n",
      " |  broadcast variables on that cluster.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n",
      " |  \n",
      " |  __exit__(self, type, value, trace)\n",
      " |      Enable 'with SparkContext(...) as sc: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the context on exit of the with block.\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)\n",
      " |      Create a new SparkContext. At least the master and app name should be set,\n",
      " |      either through the named parameters here or through C{conf}.\n",
      " |      \n",
      " |      :param master: Cluster URL to connect to\n",
      " |             (e.g. mesos://host:port, spark://host:port, local[4]).\n",
      " |      :param appName: A name for your job, to display on the cluster web UI.\n",
      " |      :param sparkHome: Location where Spark is installed on cluster nodes.\n",
      " |      :param pyFiles: Collection of .zip or .py files to send to the cluster\n",
      " |             and add to PYTHONPATH.  These can be paths on the local file\n",
      " |             system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
      " |      :param environment: A dictionary of environment variables to set on\n",
      " |             worker nodes.\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. Set 1 to disable batching, 0 to automatically choose\n",
      " |             the batch size based on object sizes, or -1 to use an unlimited\n",
      " |             batch size\n",
      " |      :param serializer: The serializer for RDDs.\n",
      " |      :param conf: A L{SparkConf} object setting Spark properties.\n",
      " |      :param gateway: Use an existing gateway and JVM, otherwise a new JVM\n",
      " |             will be instantiated.\n",
      " |      :param jsc: The JavaSparkContext instance (optional).\n",
      " |      :param profiler_cls: A class of custom Profiler used to do profiling\n",
      " |             (default is pyspark.profiler.BasicProfiler).\n",
      " |      \n",
      " |      \n",
      " |      >>> from pyspark.context import SparkContext\n",
      " |      >>> sc = SparkContext('local', 'test')\n",
      " |      \n",
      " |      >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError:...\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  accumulator(self, value, accum_param=None)\n",
      " |      Create an L{Accumulator} with the given initial value, using a given\n",
      " |      L{AccumulatorParam} helper object to define how to add values of the\n",
      " |      data type if provided. Default AccumulatorParams are used for integers\n",
      " |      and floating-point numbers if you do not provide one. For other types,\n",
      " |      a custom AccumulatorParam can be used.\n",
      " |  \n",
      " |  addFile(self, path, recursive=False)\n",
      " |      Add a file to be downloaded with this Spark job on every node.\n",
      " |      The C{path} passed can be either a local file, a file in HDFS\n",
      " |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
      " |      FTP URI.\n",
      " |      \n",
      " |      To access the file in Spark jobs, use\n",
      " |      L{SparkFiles.get(fileName)<pyspark.files.SparkFiles.get>} with the\n",
      " |      filename to find its download location.\n",
      " |      \n",
      " |      A directory can be given if the recursive option is set to True.\n",
      " |      Currently directories are only supported for Hadoop-supported filesystems.\n",
      " |      \n",
      " |      >>> from pyspark import SparkFiles\n",
      " |      >>> path = os.path.join(tempdir, \"test.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"100\")\n",
      " |      >>> sc.addFile(path)\n",
      " |      >>> def func(iterator):\n",
      " |      ...    with open(SparkFiles.get(\"test.txt\")) as testFile:\n",
      " |      ...        fileVal = int(testFile.readline())\n",
      " |      ...        return [x * fileVal for x in iterator]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
      " |      [100, 200, 300, 400]\n",
      " |  \n",
      " |  addPyFile(self, path)\n",
      " |      Add a .py or .zip dependency for all tasks to be executed on this\n",
      " |      SparkContext in the future.  The C{path} passed can be either a local\n",
      " |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n",
      " |      HTTP, HTTPS or FTP URI.\n",
      " |  \n",
      " |  binaryFiles(self, path, minPartitions=None)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Read a directory of binary files from HDFS, a local file system\n",
      " |      (available on all nodes), or any Hadoop-supported file system URI\n",
      " |      as a byte array. Each file is read as a single record and returned\n",
      " |      in a key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      .. note:: Small files are preferred, large file is also allowable, but\n",
      " |          may cause bad performance.\n",
      " |  \n",
      " |  binaryRecords(self, path, recordLength)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Load data from a flat binary file, assuming each record is a set of numbers\n",
      " |      with the specified numerical format (see ByteBuffer), and the number of\n",
      " |      bytes per record is constant.\n",
      " |      \n",
      " |      :param path: Directory to the input data files\n",
      " |      :param recordLength: The length at which to split the records\n",
      " |  \n",
      " |  broadcast(self, value)\n",
      " |      Broadcast a read-only variable to the cluster, returning a\n",
      " |      L{Broadcast<pyspark.broadcast.Broadcast>}\n",
      " |      object for reading it in distributed functions. The variable will\n",
      " |      be sent to each cluster only once.\n",
      " |  \n",
      " |  cancelAllJobs(self)\n",
      " |      Cancel all jobs that have been scheduled or are running.\n",
      " |  \n",
      " |  cancelJobGroup(self, groupId)\n",
      " |      Cancel active jobs for the specified group. See L{SparkContext.setJobGroup}\n",
      " |      for more information.\n",
      " |  \n",
      " |  dump_profiles(self, path)\n",
      " |      Dump the profile stats into directory `path`\n",
      " |  \n",
      " |  emptyRDD(self)\n",
      " |      Create an RDD that has no partitions or elements.\n",
      " |  \n",
      " |  getConf(self)\n",
      " |  \n",
      " |  getLocalProperty(self, key)\n",
      " |      Get a local property set in this thread, or null if it is missing. See\n",
      " |      L{setLocalProperty}\n",
      " |  \n",
      " |  hadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  hadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  parallelize(self, c, numSlices=None)\n",
      " |      Distribute a local Python collection to form an RDD. Using xrange\n",
      " |      is recommended if the input represents a range for performance.\n",
      " |      \n",
      " |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      " |      [[0], [2], [3], [4], [6]]\n",
      " |      >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
      " |      [[], [0], [], [2], [4]]\n",
      " |  \n",
      " |  pickleFile(self, name, minPartitions=None)\n",
      " |      Load an RDD previously saved using L{RDD.saveAsPickleFile} method.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 3).collect())\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numSlices=None)\n",
      " |      Create a new RDD of int containing elements from `start` to `end`\n",
      " |      (exclusive), increased by `step` every element. Can be called the same\n",
      " |      way as python's built-in range() function. If called with a single argument,\n",
      " |      the argument is interpreted as `end`, and `start` is set to 0.\n",
      " |      \n",
      " |      :param start: the start value\n",
      " |      :param end: the end value (exclusive)\n",
      " |      :param step: the incremental step (default: 1)\n",
      " |      :param numSlices: the number of partitions of the new RDD\n",
      " |      :return: An RDD of int\n",
      " |      \n",
      " |      >>> sc.range(5).collect()\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> sc.range(2, 4).collect()\n",
      " |      [2, 3]\n",
      " |      >>> sc.range(1, 7, 2).collect()\n",
      " |      [1, 3, 5]\n",
      " |  \n",
      " |  runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False)\n",
      " |      Executes the given partitionFunc on the specified set of partitions,\n",
      " |      returning the result as an array of elements.\n",
      " |      \n",
      " |      If 'partitions' is not specified, this will run over all partitions.\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n",
      " |      [0, 1, 4, 9, 16, 25]\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n",
      " |      [0, 1, 16, 25]\n",
      " |  \n",
      " |  sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)\n",
      " |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is as follows:\n",
      " |      \n",
      " |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n",
      " |             and value Writable classes\n",
      " |          2. Serialization is attempted via Pyrolite pickling\n",
      " |          3. If this fails, the fallback is to call 'toString' on each key and value\n",
      " |          4. C{PickleSerializer} is used to deserialize pickled objects on the Python side\n",
      " |      \n",
      " |      :param path: path to sequncefile\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter:\n",
      " |      :param valueConverter:\n",
      " |      :param minSplits: minimum splits in dataset\n",
      " |             (default min(2, sc.defaultParallelism))\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  setCheckpointDir(self, dirName)\n",
      " |      Set the directory under which RDDs are going to be checkpointed. The\n",
      " |      directory must be a HDFS path if running on a cluster.\n",
      " |  \n",
      " |  setJobGroup(self, groupId, description, interruptOnCancel=False)\n",
      " |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n",
      " |      different value or cleared.\n",
      " |      \n",
      " |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n",
      " |      Application programmers can use this method to group all those jobs together and give a\n",
      " |      group description. Once set, the Spark web UI will associate such jobs with this group.\n",
      " |      \n",
      " |      The application can use L{SparkContext.cancelJobGroup} to cancel all\n",
      " |      running jobs in this group.\n",
      " |      \n",
      " |      >>> import threading\n",
      " |      >>> from time import sleep\n",
      " |      >>> result = \"Not Set\"\n",
      " |      >>> lock = threading.Lock()\n",
      " |      >>> def map_func(x):\n",
      " |      ...     sleep(100)\n",
      " |      ...     raise Exception(\"Task should have been cancelled\")\n",
      " |      >>> def start_job(x):\n",
      " |      ...     global result\n",
      " |      ...     try:\n",
      " |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
      " |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
      " |      ...     except Exception as e:\n",
      " |      ...         result = \"Cancelled\"\n",
      " |      ...     lock.release()\n",
      " |      >>> def stop_job():\n",
      " |      ...     sleep(5)\n",
      " |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n",
      " |      >>> supress = lock.acquire()\n",
      " |      >>> supress = threading.Thread(target=start_job, args=(10,)).start()\n",
      " |      >>> supress = threading.Thread(target=stop_job).start()\n",
      " |      >>> supress = lock.acquire()\n",
      " |      >>> print(result)\n",
      " |      Cancelled\n",
      " |      \n",
      " |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n",
      " |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n",
      " |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n",
      " |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n",
      " |  \n",
      " |  setLocalProperty(self, key, value)\n",
      " |      Set a local property that affects jobs submitted from this thread, such as the\n",
      " |      Spark fair scheduler pool.\n",
      " |  \n",
      " |  setLogLevel(self, logLevel)\n",
      " |      Control our logLevel. This overrides any user-defined log settings.\n",
      " |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
      " |  \n",
      " |  show_profiles(self)\n",
      " |      Print the profile stats to stdout\n",
      " |  \n",
      " |  sparkUser(self)\n",
      " |      Get SPARK_USER for user who is running SparkContext.\n",
      " |  \n",
      " |  statusTracker(self)\n",
      " |      Return :class:`StatusTracker` object\n",
      " |  \n",
      " |  stop(self)\n",
      " |      Shut down the SparkContext.\n",
      " |  \n",
      " |  textFile(self, name, minPartitions=None, use_unicode=True)\n",
      " |      Read a text file from HDFS, a local file system (available on all\n",
      " |      nodes), or any Hadoop-supported file system URI, and return it as an\n",
      " |      RDD of Strings.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"sample-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello world!\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      ['Hello world!']\n",
      " |  \n",
      " |  union(self, rdds)\n",
      " |      Build the union of a list of RDDs.\n",
      " |      \n",
      " |      This supports unions() of RDDs with different serialized formats,\n",
      " |      although this forces them to be reserialized using the default\n",
      " |      serializer:\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"union-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      ['Hello']\n",
      " |      >>> parallelized = sc.parallelize([\"World!\"])\n",
      " |      >>> sorted(sc.union([textFile, parallelized]).collect())\n",
      " |      ['Hello', 'World!']\n",
      " |  \n",
      " |  wholeTextFiles(self, path, minPartitions=None, use_unicode=True)\n",
      " |      Read a directory of text files from HDFS, a local file system\n",
      " |      (available on all nodes), or any  Hadoop-supported file system\n",
      " |      URI. Each file is read as a single record and returned in a\n",
      " |      key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      For example, if you have the following files::\n",
      " |      \n",
      " |        hdfs://a-hdfs-path/part-00000\n",
      " |        hdfs://a-hdfs-path/part-00001\n",
      " |        ...\n",
      " |        hdfs://a-hdfs-path/part-nnnnn\n",
      " |      \n",
      " |      Do C{rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")},\n",
      " |      then C{rdd} contains::\n",
      " |      \n",
      " |        (a-hdfs-path/part-00000, its content)\n",
      " |        (a-hdfs-path/part-00001, its content)\n",
      " |        ...\n",
      " |        (a-hdfs-path/part-nnnnn, its content)\n",
      " |      \n",
      " |      .. note:: Small files are preferred, as each file will be loaded\n",
      " |          fully in memory.\n",
      " |      \n",
      " |      >>> dirPath = os.path.join(tempdir, \"files\")\n",
      " |      >>> os.mkdir(dirPath)\n",
      " |      >>> with open(os.path.join(dirPath, \"1.txt\"), \"w\") as file1:\n",
      " |      ...    _ = file1.write(\"1\")\n",
      " |      >>> with open(os.path.join(dirPath, \"2.txt\"), \"w\") as file2:\n",
      " |      ...    _ = file2.write(\"2\")\n",
      " |      >>> textFiles = sc.wholeTextFiles(dirPath)\n",
      " |      >>> sorted(textFiles.collect())\n",
      " |      [('.../1.txt', '1'), ('.../2.txt', '2')]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getOrCreate(conf=None) from builtins.type\n",
      " |      Get or instantiate a SparkContext and register it as a singleton object.\n",
      " |      \n",
      " |      :param conf: SparkConf (optional)\n",
      " |  \n",
      " |  setSystemProperty(key, value) from builtins.type\n",
      " |      Set a Java system property, such as spark.executor.memory. This must\n",
      " |      must be invoked before instantiating SparkContext.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  applicationId\n",
      " |      A unique identifier for the Spark application.\n",
      " |      Its format depends on the scheduler implementation.\n",
      " |      \n",
      " |      * in case of local spark app something like 'local-1433865536131'\n",
      " |      * in case of YARN something like 'application_1433865536131_34483'\n",
      " |      \n",
      " |      >>> sc.applicationId  # doctest: +ELLIPSIS\n",
      " |      'local-...'\n",
      " |  \n",
      " |  defaultMinPartitions\n",
      " |      Default min number of partitions for Hadoop RDDs when not given by user\n",
      " |  \n",
      " |  defaultParallelism\n",
      " |      Default level of parallelism to use when not given by user (e.g. for\n",
      " |      reduce tasks)\n",
      " |  \n",
      " |  startTime\n",
      " |      Return the epoch time when the Spark Context was started.\n",
      " |  \n",
      " |  uiWebUrl\n",
      " |      Return the URL of the SparkUI instance started by this SparkContext\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use help to obtain more detailed information\n",
    "help(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class map in module builtins:\n",
      "\n",
      "class map(object)\n",
      " |  map(func, *iterables) --> map object\n",
      " |  \n",
      " |  Make an iterator that computes the function using arguments from\n",
      " |  each of the iterables.  Stops when the shortest iterable is exhausted.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __next__(self, /)\n",
      " |      Implement next(self).\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Help can be used on any Python object\n",
    "help(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums = list(range(1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD vs DataFrame vs Dataset\n",
    "\n",
    "### Resilient Distributed Dataset (RDD)\n",
    "- from [http://spark.apache.org/docs/2.1.1/programming-guide.html#resilient-distributed-datasets-rdds](http://spark.apache.org/docs/2.1.1/programming-guide.html#resilient-distributed-datasets-rdds)   \n",
    " \n",
    "\n",
    "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. RDD was the primary user-facing API in Spark since its inception. There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\n",
    "\n",
    "### Parallelized Collections   \n",
    "\n",
    "Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:\n",
    "\n",
    "```python\n",
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "```\n",
    "\n",
    "Once created, the distributed dataset (distData) can be operated on in parallel. For example, we can call distData.reduce(lambda a, b: a + b) to add up the elements of the list. We describe operations on distributed datasets later on.\n",
    "\n",
    "One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10)). Note: some places in the code use the term slices (a synonym for partitions) to maintain backward compatibility.\n",
    "\n",
    "### External Datasets  \n",
    "\n",
    "PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "\n",
    "Text file RDDs can be created using SparkContext’s textFile method. This method takes an URI for the file (either a local path on the machine, or a hdfs://, s3n://, etc URI) and reads it as a collection of lines. Here is an example invocation:\n",
    "\n",
    "```python\n",
    ">>> distFile = sc.textFile(\"data.txt\")\n",
    "```\n",
    "\n",
    "Once created, distFile can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the map and reduce operations as follows: \n",
    "\n",
    "```python\n",
    "distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b).\n",
    "```\n",
    "\n",
    "Some notes on reading files with Spark:\n",
    "\n",
    "* If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.   \n",
    "\n",
    "* All of Spark’s file-based input methods, including textFile, support running on directories, compressed files, and wildcards as well. For example, you can use textFile(\"/my/directory\"), textFile(\"/my/directory/*.txt\"), and textFile(\"/my/directory/*.gz\").   \n",
    "\n",
    "* The textFile method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.   \n",
    "\n",
    "**Resilient Distributed Dataset (RDD) are fault tolerant**  \n",
    "\n",
    "The 'Resilient' in RDD comes from the fact that they are fault tolerant. If one fails, it can be recreated. \n",
    "\n",
    "**Resilient Distributed Dataset (RDD) stores data in memory** \n",
    "\n",
    "One of the defining features of Spark, compared to other data analytics frameworks (e.g., Hadoop), is that it stores data in memory rather than on disk.  This allows Spark applications to run much more quickly, because \n",
    "they are not slowed down by needing to read data from disk.\n",
    "\n",
    "** When to use RDDs? **  \n",
    "\n",
    "* You want low-level transformation and actions and control on your dataset   \n",
    "* Your data is unstructured  \n",
    "* you want to manipulate your data with functional programming constructs than domain specific expressions   \n",
    "*  don’t care about imposing a schema  \n",
    "* you can forgo some optimization and performance benefits available with DataFrames and Datasets for structured and semi-structured data.   \n",
    "\n",
    "**What happens to RDDs in Apache Spark 2.0?** \n",
    "\n",
    "\n",
    "Are they being deprecated? NO!\n",
    "\n",
    "You can seamlessly move between DataFrame or Dataset and RDDs at will—by simple API method calls—and DataFrames and Datasets are built on top of RDDs.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and DataFrames\n",
    "- from [https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes)   \n",
    "\n",
    "Like an RDD, a Datasets and DataFrames are immutable distributed collections of data. \n",
    "\n",
    "A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). The case for R is similar.\n",
    "\n",
    "Starting in Spark 2.0, Dataset takes on two distinct APIs characteristics: a strongly-typed API and an untyped API.\n",
    "\n",
    "A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset<Row> to represent a DataFrame.\n",
    "\n",
    "![Unified Apache Spark 2.0](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Unified_Apache_Spark_2.0.png)\n",
    "\n",
    "Image from [http://go.databricks.com/apache-spark-2.0-presented-by-databricks-co-founder-reynold-xin](http://go.databricks.com/apache-spark-2.0-presented-by-databricks-co-founder-reynold-xin)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parallelize data using 8 partitions\n",
    "# This operation is a transformation of data into an RDD\n",
    "# Spark uses lazy evaluation, so no Spark jobs are run at this point\n",
    "rdd = sc.parallelize(nums, 8)\n",
    "# Cache the RDD\n",
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "parallelize(c, numSlices=None)\n",
    "```\n",
    "\n",
    "Distribute a local Python collection to form an RDD. Using xrange is recommended if the input represents a range for performance.\n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parallelize in module pyspark.context:\n",
      "\n",
      "parallelize(c, numSlices=None) method of pyspark.context.SparkContext instance\n",
      "    Distribute a local Python collection to form an RDD. Using xrange\n",
      "    is recommended if the input represents a range for performance.\n",
      "    \n",
      "    >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      "    [[0], [2], [3], [4], [6]]\n",
      "    >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
      "    [[], [0], [], [2], [4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc.parallelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD id: 0\n"
     ]
    }
   ],
   "source": [
    "# Each RDD gets a unique ID\n",
    "print ('RDD id: {0}'.format(rdd.id()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "collect()\n",
    "```\n",
    "\n",
    "Return a list that contains all of the elements in this RDD.\n",
    "\n",
    "Note This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.\n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View entire RDD\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "take(num)\n",
    "``` \n",
    "\n",
    "Take the first num elements of the RDD.\n",
    "\n",
    "It works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit.\n",
    "\n",
    "Translated from the Scala implementation in RDD#take().\n",
    "\n",
    "Note this method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.\n",
    "\n",
    "```python\n",
    ">>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
    "[2, 3]\n",
    ">>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
    "[2, 3, 4, 5, 6]\n",
    ">>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
    "[91, 92, 93]\n",
    "```\n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 7 of the RDD\n",
    "rdd.take(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "map(f, preservesPartitioning=False)\n",
    "\n",
    "``` \n",
    "Return a new RDD by applying a function to each element of this RDD.\n",
    "\n",
    "```python\n",
    ">>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    ">>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
    "[('a', 1), ('b', 1), ('c', 1)]\n",
    "```\n",
    "\n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plus_one=rdd.map(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus_one.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[5] at RDD at PythonRDD.scala:48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create minus_one function to subtract 1 rather than a lambda expression\n",
    "def minus_one(x):\n",
    "    return (x - 1)\n",
    "minus_one=rdd.map(minus_one)\n",
    "print (minus_one)\n",
    "minus_one.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  rdd.saveAsTextFile('data/rez_1.txt')\n",
    "except:\n",
    "  print (\"File exists\")\n",
    "try:\n",
    "  rdd.map(lambda x: \"Number \" + x ).saveAsTextFile('data/formated_rez_1.txt')\n",
    "except:\n",
    "  print (\"File exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD from text\n",
    "  \n",
    "The textFile method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "RDD id: 13\n"
     ]
    }
   ],
   "source": [
    "text=sc.textFile('data/Dr_Seuss_Oh_The_Places_Youll_Go.txt')\n",
    "print (type(rdd))\n",
    "print ('RDD id: {0}'.format(text.id()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You have brains in your head. You have feet in your shoes. ',\n",
       " 'You can steer yourself in any direction you choose. ',\n",
       " \"You're on your own, and you know what you know. \",\n",
       " \"And you are the guy who'll decide where to go. \",\n",
       " \"― Dr. Seuss, Oh, The Places You'll Go!\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "RDD id: 15\n"
     ]
    }
   ],
   "source": [
    "text_p=sc.textFile('data/Dr_Seuss_Oh_The_Places_Youll_Go.txt', minPartitions=11)\n",
    "print (type(text_p))\n",
    "print ('RDD id: {0}'.format(text_p.id()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You have brains in your head. You have feet in your shoes. ',\n",
       " 'You can steer yourself in any direction you choose. ',\n",
       " \"You're on your own, and you know what you know. \",\n",
       " \"And you are the guy who'll decide where to go. \",\n",
       " \"― Dr. Seuss, Oh, The Places You'll Go!\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_p.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You have brains in your head. You have feet in your shoes. ',\n",
       " 'You can steer yourself in any direction you choose. ',\n",
       " \"You're on your own, and you know what you know. \",\n",
       " \"― Dr. Seuss, Oh, The Places You'll Go!\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesYou = text.filter(lambda line: \"You\" in line)\n",
    "linesYou.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common data flow pattern is MapReduce, as popularized by Hadoop. Spark can implement MapReduce flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('have', 2),\n",
       " ('in', 3),\n",
       " ('shoes.', 1),\n",
       " ('steer', 1),\n",
       " ('direction', 1),\n",
       " ('choose.', 1),\n",
       " (\"You're\", 1),\n",
       " ('know', 1),\n",
       " ('And', 1),\n",
       " ('are', 1),\n",
       " ('where', 1),\n",
       " ('go.', 1),\n",
       " ('Seuss,', 1),\n",
       " ('Oh,', 1),\n",
       " ('The', 1),\n",
       " ('Places', 1),\n",
       " (\"You'll\", 1),\n",
       " ('You', 3),\n",
       " ('brains', 1),\n",
       " ('your', 3),\n",
       " ('head.', 1),\n",
       " ('feet', 1),\n",
       " ('can', 1),\n",
       " ('yourself', 1),\n",
       " ('any', 1),\n",
       " ('you', 4),\n",
       " ('on', 1),\n",
       " ('own,', 1),\n",
       " ('and', 1),\n",
       " ('what', 1),\n",
       " ('know.', 1),\n",
       " ('the', 1),\n",
       " ('guy', 1),\n",
       " (\"who'll\", 1),\n",
       " ('decide', 1),\n",
       " ('to', 1),\n",
       " ('―', 1),\n",
       " ('Dr.', 1),\n",
       " ('Go!', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts = text.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**count()**  \n",
    "Return the number of elements in this RDD.\n",
    "\n",
    "```python\n",
    ">>> sc.parallelize([2, 3, 4]).count()\n",
    "```\n",
    "\n",
    "**countApprox(timeout, confidence=0.95)**   \n",
    "Note Experimental\n",
    "Approximate version of count() that returns a potentially incomplete result within a timeout, even if not all tasks have finished.\n",
    "\n",
    "```python\n",
    ">>> rdd = sc.parallelize(range(1000), 10)\n",
    ">>> rdd.countApprox(1000, 1.0)\n",
    "1000\n",
    "```\n",
    "\n",
    "**countApproxDistinct(relativeSD=0.05)**  \n",
    "\n",
    "Return approximate number of distinct elements in the RDD.\n",
    "\n",
    "The algorithm used is based on streamlib’s implementation of “HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm”, available here.\n",
    "\n",
    "Parameters:\trelativeSD – Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017.\n",
    "\n",
    "```python\n",
    ">>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
    ">>> 900 < n < 1100\n",
    "True\n",
    ">>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
    ">>> 16 < n < 24\n",
    "True\n",
    "```\n",
    "\n",
    "  [http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print (rdd.count())\n",
    "print (rdd.countApprox(1000,0.9))\n",
    "print (rdd.countApproxDistinct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**filter(f)**\n",
    "\n",
    "Return a new RDD containing only the elements that satisfy a predicate.\n",
    "\n",
    "```python\n",
    ">>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    ">>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
    "[2, 4]\n",
    "```  \n",
    "\n",
    "  [http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_lt5=rdd.filter(lambda x: x < 5)\n",
    "rdd_lt5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_even=rdd.filter(lambda x: x % 2 == 0)\n",
    "rdd_even.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**first()**  \n",
    "\n",
    "Return the first element in this RDD.\n",
    "\n",
    "```python\n",
    ">>> sc.parallelize([2, 3, 4]).first()\n",
    "2\n",
    ">>> sc.parallelize([]).first()\n",
    "Traceback (most recent call last):\n",
    "    ...\n",
    "ValueError: RDD is empty\n",
    "```\n",
    "\n",
    "**take(num)**  \n",
    "\n",
    "Take the first num elements of the RDD.\n",
    "\n",
    "It works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit.\n",
    "\n",
    "Translated from the Scala implementation in RDD#take().\n",
    "\n",
    "Note this method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.  \n",
    "\n",
    "```python\n",
    ">>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
    "[2, 3]\n",
    ">>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
    "[2, 3, 4, 5, 6]\n",
    ">>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
    "[91, 92, 93]\n",
    "```\n",
    "\n",
    "**takeOrdered(num, key=None)**   \n",
    "\n",
    "Get the N elements from an RDD ordered in ascending order or as specified by the optional key function.\n",
    "\n",
    "Note this method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.\n",
    "\n",
    "```python\n",
    ">>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
    "[1, 2, 3, 4, 5, 6]\n",
    ">>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
    "[10, 9, 7, 6, 5, 4]\n",
    "```\n",
    "\n",
    "\n",
    "**takeSample(withReplacement, num, seed=None)**      \n",
    "\n",
    "Return a fixed-size sampled subset of this RDD.\n",
    "\n",
    "Note This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.  \n",
    "\n",
    "```python\n",
    ">>> rdd = sc.parallelize(range(0, 10))\n",
    ">>> len(rdd.takeSample(True, 20, 1))\n",
    "20\n",
    ">>> len(rdd.takeSample(False, 5, 2))\n",
    "5\n",
    ">>> len(rdd.takeSample(False, 15, 3))\n",
    "10\n",
    "```\n",
    "\n",
    "  [http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)\n",
    "\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[4, 5, 6]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.take(3))\n",
    "print(rdd.filter(lambda x: x > 3).take(3))\n",
    "print(rdd.take(33)) # ok to take more elements than the RDD has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n",
      "[10, 9, 8, 7, 6, 5]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.takeOrdered(6))\n",
    "print(rdd.takeOrdered(6, key=lambda x: -x)) # Reverse order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 1, 3, 1, 10, 5, 1]\n",
      "[6, 10, 4, 5, 7, 8, 3]\n",
      "[6, 10, 4, 5, 7, 8, 3]\n",
      "[2, 6, 7, 1, 10, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.takeSample(True, 7, 1))\n",
    "print(rdd.takeSample(False, 7, 2))\n",
    "print(rdd.takeSample(False, 7, 2))\n",
    "print(rdd.takeSample(False, 7, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduce(f)**\n",
    "\n",
    "Reduces the elements of this RDD using the specified commutative and associative binary operator. Currently reduces partitions locally.\n",
    "\n",
    "```python\n",
    ">>> from operator import add\n",
    ">>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
    "15\n",
    ">>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
    "10\n",
    ">>> sc.parallelize([]).reduce(add)\n",
    "```\n",
    "\n",
    "Traceback (most recent call last):\n",
    "    ...\n",
    "ValueError: Can not reduce() empty RDD  \n",
    "\n",
    "**\n",
    "reduceByKey(func, numPartitions=None, partitionFunc=<function portable_hash at 0x7fc35dbc8e60>)**  \n",
    "\n",
    "Merge the values for each key using an associative and commutative reduce function.\n",
    "\n",
    "This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a “combiner” in MapReduce.\n",
    "\n",
    "Output will be partitioned with numPartitions partitions, or the default parallelism level if numPartitions is not specified. Default partitioner is hash-partition.\n",
    "\n",
    "```python\n",
    ">>> from operator import add\n",
    ">>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    ">>> sorted(rdd.reduceByKey(add).collect())\n",
    "[('a', 2), ('b', 1)]\n",
    "```\n",
    "\n",
    "**reduceByKeyLocally(func)**  \n",
    "\n",
    "Merge the values for each key using an associative and commutative reduce function, but return the results immediately to the master as a dictionary.\n",
    "\n",
    "This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a “combiner” in MapReduce.\n",
    "\n",
    "```python\n",
    ">>> from operator import add\n",
    ">>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    ">>> sorted(rdd.reduceByKeyLocally(add).items())\n",
    "[('a', 2), ('b', 1)]\n",
    "```\n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "-23\n",
      "55\n",
      "55\n",
      "-23\n",
      "-7\n"
     ]
    }
   ],
   "source": [
    "from operator import add, sub\n",
    "print(rdd.reduce(add))\n",
    "print(rdd.reduce(sub))\n",
    "print(rdd.reduce(lambda a, b: a + b))\n",
    "print(rdd.reduce(lambda a, b: b + a))\n",
    "# Note that subtraction is not both associative and commutative\n",
    "print(rdd.reduce(lambda a, b: a - b))\n",
    "print(rdd.reduce(lambda a, b: b - a ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_tmp = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd_tmp.reduceByKeyLocally(add).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_tmp = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd_tmp.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**flatMap(f, preservesPartitioning=False)**   \n",
    "\n",
    "\n",
    "Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.\n",
    "\n",
    "```python\n",
    ">>> rdd = sc.parallelize([2, 3, 4])\n",
    ">>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
    "[1, 1, 1, 2, 2, 3]\n",
    ">>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
    "[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
    "```\n",
    "\n",
    "**flatMapValues(f)**    \n",
    "\n",
    "Pass each value in the key-value pair RDD through a flatMap function without changing the keys; this also retains the original RDD’s partitioning.\n",
    "\n",
    "\n",
    "```python\n",
    ">>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
    ">>> def f(x): return x\n",
    ">>> x.flatMapValues(f).collect()\n",
    "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
    "```\n",
    "\n",
    "Applying a map() transformation would yield a new RDD made up of iterators. Each iterator could have zero or more elements. Instead, we often want an RDD consisting of the values contained in those iterators. The solution is to use a flatMap() transformation, flatMap() is similar to map(), except that with flatMap() each input item can be mapped to zero or more output elements.\n",
    "\n",
    "  [http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 8, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.flatMap(lambda x: range(1, x)).collect())\n",
    "print(sorted(rdd.flatMap(lambda x: range(1, x)).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (10, 10)]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.flatMap(lambda x: [(x, x)]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range(1, 1), range(1, 2), range(1, 3), range(1, 4), range(1, 5), range(1, 6), range(1, 7), range(1, 8), range(1, 9), range(1, 10)]\n",
      "[1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.map(lambda x: range(1, x)).collect())\n",
    "print(rdd.flatMap(lambda x: range(1, x)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1, 1)], [(2, 2)], [(3, 3)], [(4, 4)], [(5, 5)], [(6, 6)], [(7, 7)], [(8, 8)], [(9, 9)], [(10, 10)]]\n",
      "[(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (10, 10)]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.map(lambda x: [(x, x)]).collect())\n",
    "print(rdd.flatMap(lambda x: [(x, x)]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Caching RDDs\n",
    "\n",
    "Spark keeps your RDDs in memory. However, when memory is limited Spark will automatically delete RDDs from memory to make space for new RDDs.  \n",
    "\n",
    "If you plan to use an RDD many times, you can Spark to cache that RDD. You can use the `cache()` operation to keep the RDD in memory.\n",
    "\n",
    "**cache()**\n",
    "\n",
    "Persist this RDD with the default storage level (MEMORY_ONLY).\n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<bound method RDD.name of Bears RDD ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Name the RDD\n",
    "rdd.setName('Bears RDD')\n",
    "# Cache the RDD\n",
    "rdd.cache()\n",
    "# Is it cached\n",
    "print (rdd.is_cached)\n",
    "print (rdd.name)\n",
    "# Release the RDD to free memory \n",
    "rdd.unpersist()\n",
    "print (rdd.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**cartesian(other)**  \n",
    "\n",
    "Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in self and b is in other.\n",
    "\n",
    "```python\n",
    ">>> rdd = sc.parallelize([1, 2])\n",
    ">>> sorted(rdd.cartesian(rdd).collect())\n",
    "[(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "```\n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'),\n",
       " (1, 'b'),\n",
       " (2, 'a'),\n",
       " (2, 'b'),\n",
       " (3, 'a'),\n",
       " (3, 'b'),\n",
       " (4, 'a'),\n",
       " (5, 'a'),\n",
       " (4, 'b'),\n",
       " (5, 'b'),\n",
       " (6, 'a'),\n",
       " (6, 'b'),\n",
       " (7, 'a'),\n",
       " (7, 'b'),\n",
       " (8, 'a'),\n",
       " (8, 'b'),\n",
       " (9, 'a'),\n",
       " (10, 'a'),\n",
       " (9, 'b'),\n",
       " (10, 'b')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab = sc.parallelize(['a', 'b'])\n",
    "rdd.cartesian(ab).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pipe(command, env=None, checkCode=False)**   \n",
    "\n",
    "Return an RDD created by piping elements to a forked external process.\n",
    "\n",
    "```python\n",
    ">>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
    "[u'1', u'2', u'', u'3']\n",
    "```\n",
    "\n",
    "Parameters:\tcheckCode – whether or not to check the return value of the shell comman\n",
    "\n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.pipe(\"grep 5\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**repartition(numPartitions)**  \n",
    "\n",
    "Return a new RDD that has exactly numPartitions partitions.\n",
    "\n",
    "Can increase or decrease the level of parallelism in this RDD. Internally, this uses a shuffle to redistribute data. If you are decreasing the number of partitions in this RDD, consider using coalesce, which can avoid performing a shuffle.\n",
    "\n",
    "```python\n",
    ">>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    ">>> sorted(rdd.glom().collect())\n",
    "[[1], [2, 3], [4, 5], [6, 7]]\n",
    ">>> len(rdd.repartition(2).glom().collect())\n",
    "2\n",
    ">>> len(rdd.repartition(10).glom().collect())\n",
    "10\n",
    "```\n",
    "\n",
    "**repartitionAndSortWithinPartitions(numPartitions=None, partitionFunc=<function portable_hash at 0x7fc35dbc8e60>, ascending=True, keyfunc=<function <lambda> at 0x7fc35dbcf758>)**  \n",
    "\n",
    "Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys.\n",
    "\n",
    "```python\n",
    ">>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
    ">>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, 2)\n",
    ">>> rdd2.glom().collect()\n",
    "[[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
    "```\n",
    "\n",
    "**glom()**  \n",
    "\n",
    "Return an RDD created by coalescing all elements within each partition into a list.\n",
    "\n",
    "```python\n",
    ">>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    ">>> sorted(rdd.glom().collect())\n",
    "[[1, 2], [3, 4]]\n",
    "```\n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n",
      "[[1], [2, 3], [4, 5], [6, 7]]\n",
      "2\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "rdd_tmp = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    "print (rdd_tmp.collect())\n",
    "print (sorted(rdd_tmp.glom().collect()))\n",
    "print (len(rdd_tmp.repartition(2).glom().collect()))\n",
    "print (len(rdd.repartition(10).glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Checkpointing RDDs\n",
    "\n",
    "Checkpointing saves the data to HDFS, which provide fault tolerant storage across nodes.\n",
    "\n",
    "**setCheckpointDir(dirName)**\n",
    "\n",
    "Set the directory under which RDDs are going to be checkpointed. The directory must be a HDFS path if running on a cluster.\n",
    "\n",
    "**checkpoint()**  \n",
    "\n",
    "Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint directory set with SparkContext.setCheckpointDir() and all references to its parent RDDs will be removed. This function must be called before any job has been executed on this RDD. It is strongly recommended that this RDD is persisted in memory, otherwise saving it on a file will require recomputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.setCheckpointDir(\"data\")\n",
    "rdd_tmp = sc.parallelize([1,2,3,4,5])\n",
    "\n",
    "for i in range(999):\n",
    "    rdd_tmp_plus_one = rdd_tmp.map(lambda x: x + 1)\n",
    "\n",
    "    if i % 100 == 0: \n",
    "        rdd_tmp_plus_one.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Broadcast Variables\n",
    "\n",
    "Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are variables that are only “added” to, such as counters and sums.   \n",
    "\n",
    "**class pyspark.Broadcast(sc=None, value=None, pickle_registry=None, path=None)**    \n",
    "\n",
    "A broadcast variable created with SparkContext.broadcast(). Access its value through value.\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    ">>> from pyspark.context import SparkContext\n",
    ">>> sc = SparkContext('local', 'test')\n",
    ">>> b = sc.broadcast([1, 2, 3, 4, 5])\n",
    ">>> b.value\n",
    "[1, 2, 3, 4, 5]\n",
    ">>> sc.parallelize([0, 0]).flatMap(lambda x: b.value).collect()\n",
    "[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n",
    ">>> b.unpersist()\n",
    ">>> large_broadcast = sc.broadcast(range(10000))\n",
    "```\n",
    "\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Accumulators  \n",
    "\n",
    "**class pyspark.Accumulator(aid, value, accum_param)**\n",
    "\n",
    "A shared variable that can be accumulated, i.e., has a commutative and associative “add” operation. Worker tasks on a Spark cluster can add values to an Accumulator with the += operator, but only the driver program is allowed to access its value, using value. Updates from the workers get propagated automatically to the driver program.\n",
    "\n",
    "While SparkContext supports accumulators for primitive data types like int and float, users can also define accumulators for custom types by providing a custom AccumulatorParam object. Refer to the doctest of this module for an example.\n",
    "\n",
    "_add(term)_  \n",
    "\n",
    "Adds a term to this accumulator’s value\n",
    "\n",
    "_value_  \n",
    "\n",
    "Get the accumulator’s value; only usable in driver program\n",
    "\n",
    "**class pyspark.AccumulatorParam**    \n",
    "\n",
    "Helper object that defines how to accumulate values of a given type.\n",
    "\n",
    "_addInPlace(value1, value2)_  \n",
    "\n",
    "Add two values of the accumulator’s data type, returning a new value; for efficiency, can also update value1 in place and return it.\n",
    "\n",
    "_zero(value)_  \n",
    "\n",
    "Provide a “zero value” for the type, compatible in dimensions with the provided value (e.g., a zero vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Apache Spark 2.0 Review  \n",
    "\n",
    "![Apache Spark 2.0 Review](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Apache_Spark_2.0_Review.png)\n",
    "\n",
    "\n",
    "Apache Spark 2.0 [https://youtu.be/ssPBlqiRJGY](https://youtu.be/ssPBlqiRJGY)\n",
    "                                                \n",
    "                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update October 3, 2017 \n",
    "\n",
    "The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
